* Homework 10

** Multiple Solutions to the first problem

   f continuous implies f((closure E)) subset (closure f(E))

*** Proof 1

    x in E implies f(x) in f(E) subset of closure.

    x in E' implies x = lim x_n, all x_n in E implies lim f(x_n) = f(x), f(x)
    in (closure f(E))

*** Proof 2

    f continuous implies f^-1((closure f(E))) closed. pretty much that.

** 4.3 Show Z(f), set of x s.t. f(x) = 0, closed

*** Proof 1

    {0} closed in RR, so f^-1({0}) is closed.

*** Proof 2

    Suppose x in Z(f)'

    'Analysis is easier than algebra - many shots' - Dr. Ball

** 4.4 E dense in X -> f(E) dense in f(X)

*** Part a

**** low brow solution - choose y = f(x) in f(X).

     then x in X implies x = lim x_n so x_n in E implies f(x) = lim f(x_n)

     so, as f is continuous, f(x) in (closure E).

**** high brow solution

     f(X) = f((closure E)) subset (closure f(E))  done. Much quicker!

*** Part b

    g(p) = f(p) for all p in E -> g(x) = f(x) for all x in X

**** low brow solution

     x in X so x = lim x_n, so we can do it by uniqueness.


* Discontinuities

  f :: E to Y, as usual.

  (if (and (x not an isolated point of E)
           (not (lim (t -> x) f(t) = f(x)))))

  so there exists some epsilon s.t. for all delta > 0,
  exists t_delta with 0 < d(t_delta, x) < delta yet d(f(t),f(x))
  .GEQ. epsilon.

** Discontinuities of the First Kind

   discontinuity of the first kind : f discontinuous at x, but the left and
   right limits exist.

   second kind : not first.

*** Example

    x not in QQ -> f continous at x

    x in QQ \ {0}  -> discontinuity of the first kind at x. then
    0 < abs(t  - x) < delta

    so t = m/n as n-> inf must go to zero.

** Consider X = Y = RR

*** first kind of discontinuity

    lim (t -> x+) f(t) and lim (t -> x-) both exist, but one or both not equal
    to f(x).

*** second kind of discontinuity

    f has some discontinuity at x, but it is not of the first kind.

    favorite example : f(x) = sin(1/x) for some x /= 0 and 0 at x = 0

    quick review of something that we have not done before.

    so the lim sup (t -> inf) = sup  { f(t) such that 0 < abs(t - x) < delta}

** Useful application of the boundedness of a monotone function

   define lim sup (t -> x) f(x) =
          lim (delta -> 0+) sup (f(t) | 0 < abs(t - x) < delta)

   Therefore g(delta) increases (more things, supremum goes up)

   Therefore g(0+) = lim (delta -> 0+) g(delta)

   Therefore this is equivalent to
   inf (delta > 0) sup (t : 0 < abs(t - x) < delta) f(t)

*** Even more compact notation

    lim sup (t -> x+) = lim (delta -> 0+) sup (f(t) : x < t < x + delta)

    call the thing inside the limit g(delta) (it is an increasing function,
    but not the same function)

    (the one sided limit from the right is the infimum based on the proof
    regarding monotonic functions)

    = inf (delta) sup(t : x < t < x + delta) f(t)

*** Then, for continuity

    lim inf (t -> x) f(t),
    lim (t -> x+) f(t),
    lim sup (t -> x-) f(t),
    lim inf(t -> x-) f(t)

    all need to be defined and shown equal for no discontinuities.

*** so, in conclusion

    lim (t -> x+) f(t) = L means:
    lim sup (t -> x+) f(t) = lim inf (t -> x+) f(t) = L


** Example (discontinuity of the second kind)

   f(t) = sin(1/t) for t /= 0, 0 for t = 0.

   then lim sup (t -> 0+) f(x) = 1 but lim inf (t -> 0+) f(t) = -1.

   Therefore the limit itself does not exist.


** More fun examples

   Dirichlet function

   (defun dirichlet-function (x)
     (cond
       ((rationalp x) 1)
       ((irrationalp x) 0)))

   We can create a lim sup and lim inf that are unequal so we get a
   discontinuity of the second kind immediately.

   f(0) = 0

   f(x) = 0 if x is irrational

   f(x) = 1/n if x = m / n



* Monotonic Functions

  f on (a,b) (real-valued; orderable sets) monotonically increasing means

  a < x < y < b  implies that f(x) < f(y)

  and decreasing - f(x) > f(y)

*** Theorem : Monotonically Increasing Functions

    if f is monotonically increasing on (a,b) then

    f(x+) = lim f(t) for (t -> x+) and f(x-) = lim (t -> x-) f(t) exist.

    so the only possibilities for discontinuities for f are discontinuities of
    the first kind (that is, whole function jumps up or jumps down).

**** Proof

     We need a candidate for some L = lim (t -> x+) f(t). Pick some

     L = inf (f(t) s.t. t > x). We can do this directly (no need for
     sequences).

     Let epsilon > 0. Then we must find some delta > 0 so that

     x < t < x + delta implies abs(f(t) - L) < epsilon

     since we are in the real numbers, we know that L - epsilon < f(t).

     (recall that L is the greatest lower bound of the set by definition)

     then L + epsilon cannot be a lower bound. This implies that there exists
     some t0 where t0 > x and f(t0) < L + epsilon.

     However, since t < t0 implies that f(t) .LEQ. f(t0) < L + epsilon

     Therefore we should take delta = t0 - x > 0. Then x < t < t0 + delta

     so x + delta (which is our t0) implies that f(t) .LEQ. f(t0) < L +

     epsilon. Therefore we are done


*** Theorem : limits exist on monotonic functions

    more specifically - lim (x -> t+) f(x) and lim (x -> x-) f(x) always exist.

    put another way - all discontinuities are of the first kind.

    'monotonic functions are tame'


*** Theorem : Countable Discontinuities

    f :: (a,b) -> RR monotonically increasing or decreasing: then the number of
    discontinuities is at most countable.

    (this is a corollary of a more general result)

**** Sets of discontinuities

     the set of discontinuities of the first kind is at most countable. (this
     is an exercise, #17, next homework)

**** Direct proof for monotonic

     Say that f is increasing. Let E = { x : f(x) discontinuous}

     general facts - for all x, f(x-) .LEQ. f(x) .LEQ. (fx+) = inf f(t)

     discontinuity implies that f(x-) < f(x+) .

     Furthermore - a < x < y < b so f(x+) .LEQ. f(y-). (may not be
     discontinuous).

     Therefore, given some discontinuity x, choose some rational r(x) with
     f(x-) < r(x) < f(x+) (open interval contains a rational)

     If x1 /= x2 (say x1 < x2) then f(x1+) .LEQ. f(x2-) (monotonic.)
     this forces r(x_1) < r(x_2) therefore r :: E -> QQ is 1-1.

     (1-1 relationship between E and QQ implies that E is countable.)


*** Discontinuous Monotonic Functions

    something with jumps. Discontinuities of the first kind do not need bo be
    isolated! we can say something like

    E = {x1, x2, ...}

    that there is a f (monotonically increasing) with a set of discontinuities
    exactly equal to E. Therefore if E = QQ then we have a jump discontinuity
    at every rational.

    construction : given numbers c_m > 0 with SUM c_n < inf (e.g. c_n = 1/2^n)
    define f(x) = SUM (n : x_n < x) c_n

    Then : f is monotonically increasing (it is a sum) so
    for x < y we get {n : x_n < x} subset {n : x_n < y}

    implies f(x) = SUM (n : x_n < x) c_n .LEQ. SUM (n : x_n < y) c_n = f(y)

    then the set of discontinuities of f is E.

    Computation : f(x-) = sup (y < x) f(y) = sup (y < x) SUM (n : x_n < y) c_n

    may be we should prove this?

**** Proof
     Clearly : SUM (n : x_n < x) c_n = upper bound for any sum constrained by
     x_n < y.

     Suppose epsilon > 0. Then

     L = SUM (n : x_n < x) c_n - epsilon

     then there exists finitely subsets F of E of ns with each x_n < x

     with SUM (n in F) c_n > L - epsilon.

     Take  = max { x_n : n in F} < x

     then SUM (n : x_n < y) c_n .GEQ. SUM (n in F) c_n > L - epsilon.

     L - epsilon is not an upper bound so L = sup (f(y) : y < x)

     Similarly : f(x+) = inf (y > x) f(y)
     = inf (y > x) SUM (n : x_n < y) c_n = SUM (n : x_n) c_n = f(x)
     (if x /= any x_n)

     or it equals f(x) + c_n_0  if x = x_n_0 (not part of the definition, so we
     need one more term : this is the jump.)

     Therefore f is left continuous (meaning f(x-) = f(x)) but there are jumps
     c_n at each x_n for each n.

**** Alternate Notation

     I(y) = 0 if x < 0, 1 if x .GEQ. 0

     then f(x) = SM c_n I(x - x_n) (integrals!)

     Riemann-Stieltjes integral


* Infinite Limits

  Last topic!

  RR overline = RR Union {+inf, -inf} is a metric space with

  d overline (x,y) = abs( arctan(x) - arctan(y) )
  where arctan(+inf) = pi/2
  where arctan(-inf) = -pi/2

  Yes, the distance between inf and -inf is pi.

** What are the neighborhoods?

   N_delta(x) overline = { y in RR overline : d overline(x,y) < delta}
   for some x in RR overline.

   for x in RR, N_delta(x) = {y in RR : abs(y - x) < delta}

*** Relationship between neighborhoods

    We may think of RR as the metric space in the d overline metric (as a
    subset of RR)

*** Metric Properties

    arctan is continuous, RR -> (-pi/2, pi/2)
    has continuous inverses. For x in RR, we have delta > 0; there exists some
    delta' s.t. N_delta(x) `subset` N_delta'(x)

    therefore RR has the same open sets whether we use d or d overline
    (we say that they are equivalent metrics.)

*** Neighborhood of Infinity

    We have to work with d overline. Consider +inf.

    N_delta overline = {y in RR overline : abs(arctan(y) - pi/2) < delta}
    well, arctan is bounded by pi/2 anyway, so

    pi/2 - delta < arctan y. Take the tangent of both sides.


* Homework 11

** Homework Hints

*** #6 - f :: E -> Y metric spaces, E is compact. Then, show that

    f continuous on E <-> set {(x,f(x)) | x in E} is compact.

    do this problem at the level of metric spaces.

    Example : E subset RR, Y = RR. Then set {(x,f(x))} subset RR^2.

    open cover characterizations - we have sequential Bolzano-Weirstrass style,
    or just for pure metric spaces.

*** #7 -

    Define f, g :: RR^2 -> R by

    f(0,0) = 0
    f(x,y) = xy^2/(x^2 + y^4)

    g(0,0) = 0
    g(x,y) = xy^2 / (x^2 + y^6)

    then show that neither function is continuous near zero. Can't fail it by
    drawing lines.

    Hint : look at curves f(alpha * t^m, beta * t^n) and attempt to make it
    fail.

    from maxima session - try x = t^2, y = t.

** Problem 5

   f :: E -> RR1 continuous, E closed implies f extends to continuous
   g :: RR -> RR.

   Solution: E closed so E^c is some collection of open intervals.

   say that E^c = Union (a_n,b_n)

   g(x) = f(x) for x in E

   g(x) = (f(b_n)  - f(a_n))/ (b_n - a_n) * (x - a_n) (point slope form)

*** Show continuous

    x in E - continuous because f continuous
    x in some (a_n, b_n) - continuous because linear.

    how about at limit points?

    show that lim (x -> a_n+) g(x) = lim (x -> a_n-) f(x)
    valid by definition.

    for something like the Cantor set - no interior! no x in E^o.

    how about isolated points?

    on either side - g is a line, so continuous.

    how about E \ E^o?

    so every deleted neighborhood intersects E and E^c.

    Let {x_n} = {y_n} `union` {z_n}

    so y_n in E, z_n in E^c. Then the whole thing is these two
    subsequences. Therefore it suffices to show that g(y_n) -> g(x) and g(z_n)
    -> g(x). Therefore as y_n in E, z_n in E^c, we get that f(y_n) -> f(x).

    How about z_n? z_n -> x, z_n in E^c, and z_n -> x. In this situation,
    a_n -> x and b_n -> x (think of the Cantor set). Therefore the end-points
    are converge to f(x), which is g(x), so z_n is sandwiched between things
    that converge to g(x).

** Problem 6

   E is a compact metric space and E is the graph. Then f continuous iff the
   graph is compact.

*** Solution 1 : Topologist's Proof

    (->) f continuous implies that g : x -> (x, f(x)) is continuous.

    E compact implies (by theorem) that g(E) is compact. Done.

    (<-) Assume that the graph and E are compact. Then h :: (x, f(x)) -> x is
    continuous. Therefore h :: graph -> E, and the graph is continuous (as is
    E). Therefore h^-1 is continuous (by 4.19). Therefore f = PI_2 . h^-1 is
    continuous (where PI_2 :: (x,y) -> y is continuous). Composition of
    continuous functions is continuous, so f is continuous.

*** Solution 2 : Sequences.

    (->) Choose some {x_n, f(x_n)} subset G - we must find a convergent
    sequence in E, so that as E is compact -> exists subsequence converging to
    x_0 in E.

    f is continuous, so the subsequential limits are equal. Therefore (x_n_k,
    f(x_n_k)) converges to some (x_0, f(x_0)) -> done.

    (<-) Assume that the graph is compact (show that f is continuous). We are
    stuck working with subsequences in this world, so by HW 7.1

    f(x_n) -> f(x) is the same as {f(x_n_k)} in {f(x_n)} has another
    subsequence inside where {f(x_n_k_j)} -> f(x).

    Therefore as the subsequence converges to f(x), the whole thing
    does. Perhaps this is shorter than contrapositive.

    Given some n1 < n2 < ... then the sequence {x_n_k, f(x_n_k)} in the graph
    (assumed compact) implies that there exists a subsequence (x_n_k_j,
    f(x_n_k_j)) which converges to x_0, y_0 in the graph. However, the
    original sequence is convergint to x_0, so by uniqueness of limits all
    subsequences converge to x_0 so x = x_0 and in addition as G is compact,
    the limit point must be in the graph so y_0 = f(x).

    Thus, actually f(x_n_k_j) -> f(x) -> done.

** Problem 7

   I got this one right! Yaaaay

** Problem 11

   Show that f uniformly continuous implies that {f(x_n)} is cauchy.

   Let epsilon > 0. Show that there exists N s.t. n,m .GEQ. N implies that
   d(f(x_n),f(x_m)) < epsilon.

   as f is uniformly continuous we know that exists delta > 0 s.t.

   d(x,y) < delta -> d(f(x), f(y)) < epsilon for all epsilon.

   we also know that {x_n} cauchy -> exists N s.t. n,m .GEQ. N implies
   d(x_n,x_m) < delta. Turn the crank.


* Homework 12

  Due Friday, November 4 : page 100 15,17; page 114 1,2,12,13

** Number 15

   f :: RR -> RR continuous  open mapping

   we know that if U is open, f(U) is open, f monotonic.

   Show that f continuous, f not monotonic -> exists U open with f(U) not open.

*** Solution

    Show that f continuous and not monotonic implies that f not open.

    case 1 - f continuous on [x,z] -> f has a max on [x,z]

    Therefore there exists some x0 in [x,z] with f(x0) .GEQ. f(x) for all x in
    [x,z]. Therefore f(x0) is not an interior point, so the codomain is not open.

    case 3 - f has a minimum on [x,z]. Easy way out - replace f by -f.

** Number 17

   Set of discontinuities of the first kind for any RR is continuous.

*** Solution

    f :: (a,b) -> RR, E is the set of simple discontinuities for f.

    E = union of E1 , E2, E3, E4 where

    E1 is the set {x in E : f(x-) < f(x+)}
    E2 is the set {x in E : f(x-) > f(x+)}
    E3 is the set {x in E : f(x-) = f(x+) < f(x)}
    E4 is the set {x in E : f(x-) = f(x+) > f(x)}

    Lazy reduction - replace f by -f to reduce E2 to E1, E4 to E3. Therefore we
    only need to do two cases.

    x in E1 -> (p,q,r) in QQ^3, so we get the three properties:

    1. f(x-) < p < f(x+)
    2. a < p < t < x -> f(t) < p
    3. x < t < q < b -> f(t) > p.

    For E1 : pick x < y but they have the same rational triple. Then f(t) > p
    and f(t) < p, which doesn't work -> one tripple per number.

    For E3 :

    1. f(x+) < p < f(x)
    2. a < q < t < x -> f(t) < p
    3. x < t < r < b -> f(t) < p

    use a similar trick - fx) > p, f(y) > p, but we can show that f(x) < p,
    f(y) < p

** Number 13

   f(0) = 0
   f(x) = x^a sin(1/x^c)

   Examine what happens for different values of c and a. We did c = 1, a = 0,
   1, 2.

** Number 1 (114)

   We want to do this one in a polished way. We can use the sandwich theorem.

   abs((f(x) - f(y))/(x - y)) .LEQ. abs(x - y) -> f'(y) exists, equals 0 for
   all x -> f constant.

   Therefore, by the hypothesis : - abs(x - y) .LEQ. (f(x) - f(y))/(x - y)
   .LEQ. abs(x - y)

   so we can force lim sup and lim inf to equal zero. Then we are done.

** Number 2 (114)

   f'(x) > 0 on (a,b) -> (by MVT) (f(y) - f(x)) = f'(c)(y - x)

   By assumption, the derivative is strictly positive, so for y > x f(y) >
   f(x). Done.

   To find g = f^-1 and its derivative, we must show that g is differentiable.

   How do we know that g is continuous?

*** High-Brow Proof

    Suppose that x0 in (a,b). Choose a < alpha < beta < b so [alpha,beta]
    compact implies that f continuous on [alpha, beta] -> f([alpha, beta])
    continuous and g = f^-1 is continuous.

*** Low-Brow Proof

    Use deltas and epsilons. It is ugly.


* Derivatives.

  given some $f :: [a,b] -> RR$

  $$f'(x) = lim (t -> x) (f(t) - f(x))/(t - x)$$

  Not defined for $t = x$, really means:

  $\lim (t -> x) phi_x(t)$, so we essentially have a fixed $x$.

** Continuity Properties

   f'(a) < L < f'(b) -> exists c s.t. f'(c) = L.

   Compare this to the intermediate value theorem -
   (take any connected topological space, continuous image of connected set is
   connected.)

   (image of an interval is an interval - any two points in the image, whatever
   inbetween is also in the image.)

   If f' exists everywhere, then discontinuities of f' are necessarily of the
   second kind (if not, then we would violate the mean value theorem)

*** Proof

    Given f (as in intermediate value theorem for derivatives) then set

    h(t) = f(t) - L*t.

    Then h'(a) = f'(a) - L < 0 where h'(a) = lim (t -> a) (h(t) - h(a))/(t - a)
    implies that there exists some t1 where h(t1) < h(a). (numerator negative
    once we are close enough). Similarly, h'(b) = f'(b) - L > 0.

    Therefore there exists some t2 in (a,b) with h(t2) < h(b). h is continuous
    on [a,b] -> h achieves a minimum on [a,b] (continuous image of compact is
    compact) and the point at which h is minimum cannot be a or b.

    Therefore h has some interior minimum, and h is differentiable on [a,b] ->
    (Fermat's Theorem) then h'(c) is zero, c in (a,b), so f'(c) = L.

** Another definition

   Given some epsilon > 0 , exists delta s.t.

   t in [a,b]\{x} and abs(t - x) < delta implies

   abs((f(t) - f(x))/(t - x) - f'(x)) < epsilon. This is also defined at the
   endpoints of the domain (a and b)


** Theorem (Differentiation and Continuity)

   if f is defined on [a,b], f differentiable at x -> f continuous at x.

*** Proof

    t - x goes to 0, and the derivative goes to a real number. Therefore
    f'(x) * (t - x) goes to 0, so lim (t -> x) abs(f(t) - f(x)) -> 0.

*** Converse - not true.

    example - f(x) = abs(x) is not differentiable at x = 0, but it is continuous.

*** Another fun counter-example to the converse.

    f 0 = 0
    f x = x * sin (1/x)

    f is continuous at all x /= 0. What happens at 0?

    Check at zero: lim (t -> 0) f(t) = lim t * sin 1/t

    t goes to zero and sin 1/t is bounded -> f(t) goes to zero.

    Check the derivative at zero - we secretly already know the chain rule and
    may compute

    f'(x) = sin 1/x - 1/x cos 1/x , where 1/x cos 1/x is very badly behaved
    near zero.

    Check by definition:

    lim (t -> 0) (f(t) - f(0))/ (t - 0) = lim (t -> 0) t*sin(1/t) / t = sin(1/t)

    this has a lim sup of 1 and lim inf of -1 -> limit does not exist.

    Therefore this is continuous but not differentiable at x = 0.

*** One more counter-example.

    f(x) = x^2 sin(1/x)

    f is continuous for all x. However we get

    f'(x) = 2*x*sin(1/x) - cos(1/x)

    cos(1/x) oscillates wildly, so the limit does not exist.

*** Weirstaruss' Monster

    function continuous but 'nowhere differentiable'


** Calculus Rules

   (f + g)'(x) = f'(x) + g'(x)

   (fg)'(x) = f'(x)g(x) + f(x)g'(x)

   (f/g)'(x) = ...

   Direct calculus - d/dx x*x = 2x. We can use this inductively to get d/dx
   x^n = n x^(n-1).

*** Chain rule

    f continuous on [a,b], f'(x) exists at some x in [a,b], g defined on some
    interval that is a subset of the range of f (so g . h is defined)

    g differentiable at f(x) -> h'(x) = g'(f(x)) f'(x)


**** Proof

     'careless proof'

     (g . f t - g . f x) / (t - x)
     = (g(f(t)) - g(f(x)))/(f(t) - f(x)) * (f(t) - f(x)) / (t - x)

     -> g'(f(x)) f'(x)

     This is bad because if f is a constant function we divide by zero. This
     proof is bogus.

     The fix: keep track of error functions.

     write out f(t) - f(x) = (t - x) (f'(x) + u(t)) where u(t) -> 0 as t -> x
     (error term)

     For y = f(x) : in general,

     g(s) - g(y) = (s - y) (g'(y) + u(s)) where u(s) -> 0 as s -> y.

     Let s = f(t). Then h(t) - h(v) = g(f(t)) - g(f(x))

     = (f(t) - f(x)) ( g'(f(x)) + u(f(t)))

     = (t - x) (f'(x) + u(t)) (g'(f(x)) + u(f(t)))

     which implies that

     (g(f(t)) - g(f(x))) / (t - x) =

     (f'(x) + u(t))(g'(f(x)) + u(f(t))) -> f'(x)g'(f(x)) woohoo


* Mean Value Theorem

  We can get this from Rolle's Theorem.

  Recall : f :: [a,b] -> RR, , x in [a,b]
  f differentiable at x -> lim (t -> x) phi(t) = L = f'(x) exists. The limit
  itself is defined on [a,b]\{x}.

** Local Maximum

   f : X -> RR has a local minimum at a point p in X if exists delta > 0 s.t.

   q in X with d(q,p) < delta -> f(q) .LEQ. f(p)

** Fermat's Theorem

   f defined on [a,b], f has a local maximum at some point x in (a,b), f'(x)
   exists -> f'(x) = 0

*** Proof by difference quotient

    t in N_delta(x) where approach from left, derivative .GEQ. 0, approach
    from right, derivative .LEQ. 0. Therefore by uniqueness of the limit we
    get that it is zero, or f'(x) = 0.

    Note that this arguement fails at end points - only applies to open
    intervals.

** Traditional Approach

   We know that f is continuous and differentiable on [a,b], so there exists
   some c in (a,b) where f(b) - f(a) = f'(c) (b - a).

   Remark - in case f(a) = f(b) = 0 we get Rolle's Theorem.

*** Proof

    Reduce to Rolle's Theorem by introducing some auxillary function g, where
    g(x) = f(x) - value on the secant line
         = f(x) - point-slope formula between (a,f(a)) and (b,f(b)).

    Then we can almost keep Rolle's Theorem. g continuous on [a,b],
    differentiable on (a,b), g(a) = g(b) = 0 (Rolle's!). Therefore there is
    some c in (a,b) where g'(c) = 0

    however, g'(x) = f'(x) - (f(b) - f(a))/(b - a)
                 0 = f'(c) - (f(a) - f(b))/(b - a) Done.

**** Corollary - increasing stuff.

     f'(x) .GEQ. 0 implies that f is increasing.
     f'(x) > 0 implies that f is strictly increasing.

     f'(x) .LEQ. 0 implies that f is decreasing.
     f'(x) < 0 implies that f is strictly decreasing.

     f'(x) = 0 -> f is a constant.

     These are intuitive, but now we may rigorously show that they are true.

** General Case

   f,g continuous on [a,b] and differentiable on (a,b) implies that there
   exists some c in (a,b) such that

   (f(b) - f(a))g'(c) = (g(b) - g(a))f'(c)

   if g(b) /= g(a) and g'(c) /= 0 we can rewrite this as

   f'(c) / g'(c) = (f(b) - f(a)) / (b - a)

   where a special case (g(x) = x) is the usual mean value theorem.

*** Proof

    Recall that h~(x) = f(x) - (f(a) + (f(b) - f(a)) / (b - a) * (x - a))

    h(x) = h~(x) (b - a)
         = (b - a) f(x) - (b - a)*f(a) + (f(b) - f(a))*(f(b) - f(a))(x - a)

         = (g(b) - g(a))*(f(x) - f(a)) - (f(b) - f(a))(g(x) - g(a))

    Therefore h(x) is continuous on [a,b] (composition of continuous
    functions)

    h(a) = 0 by substitution, and there exists some c s.t. h'(c) = 0, so we
    get what we wanted.

** Standard MVT

   g(x) := x. We can say that the slope of the tangent line matches the slope
   of the secant line in at least one place.

   Consider t -> (g(t), f(t)) as a parameterized curve, mapping [a,b] -> RR^2.
   Assume that this parameterization has no singularities. This means that the
   tangent vector (g'(t), f'(t)) exists and is not the zero vector
   everywhere. Then the slope of the tangent line is (back to calculus of
   several variables) at (f(x), g(x)) is

   (g(x), f(x)) + t(g'(x), f'(x))

   then the slope of the tangent line at (g(x), f(x)) is f'(x) / g'(x) and we
   assume that both are not zero at the same time. Therefore we can create
   some well defined slope.

   Assume taht the secant line is well-defined. This means that

   (g(b),f(b)) /= (g(a),f(a))

   and the geometric interpretation is just the general MVT, that is

   slope of tangent line at c = (f(b) - f(a))/(g(b) - g(a)) = f'(c)/g'(c)

** What makes the Generalized MVT more general?

   We have two functions (f and g) satisfying the hypotheses.

   f(b) - f(a) = f'(c_f) (b - a)
   g(b) - g(a) = g'(c_g) (b - a)


   If it is possible to take c_f = c_g then we recover the other versions of
   the Mean Value Theorem.

** Speculation : Vector MVT

   This is not true! We will shoot down this conjecture shortly.

   So that f :: [a,b] -> RR^k is continuous and differentiable on (a,b).

   Then we can say f'(x) = [f1'(x), f2'(x), ...]

   and that this equals lim (y -> x) 1/(y - x) (f(y) - f(x))

   Therefore there should exist some point in (a,b) where

   f(b) - f(a) = f'(c) (b - a)

   What happens when we take this appart component-wise?

   f1(b) - f1(a) = f1'(c) (b - a)
   f2(b) - f2(a) = f2'(c) (b - a)
   ...

   This is the same as the system of scalar equations

   f_j(b) - f_j(a) = f_j'(c) (b - a)

   where c is independent of j. We can certainly do this individually.

   Counter-Example: f(t) = [cos t, sin t] :: [0,2Pi] -> RR

   Then f(0) = (1,0), yet norm(f'(t)) = 1

   but f(2pi) - f(0) = f'(c)(2pi)

   f(2pi) - f(0) is zero, but f'(c) is not. Therefore the conjecture fails.

   New conjecture - MVT fails for complex because it fails for RR^2.

*** Theorem 5.19

    f is a continuous mapping of [a,b] into RR^k and f is differentiable on
    (a,b) implies that there exists some point (a single point!) in (a,b)
    where the estimate

    norm(f(b) - f(a)) .LEQ. (b - a) norm(f'(c))

**** Proof - By 4226

     We secretly already know how to integrate.

     norm(f(h) - f(a)) = norm(integral (0 to 1) d/dt f((1 - t)*a + th) dt)
     crash through the integral:


     .LEQ. integral norm(f'((1-t)a + th)) dt (b - a)
     we can take the norm and bound and get

     .LEQ. sup(norm(f'(t))) (b - a)

** L'Hopital's Rule

   consequence of the general case of the Mean Value Theorem. We can now
   manipulate limits as they go to infinity.

*** Fixing L'Hospital for complex numbers

    Apply the good L'Hospital to the real and imaginary parts separately. To
    finish we need a clever identity:

    f(x)/g(x) = (f(x)/ x - A) * x/g(x) + A*x/g(x)

*** Fixing L'Hospital for vector functions

    We fixed for complex, how about vector?

    By MVT - f(b) - f(a) = f'(c) (b - a)

    Useful corollary - abs(f(b) - f(a)) = abs(f'(c)) abs(b - a)

    Therefore, if we bound the derivative above:

    abs(f(b) - f(a)) sup (t in (a,b)) abs(f'(t)) (b - a)

** Definition

    f, g real and differentiable on (a,b), g'(x) /= 0. Suppose that

    limit (x -> a+) f'(x)/g'(x) = A.

    Then if f(x) -> 0, g(x) -> 0 (or inf, or -inf) as x -> a+, then

    limit (x -> a+) f(x)/g(x) = A.

*** Proof

    Given A in closure RR, and A- < A < A+ (if A = -inf then A- does not
    exist) (if A = +inf then A+ does not exist)

    Then there should eist some a-, a+ in the closure with a- < a < a+ (same
    rules; if a = +/- inf then forget about the appropriate things) so that x
    in domain(h) with a- < x < a+ -> A- < h(x) < A+.

    Therefore, if A is finite, then take some A- = A - epsilon, A+ = A +
    epsilon.
    If A = +inf then pick A- = M in RR, ignore A+.
    If A = -inf then pick A+ = M in RR, ignore A-.

    and, if a in RR, pick a- = a - delta, a+ = a + delta. Do similar ignoring
    for a = +/- inf.

    This should be all the set up that we need.Assume that lim (x -> a+)
    f'(x)/g'(x) = A. Show that lim (x -> a+) f(x)/g(x) = A.

    How we have A- < A < A+, choose real p, r where A- < p < A < r < A+.

    Assume that there is some c in (a,b) with a < x < c. Then

    p < f'(x) / g'(x) < r.

    For any x,y with a < x < y < c, the generalized mean value theorem gives
    that

    (f(x) - f(y)) g'(t_x,y) = (g(x) - g(y))f'(x)

    (recall - we assumed that g'(x) /= 0)

    Therefore (g(y) - g(x)) /= 0, so

    p < (f(x) - f(y))/(g(x) - g(y)) = f'(t)/g'(t) < r

    FInally, let x -> a+, so

    A- < p .LEQ. f(y)/f(x) .LEQ. r < A+

    Therefore, given some A- and A+, we found some c > a where if y in (a,c)
    then A < f(y) / g(y) < A+ -> definition of lim (y -> a) f(y)/g(y) = A.

**** case b : lim (x -> a+) f'(x)/g'(x) = A and g(x) -> +inf.

     Proof - Choose some A-, A+ in RR (if possible) with

     A- < A < A+ . Choose some p,r in R, where (again)

     A- < p < A < r < A+

     Same work as before holds. We can form some t between x and y such that

     p < (f(x) - f(y))/(g(x) - g(y)) = f'(t)/g'(t) < r (due to the
     hypothesis/generalized mean value theorem).

     Keep y fixed - choose some c1 in (a, y) s.t. g(x) > g(y) and g(x) > 0 (as
     we assume g(x) -> inf) such that if a < x < c1 then g(x) > 0.

     Multiply the inequality by (g(x) - g(y)) / x > 0, so we get

     (f(x) - f(y)) / (g(x) - g(y)) (g(x) - g(y))/g(x) = f(x) / g(x) - f(y) /
     g(x)

     hmm, what do we do with that?

     p - p(g(y)/ g(x)) = p (g(x) - g(y))/ g(x) < f(x) / g(x) - f(y) / g(x)

     < r (g(x) - g(y))/g(x) = r - r(g(y) / g(x))

     Therefore p - p g(y) / g(x) + f(y) / g(x) < f(x) / g(x) < r = r g(y) /
     g(x) + f(y) / g(x).

     Let x -> a+. Then there exists some c2 in (a,c1) such that

     a < x < c2 -> A- < f(x)/g(x) < A+ Done.

** Rolle's Theorem

   f continuous on [a,b], f differentiable on (a,b), f(a) = f(b) = 0, then
   exists c in (a,b) where f'(c) = 0.

*** Proof

    Case 1 : if f is a constant then f'(c) = 0 for all c. Done.

    Case 2 : exists x in (a,b) with f(x) > 0. We know that f(compact) is a
    compact set, so f achieves a maximum value on some [a,b]. Therefore, as
    there exists some x in (a,b), f(x) > f(a), f(b) = 0, implies that the
    maximum is at an interior point.

    Then by Fermat's theorem we get that the max at the interior has f'(c) = 0.

    case 3 : exists x in (a,b) where f(x) < 0 -> f has some interior minimum at
    c. The same story as above applies, and we get f'(c) = 0.

*** Applications

    Use it to derive the Mean Value theorem.

** Vector Functions

*** Theorem 5.19

    f : [a,b] -> RR^k
    norm(f(a) - f(b)) .LEQ. (sup norm(f'(t)))(b - a)

    Proof - 'crash through with integrals'

    Refinement - Actually exists some c between a and b such that
    norm(f'(b) - f'(a)) .LEQ. norm(f'(c)) (b - a)


**** Proof

     z in RR^k. Then Phi(x) = z cdot f(x) is a scalar function.

     Therefore, by the mean value theorem, exists some c (dependent on z) in
     (a,b) s.t.

     Phi(b) - Phi(a) = Phi'(c) (b - a)
     z cdot f(b) - z cdot f(a) = z cdot f'(c) (b - a)
     z cdot (f(b) - f(a)) = z cdot f'(c) (b - a)

     Choose z = 1/(norm (f(b) - f(a))) (f(b) - f(a))

     then norm(f(b) - f(a)) = z0 cdot f'(c) (f(b) - f(a))

     so by C-S we get norm(f(b) - f(a)) = norm(z_0) norm(f'(c)) (b - a). Done.


* Something.

  Let x in (bar E) \ E `subset` E'

  Choose any {x_n} in E with x_n -> x.

  Define g(x) = if x in E then lim(f(x_n)) else f(x).

** Check sequence well-defined

   if {y_n} in E and y_n -> x:

   then some z_n = if n odd then x_n else y_n -> x then {z_n} cauchy -> f({z_n})
   cauchy -> f(z_n) -> L so f(x_n) and f(y_n) go to the same L. Therefore we can
   make conclusions for any arbitrary sequence.


** Is g continuous?

   Suppose that x_n -> x in X = closure E.

   Pick some e_n in E, where d(e_n, x_n) < 1/n (assumed density)

   Estimate - d(g(x_n), g(x)) .LEQ. d(g(x_n), g(e_n)) + d(g(e_n), g(x))

   therefore d(g(e_n), g(x)) goes to zero by construction of the g formula.

   The first part d(g(x_n), g(e_n)) goes to 1/n by construction. Therefore
   d(g(x_n),g(x)) -> 0 and we are done.


* Homework 13

  Due 11/11/11

  114 - 3,4,5,6,8,11 (on the mean value theorem)

** Problem 8

   f continuous on [a,b], epsilon > 0 -> uniform continuity of f'.

   use the mean value theorem.

   as f' is continuous on [a,b] we can use

   abs(t - x) < delta -> abs(f'(t) - f'(x)) < epsilon

   therefore we can use MVT -

   f(t) - f(x) / (t - x) = f'(c), then done.

*** for vector valued functions

    Suspect no. f(t) = (cos(t), sin(t)) is a counterexample to the MVT and we
    used the MVT. However, we also have the component-wise MVT:

    (F(t) - F(x))/(t - x) = (f1(c1), f2(c2), ...) for ci between x and t.

    Choose some delta > 0 s.t.
    abs(t - x) < delta -> abs(f1'(t) - f1'(x)) < epsilon/sqrt(k)

    Then norm((f(t) - f(x))/(t-x) - f'(x)) < epsilon. Therefore the component
    wise MVT was enough for this to hold true.

** The Finite Difference Problem

   if the second derivative exists then
   lim (h -> 0) (f(x + h) - 2f(x) + f(x - h))/h^2 = f''(x)

   By L'Hospital - differentiate top and bottom with respect to h.

   False way: Then do L'Hospital again with respect to h:

   (f''(x + h) + f''(x - h))/2. Valid if we can assume f'' continuous, but we
   don't know that!

   Better way: rewrite as

   1/2 * (lim (h -> 0) (f'(x + h) - f'(x))/h + (f'(x - h) - f'(x))/(-h))

   which is the average of the left and right first derivatives (which we know
   are equal by assumption). Done.


* Extra Credit

  114 20,22

** 20 - Q /= E `subset` metric space X, define P_E(x) by inf (p in E) d(x,z)

   and p_E(x) = 0 <-> x in E closure.

   b. x -> P_E(x) is uniformly continuous (Lipschitz with L = 1)

   so abs(P_E(x) - P_E(y)) .LEQ. d(x,y)

   and P_E(x) .LEQ. d(x,z) .LEQ. d(x,y) + d(y,z) -> P_E(x) .LEQ. d(x,y) + P_E(y)

   which implies that p_E(x) - p_E(y) .LEQ. d(x,y)

   similarly, p_E(y) - p_E(x) .LEQ. d(y,x) = d(x,y)

   so abs(P_E(x) - P_E(y)) .LEQ. d(x,y) which implies uniform continuity.

** 22 - A, Bdisjoint and closed in X

   define f(p) = P_A(P) / (P_A(p) + P_B(p)) -> f is continuous on X with range
   f in [0,1)

   p -> f(p) continuous, it follows from the calculus theorems

   P_A(p) + P_B(p) /= 0 for all p.

   p in (closure A) Union (closure B) <-> A intersect B = nullset.

   f^{-1}({0}) = A, f^(-1)({1}) = B. Done.


* What is on Test 2?

** Rudin

   Chapter 2 (connectedness)
   Chapters 3,4,5 (excluding Taylor's Theorem)

** Emphasis for Test

   Theorems and Definitions from the text. Know the tricks Rudin uses.

   Dr. Ball says 'convince me it was too easy!'

** Topics

   Limits of sequences, functions
   Lim Inf and Lim Sup (they always exist)

   Existence results - a lot about infinite series'. (summation bounded,
   greater than 0 -> sums convergent)

   Monotone convergence - from completeness.

   Monotone convergence theorem -> comparison test -> root test -> ratio test
   (recomendation - fill in the proof for any of those)
   'facility with the language of calculus'

   series of functions (power series) (more on that next semester)

   uniform continuity - f continuous on some compact X -> f uniformly
   continuous (we just used this in the last homework)

   monotonic functions - 1-sided limits, f(x +/-) always exists,
   discontinuities (monotonic -> all discontinuities are of the first kind)

   discontinuities - derivatives may have discontinuities not of the first
   kind.

*** Differentiation

    Fermat's, Rolles', MV, Generalized MV, L'Hospital's, vector theorems.

    be able to prove Fermat and Rolles'


* Test 2 Overview

** Overview

   How do we know that things exist? Completeness of reals, LUB, monotone
   convergence.

** Problem 1

   a - monotone convergence theorem
   b - Cauchy criterion
   c - apply a and b.
   d - apply c with b_n = r^m where a_m+1/a_m < r < 1.

** Problem 2

   a - f uniformly continuous = f continuous at all p.
   b - negate the definition -
       exists epsilon s.t. forall delta, exist p,q where d(p,q) < delta but
       d(f(p),f(q)) > epsilon
       (apply with delta = 1/n for any n, so the distance goes to zero while
       f(p) - f(q) > epsilon)
   c - f(x) = x^2 - see part one - apply b with something that goes to
       infinity. part two - f(x) = x is uniformly continuous on [-M,M] forall
       M. Quote the theorem, continuous on compact -> uniformly continuous.

   b - f'(x) = 2x .LEQ. 2M -> f(y) - f(x) < 2M (y - x) for x, y in [-M,M] so we
       get a Lipschitz contant of 2M.
   c - Elementary proof : f(x) - f(y) = x^2 - y^2 = (x+y)(x-y)

** Problem 3

   a - Use the definition of a limit. Alternatively - use the sequential
       characterization.

   b - left out - y_n -> x (need to assume this). for 1 -> 2
       straightforward. for 2-> use contrapositive.

   c - Parallel at the lim sup level - we can connect the functional and
       sequential with inf sup.

       notation : A = lim (y -> x) sup f(y)
       and B = sup {lim sup f(y_n), y_n -> x}

       step 1 - argue that B .LEQ. A.

       forall epsilon > 0, exists delta s.t. 0 < d(x,y) < delta so f(y) < A +
       epsilon, or for n > some N, d(y_n, x) < delta so f(y_n) < A + epsilon so
       lim sup of the sequence is .LEQ. A.

       step 2 - exists a subsequence , {y_n} -> x and f(y_n) -> A. Then A
       .LEQ. B and sup = max. Then 'the delta takes you on in'


* Homework 14

  One more homework - number 14.

  For Friday, December 2, Page 115 15,17.


* More Series Stuff

  Indexing over multiple indicies.


* Taylor Series

** Motivation

   P(x) is a polynomial:

   SUM (k=0 to n) a_k x^k polynomial of degree n.

   We may recover the coefficients by calculating the derivative of zero
   divided by the factorial of the power. Therefore, we can rewrite the
   original:

   P(x) = SUM (k=0 to n) P^k(0)/k! x^k
   More generally - if P~(x) = SUM (k=0 to n) P^k(x0)/k! (x - x0)^k

   then P~(x) = P(x).

** Expansion to arbitrary functions

   Suppose f :: [a,b] -> RR has n continuous derivatives on [a,b] and x0 in
   [a,b]. Then

   P_n, f, x0(x) = SUM (k = 0 to n) f^(k)(x0)/k! (x - x0)^k

   therefore we recover each derivative of f(x0).

** When is this exact?

   Say that f(x) = SUM a_n (x - x_n)^n on (x0 - r, x0 + r) (for r > 0)

   and differentiable term-by-term is 'cool' (we will prove this next semester)
   then a_n = f^n(x0)/n!

*** Other direction

    When is f(x) = SUM f^n(x0)/n! (x - x_n)^n true everywhere?

    We can cook up examples, like f(x =< 0) = 0, f(x > 0) = e^(-1/x) is
    infinitely differentiable but the taylor series breaks down.

** Residuals

   Define R_n(x) = f(x) - SUM (k=0 to n) f^k(x0)/k! (x - x0)^k z (remainder)

   Then : f(x) = (infinite taylor series) is the same as R_n(x) -> 0.

*** Corollary

    Suppose f^n(x) =< M for all n and x in some neighborhood of x0. Then the
    function is equal to the infinite taylor series.

**** Proof

     Squash the f^n(x) terms by bounding them and using n!. One may use the
     ratio test to show convergence.

**** Example 1

     f(x) = e^x. For any interval we can bound the derivatives. The ratio test
     shows that the series converges; we must also show that it converges to
     the correct thing.

**** Example 2

     f(x) = cos(x); we may certainly bound the derivatives by 1. This Taylor
     series is nice because all the odd terms disappear (they are sines
     evaluated at zero).

*** Moral of the Story

    The neighborhood of convergence predicted by looking at the remainder is
    not always sharp.

**** Proof 1

     R_n(x) / x^(n+1) =
     (R_n(x) - R_n(0)) / (x^(n+1) - 0) = (R'_n(x1) - R_n'(0)) / ((n+1)x1^n - 0)
     = ... = R_n^(n+1)(c) / (n+1)!
     which implies that R_n(x) = f^(n+1) (c) / (n+1)! x^(n+1) (Lagrange form of
     the remainder)

**** Proof 2

     Define phi(t) =
     f(x) - SUM (k = 0 to n) f^k(t)/k! (x-t)^k - R_n(x) (x-t)^(n+1) /(x -
     x_c)^(n+1)

     Then phi is continuous on [a,b] and phi' exists. However, by design
     pho(x0) = phi(x1) = 0 so phi'(some c) = 0. Then we get the formula by
     magic.

** How good an approximation is this?

   Assume that f is n times continuously differentiable on the closed interval
   [a,b]. Assume that the n+1 derivative exists on (a,b). Let x0, x be distinct
   points in [a,b]. Then there exists some c in (x0,x) so we can use the
   Lagrange form of the remainder:

   f(x) = SUM (k = 0 to n) f^(k)(x0)/k! (x - x0)^k + f^(n+1)(c) / (n+1)! (x -
   x0)^(n+1)

   (note that n = 0 is the MVT)

*** Proof

    (Abbott) Consider R(x) := f(x) - P_n,f,0(x). THen R is a continuously
    differentiable function on [a,b] and R_n^k(0) = 0 for 0 .LEQ. k .LEQ. n. We
    may then apply the generalized mean value theorem and get

    R_n(x) / x^(n+1) = R'(x1)/((n+1)x1^n). We can do this again for x2 between
    x1 and 0, and keep going until the denominator is a constant.


* Convex functions

** Definition

   f :: (a,b) -> RR *convex* means that for all x,y in (a,b),
   and 0 < lambda < 1,

   f(lambda x + (1 - lambda) y) .LEQ. lambda f(x) + (1 - lambda) f(y)

   esentially: the secant line is above the graph.

** Properties

*** f convex -> f continuous.

    This is only valid at interior points.

**** Proof

     Say a < s < u < v < t < b. Then

     1. f(u) .LEQ. f(s) + (f(v) - f(s))/(v - s) * (u - s) (solve for f(v))

     2. f(v) .LEQ. f(u) (f(t) - f(u))/(t - u)

***** Squeeze from Left

      Therefore, solving for f(v) we can get

      (v - s)/(u - s) f(u) - (v - s)/(u - s) f(s)       .LEQ. f(v) - f(s)
      and
      (v - s)/(u - s) f(u) + (1 - (v - s)/(u - s))f(s)  .LEQ. f(v)
      and
      f(s) + (f(u) - f(s))/(u - s) * (v - s)            .LEQ. f(v)
      therefore
      f(s) + (f(u) - f(s))/(u - s) * (v - s)            .LEQ. f(v)
      .LEQ. f(u) + (f(t) - f(u))/(t - u) (v - u)

      so we can squeeze f(u) .LEQ. lim inf f(v_n) .LEQ. lim sup f(v_n)
      .LEQ. f(u)

      so the liminf and limsup are equal, so we get continuity.

***** Remaining parts

      Squeeze from right - a similar process.

      By picture - we can see that f(u) is less than the value at u on the
      secant line connecting s and v. We can check via algebra.

      Similarly:

      f(v) .LEQ. f(u) + (f(t) - f(u))/(t - u) (v - u)

*** g increasing, g convex, f convex -> g . f convex

**** Proof

     To show : g . f (lambda x + (1 - lambda )y) .LEQ.
     lambda g . f (x) + (1 - lambda) g . f (y)

     Therefore, as g is increasing and f has the convexity property with
     lambdas, we are done by substituting in.

*** Slopes of Secant Lines increase.

    f convex on (a,b), a < s < t < u < b
    Then (f(t) - f(s))/(t - s) .LEQ. (f(u) - f(s))/(u - s)
    .LEQ. (f(u) - f(t))/(u-t)

    (the slopes are increasing.)

**** Proof

     Use the point-slope formula.

     (value less than or equal to value of secant line)
     f(t) .LEQ. f(s) + (f(u) - f(s)).(u - s) * (t - s)

     there is the first one! We just need to rearrange.

     Similarly,
     f(t) .LEQ. f(u) + (f(u) - f(s))/(u - s) * (t - u)
     therefore, by rearrangement,
     (f(u) - f(s))/(u - s) .LEQ. (f(u) - f(t))/(u - t)

*** The secant line is above, but the tangent line is below.

    More exactly: the secant line connecting two points is always above the
    graph, while the tangent line on one point is always lower than the graph.

**** Proof

     Assume that x0 < x < x1 and f is convex. We want to show that

     f(x0) .GEQ. f() + f'(y) * (x0 - y)
     f(x) .GEQ. f() + f'(y) * (x - y)

     Choose lambda where

     x = (1 - lambda) * x0 + lambda * x1

     then 0 = (1 - lambda) * x0 - (1 - lambda) * y + lambda * x - lambda * y

     therefore (1 - lambda) * f(x0) + lambda * f(x) .GEQ. f(y) +
     f'(y) ((1 - lambda) * (x0 - y) + lambda * (x - y)) (last term goes to
     zero)

     so (1 - lambda) * f(x0) + lambda * f(x) .GEQ. f(y) done!

*** Choices for lambda

    for continuous f, f((x + y)/2) .LEQ. (f(x) + f(y))/2, (ie lambda = 1/2)
    then we may choose any 0 < lambda < 1 and f is convex.

**** Proof (by continuity)

     it suffices to verify that
     f(lambda x + (1 - lambda) y) .LEQ. the usual only for

     lambda = m / 2^n (for 0 < m < 2^n) (the diadic rationals)

     Take n = 2 - the algebra works out well for

     f(1/4 x + 3/4 y) .LEQ. 1/4 f(x) + 3/4 f(y). (this is the base case; n = 1
     and n = 2 both hold so far).

** Rudin Problems

   101 23,24 and 115 14.

** What may we do when we know derivatives?

*** f differentiable, convex -> f' increasing

**** Proof

     (f(t) - f(s))/(t - s) .LEQ. what we did before.

     what we need to show is that the derivatives go to the secant lines when
     we smash the values (s,t,u) together.

     Let t decrease down to s. Then we get the derivative from the left side:

     f'(s) .LEQ. (f(u) - f(s))/(u - s).

     Let t go up to u in 2. Then

     (f(u) - f(s))/(u - s) .LEQ. f'(u)

     therefore f'(s) .LEQ. f'(u) (there is something in the way)

     Therefore, for f' increasing locally, by the mean value theorem

     (f(x) - f(x1))/(x - x1) = f'(xi1), and (f(x2) - f(x))/(x2 - x) = f'(xi2)
     for x1 < xi1 < x and x2 < xi2 < x. Therefore we have that

     (f(x) - f(x1))/(x - x1) .LEQ. (f(x2) - f(x))/(x2 - x)

     so convexity does imply that the derivative is monotonically increasing
     across the domain.

***** Other way : f' increasing implies convexity

      x := lambda x1 + (1 - lambda) x2 for x1 < x2, lambda in (0,1)

      then for some x in (x1, x2),

      x - x1 = (1 - lambda) (x2 - x1) , x2 - x = lambda (x2 - x1)

      Therefore, with some more rearrangement and collecting lambdas,

      f(x) .LEQ. lambda f(x1) + (1 - lambda) f(x2) so done.

*** Second Derivatives and Convexity

    Assume that f'' exists for all x in [a,b].
    Then f convex iff f''(x) .GEQ. 0.

**** Proof

     By a corollary to the MVT, f' increasing implies f'' .GEQ. 0. This is the
     end of the chain.


* Rudin's Lost Notebook - Even More Convex Fun (Derivatives)

** Derivatives

   f'^+(x) = LIM (h decrease to 0) (f(x+h) - f(x))/h

   not to be confused with
   f'(x+) = LIM (h decrease to 0) f'(x + h)

   From the previous work with secant lines - the derivative is monotonically
   increasing.

*** Proof

                           (1)                         (2)
    (f(t) - f(s))/(t - s) .LEQ. (f(u) - f(s))/(u - s) .LEQ. (f(u) - f(t))/(u-t)


    Let s go up to t in (1) and let u go down to t in (2). Then

    f'^(-)(t) .LEQ. f'^(+)(t)

    the same applies for s and u, so the derivative does indeed increase
    monotonically.

** Convexity and 1-sided derivatives

   f : (a,b) -> RR convex implies that one-sided derivatives of f exist on
   (a,b) and are monotone - f'^(+/-) exists for every x (but the + and - need
   not be equal everywhere) and

   x < y -> f'^(-)(x) .LEQ. f'^(+)(x) .LEQ. f'^(-)(y) .LEQ. f'^(+)(y)

*** Proof

    For our friend a < s < t < u < b, we have that

    (f(t) - f(s))/(t - s) .LEQ. (f(u) - f(s))/(u - s) .LEQ. (f(u) - f(t))/(u-t)

    Therefore LIM (x down to s) (f(x) - f(s))/(x - s) = f'^(+)(x) exists.

    Similarly: (f(u) - f(x))/(u - x) is increasing for x < u. Same technique :
    LIM (x up to u) (f(x) - f(u))(x - u) = sup (x < u) same deal = f'^(-)(u)
    exists.

    Then by the same arguement as before,
    f'^(+)(s) .LEQ. (f(u) - f(s))/(u - s) .LEQ. f'^(-)(u)
    Useful picture:  \_/ the tangent line may jump, but if u > s then the
    limits behave as we expect.

*** Corollary

    We can break f'^(+)(x) = f'^(-)(x) at countably many points at most.

*** Conjecture - building convex functions from monotone functions

    If g is a monotonically increasing function, then there is some convex
    function f with f' = g except at discontinuities of g.

** Convex : Sense 2

   Assume f has a continuous derivative on (a,b). Define : f is _convex in
   sense 2_ means

   f(x) .GEQ. f(x0) + f'(x0)(x - x0) for all x, x0 in f(0,h)

   Why? Think Taylor's Theorem - the second derivative is positive, so we get
   this by Taylor series and a bounded second derivative. Can we do it with-out
   the second derivative?

*** Proof with-out second derivative assumption

    f(x + h) = (f(x) + f'(x)h) = f(x + h) - f(x) - f'(x)h
                               = f'(c)h - f'(x)h
                               = (f'(c) - f'(x))h .GEQ. 0

    By a previous result, f' exists so f' must be increasing.

*** Third proof - direct.

    f(x + h) = f(x) + f'(x) h + z_x(h) * abs(h). By definition of the
    derivative, z_x(h) must go to zero as h -> 0. Therefore

    f(lambda y + (1 - lambda) y) = f(x + lambda(y - x))
    = f(x) + lambda f'(x) (y - x) + lambda*(y - x) * z(lambda (y - x))

    More work follows - we may cancel the lambdas and the the result.

*** Consequences of Convex 2

    Suppose f is convex and f' exists on some (a,b), f'(x0) = 0. Then, from
    calculus (we expect to see a minimum)

    f(x) .GEQ. f(x0) (so x0 is a *global* minimum)

    Proof : f(x) .GEQ. f(x0) + f'(x0) (x - x0). Done.

** Double Sequences and Series

   A *double sequence* is a function (N,N) -> RR^k, where
   (m,n) -> r, r in RR. We notate this by x_{m,n} and sometimes drop the comma.

*** Double Limit

    Define LIM (m,n -> (INF,INF)) x_{m,n} = L means

    for all epsilon > 0, exists N s.t. m, n .GEQ. N -> abs(x_mn - L) < epsilon.

**** Cauchy Criterion

     There exists L such that LIM (m,n -> (INF,INF)) x_{m,n} = L means the
     normal thing; but now we must use two variables.

**** Example

     f(x) = (x^2 sin(x) f(0) = 0) - exists everywhere but x = 0.

     If we let h go to zero and then x go to zero, it does not exist. However,
     if we fix h, evaluate the x limit, and then we can solve it (get f'(0),
     exists).

* Double Sums & Series'

** Theorems

*** Theorem 1 (for limits)

    LIM (m and n to inf) x_{mn} exists and LIM (n to inf) y_m exists for all m
    then x = LIM (m to inf) $ LIM (n to inf) x_{mn}

**** Proof

     Given epsilon > , exists N(epsilon) s.t.

     m, n .GEQ. N(epsilon) -> abs(x_mn - x) < epsilon.

     By hypothesis, y_m = LIM (over n) x_mn exists for all m. Then

     abs(y_m - x) .GEQ. epsilon for m .GEQ. N(epsilon) so x = LIM (m)
     y_m. Done.

**** Corollary

     LIM (m and n to inf) x_{mn} exists and LIM (n to inf) x_mn = y_m, LIM (m to
     inf) x_mn = z_n exists then x = any combination of two limits from before.

*** Theorem 2

    y_m = LIM (n to inf) x_mn, z_n = LIM (m to inf) x_mn exist and at least one
    is uniform then the double limit exists and is uniform.

**** Proof

     Assume that for all epsilon > 0, exists N(epsilon) where n
     .GEQ. N(epsilon) implies that abs(x_mn - y_m) < epsilon for all m.

     Then, by tripple-epsilon arguement:
     abs(y_r - y_s) .LEQ. abs(y_r - x_rq) + abs(x_q - x_sq) + abs(x_sq - y_s)

                    < epsilon + ? + epsilon.

     how do we get the last one? Use that {x_.q} is cauchy if we freeze
     q. Therefore there exists some R(q) (depends on q, but whatever) such that

     r, s > R(q) -> abs(x_nq - x_sq) < epsilon. Therefore we can bound all of
     the items in the tripple-epsilon arguement.

*** Theorem 3

    if SUM (i, j) x_ij is absolutely convergent then any double sum is also
    absolutely convergent.

** Examples

*** First example - derivative of our favorite function

    g(0) = 0, g(x) = x^2 sin(1/x)

    let x_mn = f(x_m, h_n) where f(x,h) = (g(x + h) - g(x)) / h

    Then the double limit LIM (m to inf) LIM (n to inf) x_mn does not
    exist. However,

*** Second example - attempt to modify Theorem 1

    x_mn = 1 for m /= n, 0 otherwise. This does not converge to anything, so
    the limit of the indicies does not exist. However, the single limits are
    okay (fix m, let n go on and vice versa)

*** Third example - double limit okay, single limits not

    x_mn = (-1)^(m + n) (1/m + 1/n)

    fix m. Then

    LIM (n to inf) x_mn = (-1)^m LIM (n to inf) ((-1)^n/m + (-1)^n/n )

    as m is fixed, this oscillates forever, so the single limits cannot converge.

*** Fourth example - lower triangular matricies

    x_mn = 1 for m .GEQ. n, 0 otherwise.

    then LIM (n to inf) for any m  = 1, but the other one goes to zero. Uh oh.

** Definitions

*** Double Sums

    Given some doubly-indexed sequence x_ij then we define the m,nth partial
    sum the normal way. We say that this converges to x in RR^k if

    forall epsilon > 0, exists M(epsilon) such that m, n .GEQ. M(epsilon)
    implies that abs(x - S_mn) < epsilon.

    put another way : LIM (m,n to inf) S_mn = x exists.

**** Absolute convergence

     Same as the 1D case; sum of absolute values converges.

**** Monotone Convergence Theorem

     Given some x_ij .GEQ. 0, then SUM x_ij converges implies there exists some
     M < inf such that S_mn .LEQ. M for all m, n (and then SUM x_ij = supremum
     of the partial sums)

*** Uniform convergence

    y_m = {x_mn}_n=1 to inf is a sequence in RR^k converging to y_m for each m.

    we say that {y_m} limits *exist uniformly* if for each epsilon > 0, exists
    N(epsilon) (independent of m) such that (n .GEQ. N(epsilon)) implies that
    abs(x_mn - y_m) < epsilon for all m.

* Final Exam, Part A

  Use completeness of the reals.

  m, n, r, s .GEQ. N implies abs(x_mn - x_rs) < epsilon

  Issue - iterated limit. Do we always get the same answer? May we flip the
  limits and do them in another order?

* Final Exam, In class

** What sort of functions are on the final exam?

   continuous, uniformly continuous, monotonic, convex, differentiable

   Given some equation, show that it is (one of those) and supply a
   theorem. Otherwise show that it is not (counter-example). Show plausible
   reasoning, not necessarily a proof

   *Techniques* weaken conclusion, less general assumptions.

** TODO What are the four important theorems? (know these!)

*** TODO we are missing one.

    There are only three! Find the last one.

*** Double limits: existence

    LIM x_mn = L and LIM (over n) x_mn = y_m and LIM (over m) x_mn = z_n
    all exist -> LIM (over n) LIM (over m) = LIM (over m) LIM (over n) = L

*** Double Limits: uniform continuity

    y_m = LIM (over n) x_mn, z_n = LIM (over m) x_mn exist (and at least one is
    uniformly continuous) then the double limit is uniformly continuous.

*** Double Sums and Absolute Convergence

    if SUM x_ij converges absolutely then SUM (over i) SUM (over j) = SUM (over
    j) SUM (over i)

**** Proof

     The double sum convergent means that there exists some N(epsilon) such
     that for all epsilon, m, n .GEQ. N(epsilon) implies

     abs(SUM (i=1 to m) SUM (j = 1 to n) x_ij - s) < epsilon

     We have absolute convergence, so we have convergence.

     Let t = sum (m,n) SUM (i=1 to m) SUM (j=1 to n) abs(x_ij) works as

     t = SUM (i,j = 1 to INF) abs(x_ij) exists by assumption.

     *Proof of the 3rd one* By hypothesis, exists A = upper bound over the
     double summation of absolute values.

     {SUM (i=1 to m) SUM (j=1 to n) x_ij}

     Therefore for some fixed n, the summation is less than or equal to A.

     as SUM abs(x_in) .LEQ. SUM SUM abs(x_ij) .LEQ. A (monotonically increasing
     and bounded implies convergence)

     Therefore the sum for fixed n converges to y_n absolutely.

     If epsilon > 0 then let some M(epsilon) be such that

     m, n .GEQ. M(epsilon) -> abs(s_mm - x) < epsilon

     whese s_mn = SUM (i = 1 to m) x_i1 + SUM (i = 1 to m) x_i2 + ...

     Therefore LIM (m to inf) = SUM x_i1 + SUM x_i2 + ...
     = y_1 + y_2 + ...

     Then pass to the limit in abs(s_mn - x) < epsilon:

     abs( SUM y_j - x) < epsilon when n .GEQ. M(epsilon) so x works as a limit
     for SUM over second index (SUM over first index).

*** Cauchy Sums

    Given some x_ij, look at

    SUM (over n) (SUM (over k) x_k,n-k) (diagonals)

    we can look at a double summation, were we just add up all the indicies we
    can think of.

    Iterated : sum each row. What happens when we get a lot of rows?

    Reverse order : sum the columns.

    Cauchy : travel along antidiagonals (from top to left)

*** If SUM x_ij converges absolutely to S then the cauchy sum converges to the same S.

    *Remark* we did this pre-thanksgiving ("before turkey") for the special
    case x_ij = a_i b_j.

    SUM (over i) (SUM (over j) a_ib_j) = SUM (over i) a_i (SUM (over j) b_j) =
    AB

    and by symmetry this works in the other order for *seperable*,
    doubly-indexed sequences. We can still play funny games with separation,
    but here it works.

*** Fun Theorem (appetizer)

    SUM x_ij converges absolutely implies that LIM (n to inf) S_mn = y_m exists
    uniformly on m and LIM (m to inf) s_mn = z_n exists uniformly in n.
