* General Idea
  We define things related to integrals in three sages:
  1. integrals of positive simple functions.
  2. use simple functions to construct positive measurable functions.
  3. use positive measurable functions to get negative measurable functions.
* Midterm
** Problem 1
   Based on theorem 1.11.

   To show that they form a sigma algebra:

       Bn = UNION (1,n) Aj and B = UNION (i,inf) Aj

       mu*(E) = mu*(E intersect Bn) + mu*(E intersect Bn^c)
       mu*(E) = SUM (1,n) mu*(E intersect Aj) + mu*(E intersect Aj^c)

       taking the limit as n -> inf we get what we want (with inequalities).
** Problem 2
   Based on theorem 1.21.

   We want to show that m(E) = m(E + x) for all Borel sets. Easy to check for
   h-intervals (h-intervals form an algebra). The next part is more difficult:

       if u,v agree on EE, EE generates MM, then do u,v agree on MM?

   not necessarily. Consider u = m, u = 2*m. These agree on the generating set
   m(-inf, a). It _does_ work if EE is an algebra. We need so say that it is an
   algebra.

   Afterwards appeal to 1.14 or 1.16.

   For part b: also appeal to the book. We have shown that this works for Borel
   sets (we want to show that it works for Lebesgue-measurable sets). By theorem
   we can write any Lebesgue-measurable set as a Borel set union null set.
* Homework
** Review of Homework 4
   E in LL, m(E) > 0, alpha < 1; show there is an open I with

       m(E intersect I) > alpha m(I).

   If m(E) = inf then pick some E~ subset E that has finite measure to work with
   instead. Then

       m(E) > m(E~)

   so we can get rid of the infinite case. Therefore assume that m(E) <
   inf. Assume that

       m(E intersect I) .LEQ. alpha*m(I)

   for all open intervals I. If A = UNION In is a disjoint union of open
   intervals In, then

       m(E intersect A) .LEQ. m(A)

   Consider some F subset LL with m(F) < inf. Let epsilon > 0. Then there exists
   a union of a finite number of open intervals such that

       m(F Delta A) < epsilon.

   then

       abs(m(F) - m(A)) .LEQ. m(F Delta A) < epsilon
       abs(m(F \ A) + m(F intersect A) - (m(A intersect F) + m(A \ F)))
       < m(F Delta A)

   so as (E intersect F) Delta (E intersect A) subset F Delta A

       abs(m(E intersect F) - m(E intersect A)) < epsilon

   so

       m(E intersect F) - epsilon < m(E intersect A)
                                  .LEQ. alpha m(A)
                                  .LEQ. alpha (m(F) + epsilon)

   so m(E) .LEQ. alpha*m(E); a contradiction.
** Homework 10
   #2 Use the trick with 'fn -> L means that every subsequence has a
   subsubsequence that converges to L' along with the fact that 'convergence in
   measure means that there is a subsequence that converges almost everywhere'
   to use the dominated convergence theorem.
* Class Administration
  Midterm in two weeks! Be ready.

  Homework 5 is due a week from Wednesday.
* Miscellaneous
** What does 'measurable on a set' mean? (page 44)
* Measurable Functions
** Overview
   Let (X, mu, MM) be a measure space. We want to define INTEGRAL f du; what
   sorts of functions can we integrate?

   Consider some function f :: X -> Y where (X, MM) and (Y, NN) are measurable
   spaces. We say that *f is measurable* or *MM, NN measurable* if:
   1. for all measurable sets in NN (that is E subset Y, E in NN) then f^(-1)(E)
      is in MM.

   That's actually it. We can make some immediate observations:
   1. Look at the collection of sets {f^(-1)(E) : E in NN}. This is a sigma
      algebra on XX (call it FF). Let
      GG = {E subset Y : F^(-1)(E) in MM} is a sigma algebra on YY.
   2. f measurable means that
          FF subset MM
          NN subset GG
   3. If EE is any collection of subsets of YY which generates NN, then
      f^(-1)(E) in MM for all E in EE. Then EE subset GG, so NN subset GG, so f
      is measurable.

   Therefore we don't have to check everything in Y, we just need to look at the
   generating sigma algebra. /* this is why we like sigma algebras! */

   Put another way: if NN is generated by some collection EE of subsets of YY,
   then it suffices to just check f^(-1)(E) for all E in EE.

   Borel measurable implies Lebesgue measurable, but not the reverse.
** Proposition 2.1
   If X and Y are topological spaces (they have open sets) and f :: X -> Y is
   continuous (so f^(-1)(open set) = another open set), and if we use BB_X
   subset MM, then f is (MM, BB_Y) measurable. That is, if we are working with
   Borel sets, continuous functions are measurable.
** Proposition 2.3
   If (X, NN) is a measurable space and f :: X -> RR (if we write RR, we assume
   the Borel sets). Then the following are equivalent:
   (recall: f^(-1)(E) = {x in XX: f(x) in E} subset MM)
   1. f is measurable ((MM, BB_RR)-measurable)
   2. f^(-1)((a, +inf)) is in MM for all a in RR
   3. f^(-1)([a, +inf)) is in MM for all a in RR
   4. f^(-1)((-inf, a]) is in MM for all a in RR
   5. f^(-1)((-inf, a)) is in MM for all a in RR

   _Proof_
   Propositions 2.1 and 1.2; the Borel sets generate the sets in 2-5.
** Proposition 2.6
*** Overview
    Let f, g :: X -> RR where (X, MM) is a measurable space. Then f + g, f*g are
    measurable.
*** Proof
**** Part 1: Addition
     Observe that for real numbers s and t, s + t < a iff there exist rational
     numbers p and q where

     s < p, t < q, then p + q < a.

     Then the set P = {(p,q) subset QQ^2 : p + q < a} is a countable
     set. Therefore

     {x in X : f(x) + g(x) < a} =
     UNION ((p,q) in P) ({x in X : f(x) < p} I {x in X : g(x) < q})

     we know that both of these sets ({x in X: f(x) < p}, {x in X: f(x) < q}) are
     both in MM. The union is countable. Therefore by closure under countable
     unions and intersections, f + g is measurable.
**** Part 2: Multiplication
     Consider f*f. Therefore we want sets that look like

         {x in X: (f(x))^2 < a} or f(x) in {-sqrt(a), sqrt(a)}

     which is a Borel set. Therefore as f is measurable for every a, f(x)*f(x)
     is measurable.

     Let f*g = 1/4*((f + g)^2 - (f - g)^2)
     (we can derive that f measurable -> -f is measurable based on the Borel
     sets)
     (we can also do c*f where c is a constant based on Borel sets as well).

     and therefore based on closure under squaring, constant multiplication,
     addition, and subtraction, f*g is measurable.
*** Application to Extended Reals
    Provided that inf - inf is handled in some way (see exercise 2). We will
    work on this more in the future.
** Day's Theorem
   f : X -> Y and g : Y -> Z
   where (X, MM), (Y, NN) and (Z, PP) are measurable. Then

       g . f : X -> Z is measurable.

   if E is in P, then (g . f)^(-1) (E) = f^(-1)(g^(-1)(E))

   where g^(-1)(E) in NN, f^(-1)(g^(-1)(E)) is in MM. In particular, if the
   codomain of f and domain of g are BB_RR then g . f is Borel measurable.

   If g is Lebesgue-measurable and g : RR -> RR (domain of G is LL) then we have
   two different sigma algebras; then g^(-1)(E) may cause a domain mismatch.

   We are okay if the outside function is Borel-measurable (if g is
   Borel-measurable and f is Lebesgue-measurable, then g . f is
   Borel-measurable).

   _Example_
   Let g be the absolute value function and f be some Lebesgue-measurable
   function.
** Proposition 2.7
*** Statement
    Suppose that {fj} is a sequence of RRbar-valued, measurable functions,
    f : X -> RRbar, then the following are measurable:
    /* we don't need convergence, so just use lim inf and lim sup */
    1. g1(x) = sup(j) fj(x)
    2. g2(x) = inf(j) fj(x)
    3. g3(x) = lim sup(j) fj(x)
    4. g4(x) = lim inf(j) fj(x)

    Note that these things are always defined in the extended real numbers.
*** Proof
    For any a in RRbar, we want to examine the set

        {x : g1(x) .LEQ. a} = INTERSECTION (j=1, inf) {x : fj(x) .LEQ. a}

    where each of the sets in the intersection is measurable. Therefore the
    intersection is measurable.

    Now look at

        {x : g2(x) < a} = UNION (j = 1, inf) {x : fj(x) < a}

    again each of the sets in the union is measurable. The same holds for g3 and
    g4; the measurability holds from what we have already done.
*** Corollaries
    if f, g : X -> RRbar are both measurable, then max(f, g) and min(f, g) are
    both measurable.

    If f(x) : X -> RRbar is the pointwise limit of a sequence of measurable
    functions, then f is measurable.
*** Conclusion
    It is hard to leave the set of measurable functions by basic operations. We
    have to try hard to leave the set.
** Decomposition in to Positive and Negative Parts
   f+(x) : max(f(x), 0)
   f-(x) : max(-f(x), 0)

   That is, we take the absolute value of the function. Note that by the
   previous result both of these are measurable. Additionally we can write

       f = f+ - f-

   as a convenient form.
** Integrals of Simple Functions
*** Indicator function
    Let XX_E = {1, x in E or 0 if x not in E} /* indicator function in E, chi */

    in other settings, some people write 1_E(x) or I_E(x).

    This function is measurable iff E in MM.
*** Definition
    A function f : X -> RR (not RRbar) is called *simple* if it has a finite
    range.

    Therefore if Ej = {x : f(x) = yj} /* finite number of js */
    this set must be in MM.

    We can represent f using the yj associated with Ej by just

        f = SUM (j=1, n) y_j X_E_j /* assume the y_js are distinct */
*** Example
    Say that f = X_[0,2] + X_[1,3]. Then we get

        f((-inf, 0)) = 0
        f([0, 1))    = 1
        f([1, 2])    = 2
        f((2, 3])    = 1
        f((3, +inf)) = 0

    If we decompose those output sets as (-inf, 0) UNION (3, +inf), [0 ,1) UNION
    (2, 3] and [1,2]: we get

        f = 0 X_E1 + 1 X_E2 + 2 X_E3

    which is the so-called *standard representation*.
** Properties of Simple Functions
*** Overview
    We will prove things about simple functions and then pass along the results
    via limits.

    Suppose that (X, MM) is a measurable space.
*** Theorem 2.10
**** Statement
     Assume that f : X -> RRbar is measurable.

     if f(x) .GEQ. 0 for all x, there exists some sequence {phi_n} (increasing)
     of simple functions:

     0 .LEQ. phi1 .LEQ. phi2 .LEQ. ...

     which converges to f pointwise. If f is bounded on E, then {phi_n}
     converges uniformly to f.

     _general case_
     There exists some sequence {phi_n} with

     abs(phi_n-1) .LEQ. abs(phi_n)

     which converges pointwise to f and converges uniformly to f when f is
     bounded on some open set.
**** Proof (Part (a))
     The general idea is to partition the range. Round f down to the nearest
     partition point. Build the next term in the sequence by splitting the old
     ones; this is where the increasing part comes from.

     Partition (0, +inf] in to intervals of the form (k/2^n, (k+1)/2^n] for k =
     0 to k = 2^(2n) - 1. Add one last interval to cover the rest of the
     positive reals: (2^n, +inf].

     Folland calls the preimage of the finite intervals E^k_n. He calls the
     'leftovers' F_n.

     Define phi_n = SUM k/2^n XX_E^k_n + 2^n X_Fn. Geometrically (not very
     rigorous) we see that this is an increasing function (given x, phi_n(x)
     .LEQ. phi_n+1(x)).

     This has to be increasing because we take the minimum over a smaller set
     repeatedly.

     if f is bounded on E, then for f(x) .LEQ. B, B .LEQ. 2^n for some N. Then
     for all n .GEQ. N, so abs(f(x) - phi_n(x)) < 1/2^n for all x in E.
     Therefore uniform convergence.
**** Proof (Part (b))
     Let {psi_n+} be simple functions for f+ and {psi_n-} be simple functions
     for f-.

     Let phi_n = psi_n+ - psi_n-. This is simple and has all the properties we
     want. Additionally

         abs(phi_n) = psi_n+ + psi_n- /* both terms are positive */

     so abs(phi_n) .LEQ. abs(phi_n+1). This gives us an explicit way of
     covering the more general case.
** Proposition 2.11
   Suppose that (X, MM, mu) is a complete measure space.
   1. If f,g XX -> RRbar, f is measurable and g = f mu-almost everywhere then g
      is measurable.
   2. If fn : X -> RRbar is measurable for each n and fn -> f almost everywhere,
      then f is measurable. /* we don't have to worry about converging on sets
      of measure zero */
# Prove this for review.
** Proposition 2.12
*** Statement
    Let (X, MM, mu) be a measure space and (X, MMbar, mubar) be its completion
    (see theorem 1.9). Suppose that

        f : X -> RRbar

    is a MMbar-measurable function. Then there exists some MM-measurable (that is
    the original measure space) function g such that f = g almost everywhere.

    Recall that

        MMbar = {E U F : E subset MM, F subset N in MM, mu(N) = 0}
*** Proof
    If f = c chi_A, A in MMbar. Then there exist E subset AA, E in MM, such that
    A \ E subset N. Then g is a subset of XX_E is MM-measurable.

    f = SUM ci chi_Ai is an MMbar-measurable simple function. Then for each Ai
    there is some Ei subset Ai such that Ei in MM and Ai \ Ei is a subset of a
    null set. Then g = SUM ci chi_Ei is a MM-measurable simple function that
    disagrees with f on a set of at most measure zero (a finite union of null
    sets). Therefore the proposition holds for simple functions.

    In general: let psi_n be a set of MMbar-measurable simple funcitons with
    psi_n -> f pointwise. We know that there exists some Psin, MM-measurable
    simple functions such that

        {x : psi_n(x) /= Psi_n(x)} subset Nn in MM, and that mu(Nn) = 0.

     Let N = U Nn. Then N is in MM, and mu(N) = 0. Therefore for x not in this
     null set psi_n(x) -> f(x).

     There is some more work here.
# finish this for review.
* Upper and Lower Semicontinuous
** General Idea
   We can split the definition of continuous in to multiple parts; we can look
   at the upper part and the lower part separately.
** Upper Semicontinuous
   Let f : RR -> RR (we just care about convergence pointwise, so any
   topological space can function as a domain). We say that f is *upper
   semicontinuous* when

       f(x) .GEQ. lim sup (y -> x) f(x)
** Lower Semicontinuous
   Let f : RR -> RR (we just care about convergence pointwise, so any
   topological space can function as a domain). We say that f is *lower
   semicontinuous* when

       f(x) .LEQ. lim inf (y -> x) f(x)
** General Properties
*** Continuity (split in to pieces)
    For every x and epsilon > 0, there exists a delta such that

        0 < abs(y - x) < delta

    implies that

        f(x) - epsilon < f(y) for LSC, f(y) < f(x) + epsilon for USC.
*** Intervals
    For every a,

        f^-1((a, inf)) is open for LSC
        f^-1((-inf, a)) is open for USC.
*** Relation to continuous functions
    There exists an increasing [decreasing] sequence of continuous functions
    {phin} that converge to f pointwise.

    That is, for upper semicontinuous functions, we need to approximate from
    above; for lower semicontinuous functions we need to approximate from
    below.
** Consequences (LSC)
   Note that all of these have USC equivalents.
*** Compact Sets
    If f is LSC, K is compact, then f is bounded below on K and achieves a
    minimum value. This is half of the extreme value theorem.
*** Max and Min
    if f and g are LSC then max(f,g) min(f,g) and f + g are LSC.
*** Sequences
    If fn converges up to f and each fn is LSC then f is LSC.
*** Measurability
    If f is LSC then it is Borel measurable.
*** Indicator Functions
    chi_A is LSC iff A is open. Think about the previous work we had with
    inverse functions.
** Consequences (both)
*** Sign flips
    f is LSC <-> -f is USC.
* Lebesgue Integrals
** Overview
   "Someday I'll connect my mouth and my brain": Dr. Day, October 5, 2012

   The only reasonable definition of the integral is to measure each set Ej and
   multiply that by yj. This should give us linearity.

   The definition we want to work with is

       INTEGRAL X_E dmu = 1*mu(E) + 0*mu(E^c)
                        = mu(E) /* recall that 0*+inf = 0 */

   Through out this section: (X, MM, mu) is a measure space. Let

       L+ = collection of all nonnegative f : X -> [0, +inf].

   If psi in L+ is simple with respect to the convention

       psi = SUM (j = 1, N) aj X_Ej

   then we define

       INTEGRAL psi dmu = SUM (j = 1, N) aj mu(Ej).

   Additionally

       INTEGRAL_A phi dmu = INTEGRAL chi_A phi dmu

   so we just make everything zero outside the desired set.
** Proposition 2.13
*** Statement
    Suppose that phi and psi are in L+ and are both simple. Then

    a. For all c .GEQ. 0, INTEGRAL c phi dmu = c INTEGRAL phi dmu.
    b. INTEGRAL phi + psi dmu = INTEGRAL phi dmu + INTEGRAL psi dmu
    c. if phi .LEQ. psi then INTEGRAL phi dmu .LEQ. INTEGRAL psi dmu
    d. mu_phi(A) := INTEGRAL_A phi dmu is a measure on (X, MM).
*** Proof
    We really want to prove b so that we may use any representation of a
    simple function for integration.
**** Part (a)
    Suppose that phi = SUM (i=1, N) ai chi_Ei is the standard representation of
    phi. Then if c = 0, then c phi = 0 chi_E, so

        INTEGRAL 0 chi_E = 0 mu(chi_E)
                         = 0*SUM(i = 1, N) ai mu(Ei)
                         = c INTEGRAL phi dmu.
**** Part (b)
     Suppose that psi = SUM (j = 1, m) bj X_Fj is the standard
     representation. Let

         UNION Ei = X = UNION Fj

     where both unions are disjoint. Then

         Ei = UNION (Ei intersect Fj)
         Fj = UNION (Ei intersect Fj)

     Then INTEGRAL psi dmu + INTEGRAL phi dmu = SUM ai mu(Ei) + SUM bj mu(Fj)
     and by combining the sets in the manner above

         = SUM (i = 1, n) SUM (j = 1, m)  (ai + bj) mu(Ei intersect Fj)

     Let Gk = {phi(x) + psi(x) = c_k}. Let Ik = {(i, j) : ai + bj = ck}. The Ik
     are disjoint, so
# redo this proof for practice.
     INTEGRAL phi + psi mu = SUM (k = 1, l) ck mu(Gk)
     but Gk is comprised of disjoint sets. Then

     INTEGRAL psi + phi dmu = SUM (k = 1, l) SUM ((i, j) in ??)
**** Part (c)
    Assume that we have the standard representations for phi and psi. Note that

        phi .LEQ. psi means ai .LEQ. bj whenever Ei intersect Fj /= emptyset

    Therefore

        ai mu(Ei intersect Fj) .LEQ. bj mu(Ei intersect Fj)

    and then summing over i and j we get that

        INTEGRAL phi dmu = SUM (i=1, n) SUM (j=1, m) ai mu(Ei intersect Fj)
                         .LEQ. SUM (i=1, n) SUM (j=1, m) bj mu(Ei intersect Fj)
                         .LEQ. INTEGRAL psi dmu
**** Part (d)
     Note that

         INTEGRAL_A phi dmu = INTEGRAL chi_A phi dmu

     If A is the empty set then chi_A phi - 0 chi_X so INTEGRAL_A phi dmu = 0.

     In general, chi_A phi = SUM (i=1, n) ai chi_A chi_Ei

     where chi_A chi_Ei = chi_(A intersect Ei). Then

         INTEGRAL_A phi SUM (i = 1, n) ai mu(A intersect Ei)

     so for each i, A intersect Ei = UNION (Ak intersect Ei)

     so

         mu(A intersect Ei) = SUM (k=1, inf) mu(Ak intersect Ei)
         SUM (i=1, n) mu(A intersect E) = SUM (i=1, n) SUM (k=1, inf) mu(Ak
         intersect Ei)

     where we may interchange the summatiosn (one is finite, all terms are
     positive) and get what we want.
** Integrating more general functions
   for f in LL+, we define

       INTEGRAL f dmu = sup({INTEGRAL phi dmu, phi in LL+, phi .LEQ. f})

   where phi is a simple function. Since integration of simple functions
   respects inequality this definition is consistent.

   _Remarks_
   If we have two functions, 0 .LEQ. f .LEQ. g in L+, then

       INTEGRAL f dmu .LEQ. INTEGRAL g dmu

   as every simple function in the set for f is in the set for g (supremum over
   a larger set).

   For any nonnegative constant c, INTEGRAL c f dmu = c INTEGRAL f dmu. If c is
   0 then we get only the zero simple function. By properties of simple
   functions we get the rest of it.
** Monotone Convergence Theorem (2.14)
*** Statement
    As usual, (X, MM, mu) is a measure space.

    Say that {fn} is a sequence in LL+ and fi(x) .LEQ. fi+1(x), x in X. Then

        LIMIT (n, inf) fn(x) = f(x)

    and

        LIMIT (n, inf) INTEGRAL fn(x) dmu = INTEGRAL f dmu
                                          = INTEGRAL LIMIT (n, inf) fn dmu
*** Proof
    Since the functions are monotonically increasing, their integrals are
    monotonically increasing (we proved this for nonnegative
    functions). Therefore the sequence of integrals is monotonically
    increasing. Therefore the limit

        LIMIT (n, inf) INTEGRAL fn dmu

    exists (and may be infinite). Since fn .LEQ. f,

        INTEGRAL fn .LEQ. INTEGRAL f.

    We know that LIMIT (n, inf) INTEGRAL fn dmu .LEQ. INTEGRAL f dmu. The other
    direction is trickier.

    For the reverse inequality, consider any simple function phi such that

        0 .LEQ. phi .LEQ. f.

    consider any 0 < alpha < 1. Let

        En = {x in X: fn(x) .GEQ. alpha phi(x)}

    it follows that for any x, fn(x) -> f(x). These sets get bigger as n gets
    bigger (fn increases). These sets are nested and their union is all of X.
    /* we picked alpha < 1 so that something has to eventually be larger than
    alpha phi(x) */

    Since alpha Chi_En phi .LEQ. Chi_En fn .LEQ. fn, it follows that

        alpha INTEGRAL (En) phi dmu .LEQ. INTEGRAL fn dmu.

    Since INTEGRAL (A) phi dmu is a measure on (X, MM) we know that

        LIMIT (n, inf) INTEGRAL (En) dmu = INTEGRAL (X) dmu = INTEGRAL phi dmu.

    Let n -> inf, so

        alpha INTEGRAL phi dmu .LEQ. LIMIT (n, inf) INTEGRAL fn dmu

    so as alpha -> 1

        INTEGRAL phi dmu .LEQ. LIMIT (n, inf) INTEGRAL fn dmu

    so by definition of INTEGRAL f dmu, taking the supremum over all such phi,
    we get

        INTEGRAL f dmu .LEQ. LIMIT (n, inf) INTEGRAL fn dmu.
*** Examples of failure for nonmonotone sequences
    fn = n Chi_(0, 1/n) converges pointwise to 0, but INTEGRAL fn = 1 and
    INTEGRAL f = 0.
** Theorem 2.15
*** Statement
    If {fn} is a finite or infinite sequence in LL+ and f = SUM fn, then

    INTEGRAL f dmu = SUM INTEGRAL fn dmu.

    That is, provided that there are no negative parts, we may interchange
    summations and integrals.
*** Proof
    Apply 2.14 to the sequence of partial sums.

    _proof for finite sequences_
    Consider just f1 and f2. Let {phij} and {psij} converge to f1 and f2; each
    are sequences of nonnegative simple functions, both monotonically
    increasing. Then phij + psij is simple, monotonically increasing and phij +
    psij converges to f1 + f2. We know that

        INTEGRAL phij + psij dmu = INTEGRAL phij dmu + INTEGRAL psij dmu

    By proposition 2.13. By the monotone convergence theorem

       IINTEGRAL (f1 + f2) dmu = LIMIT (j, inf) INTEGRAL phij + psij dmu
                               = LIMIT (INTEGRAL phij dmu + INTEGRAL psij dmu)
                               = INTEGRAL f1 dmu + INTEGRAL f2 dmu.

    By induction this proves the case of finite sequences.

    _proof for infinite sequences_
    For infinite sequences, let

        yn = SUM (j=1, n) fj

    we know that 0 .LEQ. gn .LEQ. gn+1 and gn -> f. By the monotone convergence
    theorem it holds that

        LIMIT (n, inf) SUM INTEGRAL fj dmu = LIMIT (n, inf) INTEGRAL SUM fj dmu
                                           = LIMIT (n, inf) INTEGRAL gn dmu
                                           = INTEGRAL f dmu
** Proposition 2.16
*** Setup
    Suppose that f in LL+; INTEGRAL f dmu = 0 if and only if f = 0 almost
    everywhere.
*** Proof
    Suppose that INTEGRAL f dmu = 0. but that f is not zero almost
    everywhere. Put another way, mu({x : f(x) > 0}) > 0. Let

        En = {x : f(x) > 1/n}

    then En .SUBSET. En+1. and UNION En = {x : f(x) > 0}. So

        mu(En) -> mu({f(x) > 0}) so mu(En) > 0 for some n.

    Then 1/n Chi_En .LEQ. f, so INTEGRAL 1/n Chi_En dmu .LEQ. INTEGRAL f dmu =
    0, but INTEGRAL 1/n Chi_En dmu > 0, a contradiction.

    Conversely: if f = 0 almost everywhere and

        0 .LEQ. phi .LEQ. f

    is simple; suppose phi = SUM ai Chi_ei is the standard respresentation. If
    ai > 0, then Ei subset {x : f(x) > 0}, so mu(Ei) =0. So ai mu(Ei) = 0 for
    all i. Therefore

        INTEGRAL phi dmu = SUM (i=1, n) ai mu(Ei) = 0.

    taking the supremum we obtain INTEGRAL f dmu = 0.
** Corollary 2.17
*** Setup
    if {fn} is a sequence in LL+, f in LL+, fn increases monotonically to f
    almost everywhere (i.e. there exists some E in MM, mu(E) = 0, s.t. for x
    notin E, fi(x) .LEQ. fi+1(x) fi(x) -> f(x)). Then

        LIMIT INTEGRAL fn dmu = INTEGRAL f dmu.
*** Proof
    Consider Chi_E^c fn goes up to Chi_E^c f for all x, so

        LIMIT INTEGRAL Chi_E^c fn dmu = INTEGRAL Chi_E^c f dmu.

    so f = X_E f + X_E^c f; as X_E f = 0, so INTEGRAL Chi_E f dmu = 0. Therefore

        INTEGRAL f dmu = INTEGRAL Chi_E^c f dmu.

    likewise

        INTEGRAL fn dmu = INTEGRAL Chi_E^c fn dmu for each n.

    FInally

        LIMIT INTEGRAL fn dmu = LIMIT INTEGRAL Chi_E^c fn dmu
                              = INTEGRAL Chi_E^c f dmu
                              = INTEGRAL f dmu
** Fatou's Lemma (Lemma 2.18)
*** Statement
    If {fn} is any sequence in LL+ then

        INTEGRAL (lim inf fn) dmu .LEQ. lim inf INTEGRAL fn dmu.
*** Proof
    Let gn = inf (j .GEQ. n) fj. Then gn goes up to lim inf fn. This makes it
    natural to use the monotone convergence theorem (we are approaching
    something from below).

    By the monotone convergence theorem

        INTEGRAL (lim inf fn) dmu = lim (n, inf) INTEGRAL gn dmu

    since gn .LEQ. fn, INTEGRAL gn dmu .LEQ. INTEGRAL fn dmu. Therefore

        lim (n, inf) INTEGRAL gn dmu .LEQ. lim inf INTEGRAL fn dmu.
** Corollary 2.19 (immediately follows Fatou's Lemma)
*** Statement
    If {fn} is a sequence in L+ and fn -> f in L+ almost everywhere, then

        INTEGRAL f dmu .LEQ. lim inf INTEGRAL fn dmu.
*** Proof
    Let E = {x : fn(x) -> f(x)}. Then by definition mu(E^c) = 0. If we replace f
    by chi_E f and fn by chi_E fn, we do not change the integrals (proposition
    2.16) so

        INTEGRAL f dmu = INTEGRAL chi_E f dmu
        INTEGRAL fn dmu = INTEGRAL chi_E fn dmu

    Now chi_E fn -> chi_E f holds for all x. Therefore by Fatou's lemma

        INTEGRAL  f dmu = INTEGRAL chi_E fn dmu .LEQ. lim inf INTEGRAL chi_E fn
        dmu .LEQ. lim inf fn dmu.
** Proposition 2.20
*** Statement
    If f in LL+ and INTEGRAL f dmu < inf, then mu({x : f(x) = +inf}) = 0.
*** Proof
    Left as an exercise for the reader!
** Complex Functions
   we will skip this part for this class.
** Integrals of functions with varying signs
*** Overview
    Recall that we can write

        f = f+ - f-, where f+ and f- are in LL.

    We define the integral in this context as

        INTEGRAL f = INTEGRAL f+ - INTEGRAL f-

    and we say that a function is integrable if abs(INTEGRAL f dmu) < inf. We
    can define an integral (but the function is not *integrable*) if the
    integral of f+ or f- is +inf. The value is undefined if both are +inf.
** Proposition 2.21
*** Overview
    The integrable functions form a real vector space. The map

        f -> INTEGRAL f dmu (I :: L1 -> RR)

    is a linear functional.
*** Proof
    Suppose that f is integrable. Let c in RR+ U {0} /* c is a nonnegative real
    number */. Then

        INTEGRAL (c f)^{+-} dmu = INTEGRAL c f^{+-} dmu
                                = c INTEGRAL f^{+-} dmu < inf.

    We can do a similar thing for f = f+ + f-.

    If c is a negative real number then
        (c f)+ = -c (f-)
    and
        (c f)- = -c (f+)
    Therefore
        INTEGRAL (c f)^{+-} dmu = INTEGRAL -c (f^{-+}) dmu
                                = INTEGRAL -c (f^{-+}) dmu < inf
    so
        INTEGRAL (cf) dmu = c INTEGRAL f+ dmu - c INTEGRAL f- dmu

    so L1 is closed under scalar multiplication.

    Let h = f + g, f and g in L1. Since the integrals of f and g are finite, the
    integral of h is finite, so L1 is closed under addition. Note that

        h^{+-} .LEQ. f+ + f- + g+ + g-

    Note that
        h+ - h- = h = f + g = f+ - f- + g+ - g-
    so
        h+ + f- + g- = h- + f+ + g+.

    Therefore, by Theorem 2.15,

        INTEGRAL h+ dmu + INTEGRAL f- dmu + INTEGRAL g- dmu = INTEGRAL h- dmu +
        INTEGRAL f+ dmu + INTEGRAL g+ dmu

    where all terms are finite. Then we get that

        INTEGRAL h dmu = INTEGRAL f dmu + INTEGRAL g dmu
** Proposition 2.22
*** Statement
    if f in L1 then abs(INTEGRAL f dmu) .LEQ. INTEGRAL abs(f) dmu.
*** Proof
    abs(INTEGRAL f dmu) = abs(INTEGRAL f+ dmu - INTEGRAL f- dmu)
                        .LEQ. INTEGRAL f+ dmu + INTEGRAL f- dmu
                        = INTEGRAL abs(f) dmu.
** Proposition 2.23
*** Statement
    If f in L1 then the set {x : f(x) /= 0} is sigma-finite.

    Suppose f, g in L1. The following are equivalent:
    1. INTEGRAL (E) f dmu = INTEGRAL (E) g dmu for all E in MM
    2. INTEGRAL (E) abs(f - g) dmu = 0.
    3. f = g almost everywhere.
*** Proof
    _Part (a)_
    WE know that INTEGRAL abs(f) dmu < inf.

    Let Fn = {x : abs(f(x)) in (1/(n+1), 1/n]} for n in NN.

    More happens here!
** What is L1?
   f = g a.e. is an equivalence relation among the set of integrable
   functions.

   L1 is defined to be the *collection of equivalence classes of integrable
   functions* (not integrable functions).

   Therefore when we say f in L1 we really mean 'the equivalence class of
   things that equal f almost-everywhere' is in L1.

   INTEGRAL abs(f - g) dmu = 0 -> [f] = [g] (same equivalence class) but *not*
   f = g. We can use this as a metric:

       d(f, g) = INTEGRAL abs(f - g) dmu
** Completions of spaces of measurable functions
   Let (X, MMbar, mubar) be the completion of (X, MM, mu).

   If f, g are MM-measurable then they are MMbar-measurable.
   f = g a.e. in MM -> f = g a.e. in MMbar.
   /* switching to a larger space does not move functions out of their
   equivalence class */

   if f is MMbar-measurable we know (proposition 2.12) that there is an
   MM-measurable g such that f = g a.e.; that is each MMbar equivalence class
   has a MM representative.

   Therefore there is a 1-1 correspondence between MM and MMbar.
** Dominated Convergence Theorem
*** Statement
    Suppose {fn} is a sequence in L1 such that
    1. fn -> f almost-everywhere.
    2. there exists g in L1 with abs(fn) < g almost-everywhere.

    Then f in L1 and

        LIMIT (n, inf) INTEGRAL fn dmu = INTEGRAL f dmu.

    That is, we can do this integral if all the functions are 'trapped' inside
    some integrable function.
*** Proof
    We assume that each fn is measurable.

    What about f? Is it measurable? Maybe not (we don't have a complete
    space).

    Let f~ = lim sup fn (so f~ *is* measurable) and f~ = f
    almost-everywhere. Note that if (X, MM, mu) is complete than f *is*
    measurable.

    If the space is not complete then we should alter the conclusion to read f~
    in L1 and f~ in the integral.

    Therefore it is okay to assume that f is measurable (we dealt with the
    nonmeasurable case accordingly).

    We know that abs(fn) .LEQ. g a.e. and fn -> f almost-everywhere, and
    therefore abs(f) < g almost-everywhere.

    By changing fn, f, g on a set of measure zero we can assume that the
    inequalities hold for all x without changing the values of the integrals.

    Therefore -g(x) .LEQ. fn(x), f(x) .LEQ. g(x) for every x. Therefore

        0 .LEQ. fn + g, 0 .LEQ. f + g.

    Therefore by Fatou's lemma

        INTEGRAL f + INTEGRAL g = INTEGRAL f + g
                                .LEQ. lim inf INTEGRAL fn + g
                                .LEQ. lim inf INTEGRAL fn + INTEGRAL g
                                .LEQ. (lim inf INTEGRAL fn) + INTEGRAL g

    so INTEGRAL f .LEQ. (lim inf INTEGRAL fn).

    Now we want to show how it works with the lim sup. As

        0 .LEQ. g - fn, 0 .LEQ. g - f

    Applying Fatou's Lemma again

        INTEGRAL g - INTEGRAL f = INTEGRAL g - f
                                .LEQ. INTEGRAL lim inf (g - fn)
                                = lim inf INTEGRAL (g - fn)
                                = INTEGRAL g + lim inf INTEGRAL (-fn)
                                = INTEGRAL g - lim sup INTEGRAL (fn)

    so lim sup INTEGRAL fn .LEQ. INTEGRAL f.

    Therefore INTEGRAL f = limit INTEGRAL fn = INTEGRAL limit fn.
** Theorem 2.25
*** Statement
    Suppose {fn} is a sequence in L1 for which

        SUM INTEGRAL abs(fn) dmu < inf.

    Then SUM fn convergence almost-everywhere to an L1 function and

        INTEGRAL sum fn = sum INTEGRAL fn.
*** Proof
    see book.
** Theorem 2.26
*** Statement
    Suppose that f in L1(mu) and epsilon > 0. Then
    1. There exists a simple function phi with

           INTEGRAL abs(f - phi) dmu < epsilon

    2. If mu is a Lebesgue-Stieljes measure on RR, then there exists a
       continuous function g vanishing outside a bounded interval (sometimes
       called continuous with compact support) with

           INTEGRAL abs(f - g) dmu < epsilon.

       Functions like this are dense in L1.
*** Proof (Part 1)
    By theorem 2.10b we know that there are simple functions phin -> f
    pointwise and monotonically.

    To use the dominated convergence theorem, note that

        abs(f - phin) .LEQ. abs(f + phin) .LEQ. 2f in L1.

    Therefore limit INTEGRAL abs(f - phin) dm = 0. Therefore for epsilon we can
    find some n such that

        INTEGRAL abs(f - phin) dm = 0.
*** Proof (Part 2)
    Let phi be a simple function, abs(phi) .LEQ. abs(f). by previous work we
    have that we can pick phi such that

        INTEGRAL abs(f - psi) dmu < epsilon/3.

    Let psi = SUM ai chi_Ei (standard form). Assume that all abs(ai) > 0.

    Observe that abs(ai) mu(Ei) = INTEGRAL (Ei) abs(psi) dmu
                                .LEQ. INTEGRAL abs(psi) dmu
                                .LEQ. INTEGRAL abs(f) dmu < inf

    so mu(Ei) < 1/abs(ai) INTEGRAL abs(f) dmu < inf.

    Let E be one of the Eis. By proposition 1.20 we have that there is a finite
    union of open intervals (call it A) such that mu(A Delta E) < epsilon'.

    Assume that A = UNION (1,m) Ij. Note that we have not fixed epsilon'
    yet. Then

        INTEGRAL abs(chi_E - chi_A) dmu = mu(E Delta A) < epsilon'.

    Note that chi_A = SUM chi_Ij as the Ijs are disjoint. We want to approximate
    chi_Ij by hj "from inside" /* this is like Rudin's trick of approximating
    things by drawing lines */ by bumping the step (the indicator function steps
    on both sides) over a bit with a straight line.

    For an infinite interval: for small delta, draw a line from (a, 0) to (a +
    delta, 1) to make the function continuous. Then make it come back to zero at
    a + 1/delta.

    As delta -> 0, what we called hj (the trapezoid things we constructed from
    the simple functions) by the monotone convergence theorem

        INTEGRAL hj dmu -> INTEGRAL chi_Ij dmu

    so for small enough delta

        INTEGRAL abs(chi_Ij - hj) dmu < INTEGRAL hi_Ij - hj dmu
                                      < INTEGRAL hi_Ij dmu - INTEGRAL hj dmu
                                      < epsilon'/m

    let g = SUM hj. Then

        INTEGRAL abs(chi_A - g) dmu = INTEGRAL abs(SUM (1,m) (chi_Ij - hj)) dmu
                                    .LEQ. SUM (1,m) INTEGRAL abs(chi_Ij - hj) dmu
                                    < m*epsilon'/m
                                    < epsilon'

    Therefore

    INTEGRAL abs(chi_E - g) dmu .LEQ. INTEGRAL abs(chi_E - chi_A) + abs(chi_A - g) dmu
                                < 2 epsilon'.

    Finally: for g SUM (1,n) ai gi is just a continuous function that vanishes
    outside of some bounded interval by construction. Therefore

    INTEGRAL abs(f - g) dmu = INTEGRAL abs(f - psi) dmu + INTEGRAL abs(psi - g) dmu
                            = INTEGRAL abs(f - psi) dmu
                            + INTEGRAL abs(SUM (1,n) ai (chi_Ei - gi)) dmu

                        .LEQ. INTEGRAL abs(f - psi) dmu
                              + SUM abs(ai) INTEGRAL abs(chi_Ei - gi) dmu

    where the first bit is less than epsilon/3 and the second part is less than
    epsilon'/2. Therefore pick epsilon' such that we get < epsilon.
* Riemann and Lebesgue Integrals
** Theorem 2.28
*** Statement
    Let f : [a, b] -> RR be a bounded function.

    a. If f is Riemann-integrable then it is Lebesgue-measurable and the Riemann
       and Lebesgue integrals agree, or

           INTEGRAL (a,b) f(x) dx = INTEGRAL [a,b] f dm

    b. f is Riemann-integrable iff the set of points where f is discontinuous
       has Lebesgue measure zero.
*** Brief Review: Riemann Integral
    Recall that f is Riemann-integrable if a partition of the interval [a,b] is
    a finite sequence P = {tj} where

        a = t0 < t1 < ... < tn = b.

    Given P we define Mj = sup [tj-1, tj] f(x) and mj = inf [tj-1, tj] f(x). The
    upper and lower sums are

        S_P f = SUM Mj (tj - tj-1) /* estimate from above */
        s_P f = SUM mj (tj - tj-1) /* estimate from below */

    and

        Ioverbar(a,b)  = inf (P) S_P f
        Iunderbar(a,b) = sup (P) s_P f.

    We say that f is Riemann-integrable when the upper and lower integrals
    agree. Note that this is why we need f to be bounded (so that we do not get
    -inf or +inf). Note that refining the partition moves the lower sum up and
    the upper sum down.

    There exists a sequence of refinements Pk such that Pk subset Pk+1 and the
    upper sums decrease to the upper integral. We also insist that the largest
    mesh stepsize goes to zero.
*** Proof of (a)
    For a given P, define

        Gp = SUM Mj chi_(tj-1, tj]
        gp = SUM mj chi_(tj-1, tj]

    assume that f(x) = 0 for x not in [a,b]. Note that Gp and gp are both
    bounded simple functions. Note that

        INTEGRAL Gp dm = S_P f
        INTEGRAL gp dm = s_P f

    by definition of the upper and lower sums. Therefore the S_P fs converge
    monotonically to the upper integral and the s_P fs converge monotonically to
    the lower integral. Let

        G = limit (k -> inf) G_Pk
        g = limit (k -> inf) g_Pk

    so both G and g are bounded, measurable functions. Therefore

        g .LEQ. f .LEQ. G

    pointwise. Note that g and G are in L1. Therefore as

        Iunderbar (a,b) f = INTEGRAL g dm
        Ioverbar  (a,b) f = INTEGRAL G dm

    Let C = sup [a,b] abs(f(x)). Then both abs(g) and abs(G) .LEQ. C chi_[a,b],
    so we may apply the dominated convergence theorem. As f is
    Riemann-integrable, Iunderbar and Ioverbar are equal by
    hypothesis. Therefore

        INTEGRAL g dm = INTEGRAL f(x) dx = INTEGRAL G dm.

    Therefore

        INTEGRAL G - g dm = 0, so G = g almost everywhere (G - g .GEQ. 0).

    Therefore f = g almost everywhere and f = G almost everywhere.
*** Example (Exercise 28d)
**** Overview
     No need to redo this for homework.

     We will do the case for a > 0. Determine

         LIMIT (n -> inf) INTEGRAL (0, inf) n(1 + n^2 x^2)^-1 dx

     this is an improper Riemann integral.
**** Methods
     1. We can solve this problem using only 1206 tools.
     2. We can move the limit in to the integral by some means.
**** Method 1
     The limit of the inside should be zero. We don't have anything coming down
     so let's try the dominated convergence theorem.

     Claim: if n2 > n1 > 1/sqrt(a) then n2/(1 + n2^2 x^2) .LEQ. n1/(1 + n1^2 x^2)

     This can be checked algebraically (directly). Therefore we can use the n1
     term as the dominating function. To clean things up, call n2 N.

     In particular we may interchange the limit and the integral to move the
     limit inside, and we get the integral of the zero function.

     Then

         INTEGRAL N/(1 + N^2 x^2) dm .LEQ. 1/N INTEGRAL x^-2 dm.

     Therefore, by the monotone convergence theorem,

         INTEGRAL x^-2 dm = LIMIT (b, inf) INTEGRAL [a, a + b] x^-2 dm
                          = LIMIT (b, inf) 1/a - 1/(a + b)
                          = 1/a.

     We can get the same result with improper integrals.
**** Method 2
     Let y := n x. Then

         INTEGRAL n/(1 + n^2 x^2) dm = INTEGRAL 1/(1 + y^2) dm

     We can justify this approach for the more general case:

         f(x) = chi_E(x), f(n x) = chi_(1/n E)(x)

     (this works in general, but we have to be careful when our change of
     coordinates is not monotone).
     What we want to hold holds for integrals of simple functions. By the
     monotone convergence theorem it works in the limit.

     Yet another approach: we can rewrite our integral as

         LIMIT (n, inf) INTEGRAL chi_(n a, inf) 1/(1 + x^2) dm.
*** Proof of (b)
**** Terminology
     The method is outlined in Exercise 23 (page 59). Let
     /* note that we allow H(x) and h(x) to have the value of f(x) */

         H(x) = LIM (delta -> 0+) sup ((y - x) < delta) f(y)
         /* upper semicontinuous envelope of f */
         h(x) = LIM (delta -> 0+) inf ((y - x) < delta) f(y)
         /* lower semicontinuous envelope of f */

     recall that a function is *upper semicontinuous* at c if

         f(c) >= lim sup (x -> c) f(x)

     and a function is *lower semicontinuous* at c if

         f(c) <= lim inf (x -> c) f(x)
**** Why are H and h USC and LSC?
     Consider h(x). Given x, epsilon > 0 ther e is delta0 > 0 such that

         h(x) - epsilon < inf (abs(y - x) < delta0) f(y)

     consider some delta0 neighborhood of x. Then for x~ in the neighborhood,
     for sufficiently small delta the delta-neighborhood of x~ is in the delta0
     neighborhood. Therefore if

         abs(x~ - x) < delta0 and delta > 0 is sufficiently small

     then

         h(x) - epsilon .LEQ. inf (abs(y - x) < delta) f(y)
                        .LEQ. inf (abs(y - x~) < delta) f(y)

     so for delta arbitrarily small

         h(x) - epsilon < h(x~)

     for all abs(x~ - x) < delta0. This shows that h is LSC.

     Something similar happens for H.
**** Fleshing out the claim g = h a.e. and G = H a.e.
     For any x in [a,b] and any delta > 0 we have that (as mesh(Pk) -> 0) the
     interval [tj, tj+1] which contains x will be a subset of (x - delta, x +
     delta). Therefore, for sufficiently refined partitions

         inf (abs(y - x) < delta) f(y) .LEQ. m
                                       .LEQ. M
                                       .LEQ. sup (abs(y - x) < delta) f(y)

     where m = gP(x), M = GP(x).  In particular, as delta -> 0, we get that

         h(x) .LEQ. g(x) .LEQ. G(x) .LEQ. H(x).

     for *all* x in [a,b].

     Now consider some x in [a,b], x not in Pk. Therefore x is an interior point
     of some interval in the partition. Therefore

         tj-1 < x < tj

     so for sufficiently small delta, (x - delta, x + delta) subset [tj-1, tj].
     Therefore

         mj .LEQ. inf (abs(y - x) < delta) f(y)
            .LEQ. sup (abs(y - x)) < delta f(y)
            .LEQ. Mj

     Therefore we get what we want except on a set of measure zero (the number
     of partition points is countable, therefore has measure zero). Therefore
     taking the limit as k -> inf

         g(x) .LEQ. h(x) .LEQ. H(x) .LEQ. G(x).

     Therefore g = h a.e. (everything but the partition points) and G = H
     a.e. (same thing).
**** Observations
     Note that

     + h(x) <= f(x) <= H(x)
     + h(x) = H(x) iff f is continuous.
     + h(x) is lower semicontinuous and H(x) is upper semicontinuous.

     and, in our proof of (a), recall that there exists a sequence of refinements
     {Pn}, Pk subset Pk+1, where the the upper sum converges from above and the
     lower sum converges from below to the Riemann integral.
     + Using the notation from before, h = g and H = G almost everywhere. This
       takes some epsilon-delta work.

     Note that f is Riemann-integrable iff Iunderbar = Ioverbar. Therefore f is
     Riemann-integrable iff

         INTEGRAL h dm = INTEGRAL H dm

     so
         INTEGRAL H - h dm = 0.

     so H - h >= 0 -> H - h = 0 almost-everywhere. Therefore the Lebesgue measure
     of the set of points where H and h disagree has measure zero; this set is
     exactly the set where f is discontinuous. Therefore f is discontinuous on a
     set of measure zero.
**** Other Justifying Work
     Suppose that f is continuous at x and

         abs(y - x) < delta_epsilon -> abs(f(y) - f(x)) < epsilon.

     Then for all 0 < delta < delta_epsilon

         f(x) - epsilon <= inf (abs(y - x) < delta) f(y)
                        <= sup (abs(y - x) < delta) f(y)
                        < f(x) + epsilon

     which implies that as epsilon is arbitrary, f(x) = H(x) = h(x).

     Now try the other way: suppose that h(x) = H(x). Then f(x) = h(x), so given
     epsilon > 0 there is delta > 0 such that

         f(x) - epsilon < inf (abs(y - x) < delta) f(y)
                        < sup (abs(y - x) < delta) f(y)
                        < f(x) + epsilon.


     Therefore, abs(y - x) < delta -> abs(f(y) - f(x)) < epsilon. Therefore we
     get continuity when H(x) = h(x).
** Improper Riemann Integrals
*** Overview
    We typically define the improper Riemann integral as

        INTEGRAL (a, inf) f(x) dx = LIMIT (b -> inf) INTEGRAL (a, b) f(x) dx

    If f is positive then we can use the monotone convergence theorem. If f is in
    L1 then we may use the dominated convergence theorem. In both cases we can
    pass in the limit.

    Otherwise: it is possible for the improper Riemann integral to exist but not
    the Lebesgue integral. This can happen when the positive and negative parts
    cancel before we pass the limit (for Lebesgue we only combine positive and
    negative at the very end).
*** Example of trouble
    Consider

        f = SUM (-1)^n / n chi_(n, n+1]

    each INTEGRAL f^(+/-) dm is infinite, so the Lebesgue integral does not
    exist. However the Riemann integral is fine (alternating convergence theorem).
* Modes of Convergence
** Overview
   If fn : X -> RR is a sequence of measurable functions we have a few notions
   of convergence:
   + fn -> f pointwise
   + fn -> f almost-everywhere
   + fn -> f uniformly

   if (X, MM, mu) is a measure space then we have some more notions of
   convergence:
   + fn -> f in L1 then INTEGRAL abs(fn - f) dm -> 0 /* metric space convergence */
   + fn -> f *in measure*

   where in measure means that for all epsilon > 0,

       mu({x : abs(f(x) - fn(x)) > epsilon}) -> 0 as n -> oo.

   We can expand this idea in to a Cauchy sequence idea.
** Cauchy in Measure
   We say that fn is *Cauchy in measure* when for every epsilon1, epsilon2 > 0
   there exists an N such that

       mu({x : abs(fn(x) - fm(x)) > epsilon1}) < epsilon2 whenever n, m >= N.
** Connections between different types of convergence
*** Relationships
    uniform convergence   -> pointwise convergence.
    pointwise convergence -> a.e. convergence.
    a.e. convergence      -> convergence in measure.
    converges in measure <-> cauchy in measure. (Proposition 2.30)

    L1 convergence -> convergence in measure. (Proposition 2.29)

    so we can draw a directed graph of convergence ideas.
*** Examples (RR, LL, m)
**** Example 1
     Let

         fn = 1/n chi_(0,n)

     This function converges uniformly to the zero function. This function does
     not converge in L1 however:

         INTEGRAL fn dm = n*(1/n) = 1.
**** Example 2
     Let

         f1 = chi_(0,1]
         f2 = chi_(0,1/2]
         f3 = chi_(1/2,1]
         f4 = chi_(0,1/4]
         f5 = chi_(1/4, 1/2]
         f6 = chi_(1/2, 3/4]
         f6 = chi_(3/4, 1] etc

     Therefore fn -> 0 in L1. However, this does not converge pointwise.
** Proposition 2.29
*** Statement
    If fn -> f in L1 then fn -> f in measure.
*** Proof
    Let epsilon > 0. Let

        En = {x : abs(fn(x) - f(x)) > epsilon}.
        /* our goal is to show that mu(En) -> 0 */

    since epsilon chi_En .LEQ. abs(fn - f) we can integrate both sides and get

        epsilon*mu(En) .LEQ. INTEGRAL abs(f - fn) dm

    so

        mu(En) .LEQ. 1/epsilon*INTEGRAL abs(f - fn) dm

    so as n -> oo mu(En) -> 0 (due to assumption of convergence in L1).
** Proposition 2.30
*** Statement
    Suppose {fn} is Cauchy in measure. Then

    1. {fn} converges in measure; i.e. there exists a measurable function f such
       that fn -> f in measure.
    2. If fn -> f (same f as in 1) and fn -> g in measure then f = g
       almost-everywhere.
    3. There exists a subsequence {fnj} converging to f almost-everywhere.
*** Proof
**** Part 1
     /* Note that folland does everything here with epsilon = min(epsilon1,
     epsilon2) */

     To prove (1) given epsilon1, epsilon2 > 0, we need some N such that

         mu({x : abs(fn(x) - fm(x)) > epsilon1/2}) < epsilon2/2, n, m >= N.

     From the subsequence, there is some nj >= N so that

         mu({x : abs(fnj(x) - f(x)) > epsilon1/2}) < epsilon2/2.

     Since abs(fm(x) - f(x)) > epsilon1 we have that either

         abs(fnj(x) - fm(x)) > epsilon1/2 or
         abs(fnj(x) - f(x))  > epsilon1/2.

     Therefore mu({x : abs(fm(x) - f(x)) > epsilon1}) <= (sum of two above) <
     epsilon2. Therefore fn -> f in measure, where f was constructed in a
     certain way.
**** Part 2
     Say that {fn} converges to both f and g in measure. Then

     {x : abs(f(x) - g(x)) > epsilon} subset {x : abs(f(x) - fn(x)) > epsilon/2}
                                       union {x : abs(g(x) - fn(x)) > epsilon/2}

     Therefore, measuring everything, we get that

         mu(a) <= mu(b) + mu(c)

     as n -> oo we get that mu({x : abs(f(x) - g(x) > epsilon)}) = 0 for every
     epsilon > 0. Therefore, taking the union for epsilon = 1/k, k -> oo we get
     that f = g almost-everywhere.
**** Part 3
     Suppose that {fn} is Cauchy in measure. We can choose a subsequence gj = fnj
     such that

         Ej = {x : abs(gj(x) - gj+1(x)) > 2^(-j)} has mu(Ej) < 2^(-j).

     This works because we can pick 2^(-l) = min(epsilon1, epsilon2). Let

         Fk = UNION (j=k, oo) Ej so mu(Fk) .LEQ. SUM (j=k, oo) 2^(-j)

     so mu(Fk) .LEQ. 2^(1 - k). Let x notin Fk. Then

         abs(gj(x) - gj+1(x)) .LEQ. 2^(-j) for all j .GEQ. k.

     Therefore for i .GEQ. j .GEQ. k we have that

         abs(gj(x) - gi(x)) .LEQ. SUM (l=j,i-1) abs(gl+1(x) - gl(x))
                            .LEQ. SUM (l=j,i-1) 2^(-l)
                            < 2^(1 - j).

     so for x not in Fk, {gi(x)} is Cauchy. Therefore it converges. Now define
     /* so except for a set of measure zero all points are in some Fk^c */

     f(x) = LIM(j -> oo) gj(x), x in (UNION Fk^c)
     f(x) = 0, x in INTERSECTION Fk.

     Therefore {fnj} = {gj} converges to f almost-everywhere.
     /* this is where we needed the extra part to get around the
     counter-example. */

     For x not in Fk, i >= j >= k, we have that

         abs(gi(x) - gj(x)) <= 2 2^(-j).

     so mu({x: abs(f(x) - gj(x)) > 2 2(-j)}) <= mu(Fk) < 2 2^(-k), so gi -> f in
     measure.
** A counter-example: convergence almost-everywhere versus in measure
   fn = chi_(n, oo)
   fn -> 0 almost-everywhere, but fn -> 0 in measure.

   This is not a counter-example if the measure of the full space is finite.
** Corollary to 2.30
*** Statement
    If fn -> f in L1 then there is a subsequence with fnj -> f
    almost-everywhere.
*** Proof
    Ommitted.
** Egoroff's Theorem
*** Statement
    Suppose that mu(X) < oo. Assume that f is finite and fn -> f
    almost-everywhere. For any epsilon > 0 there is some E (measurable) subset X
    such that mu(E) < epsilon, fn -> f *uniformly* on E^c.
*** Proof
    Let En(k) = {x : abs(fm(x) -f(x)) > 1/k for some m >= n}.
    Note that En(k) >= En+1(k). Then

    INTERSECTION (n=1, oo) En(k) = {x : abs(fm(x) - f(x)) > 1/k for finitely
    many m.}

    and this is a subset of {x : fn(x) does not converge to f(x)} (a set of
    measure zero). Since mu(X) < oo,

    LIM (n -> oo) mu(En(k)) = 0

    so for each k we can find nk such that mu(Enk(k)) < epsilon*2^(-k). Then

    E = UNION (k=1, oo) Enk(k), mu(E) < SUM (1, oo) epsilon*2^(-k) = epsilon.

    Therefore if x not in E then for every k, x not in Enk(k). Therefore

        abs(fm(x) - f(x)) <= 1/k for all m >= nk.

    This gives us uniform convergence on the compliment of E.
** Lusin's Theorem
*** Statement
    Similarly to Egoroff's Theorem, we can say that a function is continuous
    except on an arbitrarily small sets.

    The limit of uniformly continuous functions is continuous. This is the heart
    of the homework.
*** Proof
    Homework!
* Functions of multiple variables, vector-valued functions, product spaces
** Overview
   We have done measure theory for functions of several variables so far. We
   skipped most of Folland's work for product spaces.
** Product Measures
*** Overview
    Suppose we have *two* measure spaces (X, M, mu) and (Y, N, nu). We want to
    define the product sigma-algebra M x N on X x Y as well as the product
    measure mu x nu.

    Folland does cartesian products of arbitrary products of sets

        PRODUCT (alpha in A) X alpha (Xalpha, Malpha, mualpha)

    Our plan is to just do everything for two. All these results hold for
    finite and countable collections. We will not look at uncountable products.
*** Building Them
    Assume that (X, M, mu) and (Y, N, nu) are measurable spaces. We want to
    build the product measure mu x nu in (X x Y, M x N), where M x N is a
    product sigma-algebra.

    What we want is mu x nu (E x F) = mu(E) nu(F).
*** Measurable Rectangles
    The set {E x F : E in M, F in N} is an elementary family; contains the empty
    set and is closed under (term-wise) intersection.

    Let AA be the set of finite disjoint unions of measurable rectangles. This
    is an algebra of subsets of X x Y. Define

        Pi(UNION (Ei x Fi)) = SUM mu(Ei) nu(Fi).

    for disjoint pairs Ei x Fi. We need to check that Pi is well-defined and
    that Pi is a premeasure (then we may use Caratheodory).

    If E x F = UNION (j) Aj x Bj (indicies finite or countable) where the As and
    Bs are disjoint then

        Chi_(E x F)(x, y) = chi_E(x) chi_F() = SUM chi(Aj x Bj) (x, y)
                          = SUM chi_Aj(x) chi_Bj(y).

    For each y we have that

        mu(E) chi_F(y) = INTEGRAL chi_(E x F) ( , y) dm
                       = INTEGRAL SUM chi_Aj( ) chi_Bj(y) dm
                       = INTEGRAL SUM chi_Aj( ) chi_Bj(y) dm
                       = SUM (j) chi_Bj(y) INTEGRAL chi_Aj) dm
                       = SUM (j) chi_Bj(y) mu(Aj)

    Therefore mu(E) chi_F(y) = SUM (j) mu(Aj) chi_bj(y) for all y in Y. This
    shows that our definition of Pi is not ambiguous.

    If UNION Ck x Dk = UNION Aj x Bj in A then we need to show that the
    Pi-values for both representations are equal. We can do this by intersecting
    both sides and get a double union that looks like

        UNION (k) UNION (j) (Ck intersect Aj) x (Dk intersect Bj).

    We won't show that this is zero (but it is!) so Pi does what we want it to do.
*** Showing that Pi is a premeasure
    Right now we have a well-defined function Pi on an algebra A. We know that

        Pi(E x F) = mu(E) nu(F)

    If we can show that Pi is a premeasure then by 1.14 we are done. It is!
    Therefore we have that Pi extends to a measure on M x N. This is the minimal
    extension. Additionally, if mu and nu are both sigma-finite then Pi is
    sigma-finite on AA, so Pi has a unique extension from AA to M x N.

    If (X, M, mu) and (Y, N, nu) are each sigma-finite then there is a *unique*
    extension mu x nu on (X x Y, M x N) with the property

        mu x nu(E x F) = mu(E) nu(F).
** Fubini's Theorem
** Theorem 2.37
   Assume that (X, M, mu) and (Y, N, nu) are sigma-finite measure spaces.
** Product sigma-algebra
*** Points
    p in X x Y is p(x,y) such that x in X, y in Y.
*** Coordinate Maps
    These are the functions

        Pi1 : X x Y -> X; Pi1(x,_) = x
        Pi2 : X x Y -> X; Pi2(_,y) = y
*** How can we build a sigma algebra?
    Note that we have to be careful here. Consider the sigma algebra

        {X x Y, nullset}

    then Pi1^(-1)(E) = E x Y, where if E is in MM then (E, Y) is not in the new
    sigma-algebra.

    There are a couple things we want to ensure here. For example

        (E x Y) intersect (X x F) = E x F

    where E x F is a _measurable rectangle_. We want the sigma-algebra to
    contain all sets that look like that.

    It turns out that this is the right definition! The smallest sigma-algebra
    we care about is the sigma-algebra generated by the collection of all
    measurable rectangles, or

        {E x F : E in MM, F in NN}.

    Note that the sigma algebra generated by

        {E x Y : E in MM} UNION {X x F : F in NN}

    is the same sigma algebra.
*** Proposition 1.4'
**** Statement
     /* This is slightly different than Folland's version; we are only using two
     measure spaces */

     Suppose that we have a collection of sets E1 subset M such that E1
     generates M and E2 subset N generates N. Then the product sigma algebra M x
     N is generated by the measurable rectangles

         {E x F : E in E1 or E = X, F in E2 or F = Y}

     /* if we just used {E x F : E in E1, F in E2} we would not have
     compliments; we have to ensure that E and F can be their whole sets
     respectively */

     _Brief counter-example to Exclusion of X an Y_
     Let X = RR = Y, let E1 = E2 = {(-oo, 0]}, so

         M = N = {RR, nullset, (-oo, 0], (0, oo)}

     Therefore M x N contains the four quadrants. If we did *not* include RR

         M = N = {(-oo, 0] x (-oo, 0] and the complement of that thing}

     which only includes quadrant 3 and not(quadrant 3). These are not the same!
**** Proof
     Let FF = {E x F : E in E1 or E = X, F in E2 or F = Y}

     Clearly FF is contained in the product sigma-algebra, so the sigma-algebra
     generated by F is a subset of M x N.

     The set of E for which E x Y in M x N forms a sigma-algebra which contains
     E1 because E1 subset M x N. Therefore it contains all E in MM.

     The set of F for which X x F in M x N forms a sigma-algebra which contains
     E1 because E1 subset M x N. Therefore it contains all F in MM.

     Likewise the set of all F such that X x F in M(FF) is a sigma-algebra and
     it contains E2. Therefore it contains all F in NN, i.e. X x F in M(FF) for
     all F in N.

     Therefore

         E x Y, X x F in MM(FF)

     for all E in M and F in N. Therefore MM(FF) contains all such E x F
     Therefore M x N subset MM(FF). Therefore they are equal.
*** Is the product sigma algebra of a pair of Borel sets a Borel set?
    Suppose that X and Y are metric spaces with metrics dX and dY. For G subset
    X to be open means

        for every x in G, there is an epsilon > 0 s.t. the epsilon-ball around
        x is in G, or {w in X : dX(x, w) < epsilon} is a subset of G.

    Therefore we have a notion of open in both axii. What about the product
    space? The product topology is the metric topology using the product metric
    (that is, how do we measure things in the product space?)?

        Use dXY(x,y) = max(dX(x,u), dY(y,v)) for two points (x,y) and (u,v).

    The product metric creates some sort of 'open square' with sides
    2*epsilon. Note that if we used the usual pythagorean-style metric we would
    get the same things (an open set with one contains an open set with the
    other).

    The main reason this metric is the 'right' one is that our sets are now
    already rectangles.
*** Proposition 1.15'
**** Statement
     If X and Y are metric spaces and we give X x Y the product topology
     (i.e. use the product metric) then

         BX x BY subset B(X x Y)

     with equality if both X and Y are separable (that means it has a countable
     dense subset). We care most about RR x RR so we usually get what we want.
**** Proof
     If U is an open subset of X and V is an open subset of Y, then the
     cartesian product of these two sets will be open in the product space (look
     at the open balls around v in V and u in U; pick the minimum value of
     epsilon to form the open rectangle).

     Note that this includes the case U = X and V = Y. Therefore by proposition
     1.14' we have that each U x V generates a product sigma-algebra. Therefore
     the sigma algebra generated by BX x BY is a subset of B(X x Y).

     For the other direction: we need to show that all the open sets in the
     product space are in the cartesian product of the two
     sigma-algebras. *Difficulty*: in Topology arbitrary unions are fine, but
     for measure theory we need countable unions; this is why we need the extra
     thing for separable.

     Suppose that X and Y are separable; i.e. there exists C1 subset X and C2
     subset Y countable and dense. Then C1 x C2 is countable and dense with
     respect to the product measure (that is, given any (x,y) in X x Y we have
     sequences {xn} and {yn} in C1 and C2 that converge to x and y
     respectively).

     It follows from this that our open set O is the union of some set of open
     balls with centers in the dense set and rational radii. There are countably
     many balls of this type. Therefore O can be written as a countable union of
     open balls under the product metric. Each such ball looks like

     {(x, y) : max(dX(u,x), dY(v,y)) < r} < {x : dX(u,x) < r} U {y : dY(v,y) < r}
     in BX x BY.

     Pick any p in O. Then Bepsilon(p) subset O for some epsilon > 0. Therefore
     there exists some c in C with d(p, c) < epsilon/2. Then there exists r in
     QQ with d(p, c) < r < epsilon/2.

     We claim that Br(c) subset O. THen d(p,c) < r, so p in Br(c). If q in Br(c)
     then

         d(p, q) <= d(p,c) + d(c,q) < epsilon

     so q in Bepsilon(p) subset O.
** Measurability of multivariate functions
*** Proposition 2.14'
**** Statement
     Consider f : Z -> X x Y, where G is a sigma-algebra on Z.

     f is (G, M x N) measurable if and only if each pi_i . f is measurable.
**** Proof
***** Forward Direction
      Observe that pi_i are measurable (M x N, M) or (M x N, N) respectively. To
      see this, note that for any E in M

          pi_1^-1(E) = E x Y in  M x N.

      so if f is measurable then f : Z -> X x Y and pi_1 are both measurable; by
      composition pi_1 . f is measurable.
***** Backward Direction
      Conversely, suppose that both pi_i . f are measurable. To show that f is
      measurable it suffices to show that f^(-1)(E x F) in G whenever E in M and
      F in N.
          As F^(-1)(E x F) = (pi1 . f)^-1(E) INTERSECT (pi2 . f)^-1(F), by
      hypothesis both of these sets forming the intersection are in G.
*** Application: Addition of Measurable Functions
    let Phi(x,y) = x + y. This function is continuous on RR2, so with the Borel
    sigma-algebra (B_RR x B_RR, B_RR) this function is measurable.

    Say that f, g :: X -> RR are measurable. Then F = (f, g) :: X -> RR x RR is
    measurable by 2.14. Therefore

        Phi . F = f(x) + g(x)

    must be a measurable function. Same for products (proposition 2.6).
*** Set-up: measuring functions with multiple inputs
    Pick y0 in Y. Define the injection phi(x) = (x, y0). Therefore phi(x) : X ->
    X x Y. We claim that phi is measurable (for (M, M x N)).

    Consider E x F; then phi^(-1)(E x F)  E, if y0 in F and nullset if y0 not in
    F. This is always in M. Therefore phi is measurable. Also:

        phi^(-1)(A) = {x in X : (x,y0) in A}

    we sometimes all this the A_y0 cross section of A. Similarly psi(y) = (x0,
    y) is (N, M x N) measurable. This is sometimes called A_x0 (same idea as
    before).

    If f(x,y) is (M x N, G) measurable then both f . phi and f . psi are
    measurable (these freeze one argument of f). As usual these are notated by
    f_y0 and f_x0.
*** Proposition 2.34
**** Statement
     Assume that (X, M) and (Y, N) are measurable spaces and f : X x Y -> Z is
     (M x N, G) measurable.

     1. If A in M x N then Ax in N, Ay in M for all x in X, y in Y.
     2. fx : Y - Z and fy : X -> Z are (N, G) and (M, G) measurable.

     The converse of this is not true! Recall that if a function is continuous
     with respect to x and with respect to y it may still fail to be continuous.
**** Counter-Example to The Converse
     Let A in RR be nonmeasurable. Define f : R x R -> R by f(x,y) = 1 if x = y
     in A and zero otherwise.

     If we freeze an y0 then we get that f(x, y0) = 0 if y0 not in A, 1 if x0 =
     y0 in A. This is a measurable function (indicator function for a single
     point) for each y0.

     Now consider the function g : RR -> RR x RR defined by g(x) = (x, x). This
     is a measurable function (recall what we did with composition of
     pi_i). Therefore if f is measurable then f . g is measurable; however this
     function is

         f . g = f(x,x) = 1 for x in A, 0 for x not in A

     which is not measurable.
**** Proof
     See the previous work.
* Running Tab
** Summary for 10/24/12
   L1 is defined in terms of equivalence classes, not functions. We do this so
   that the metric

       d(f, g) = INTEGRAL abs(f - g) dmu

   makes sense (if d(f, g) = 0 then [f] = [g] in the set)

   Consider (X, MMbar, mubar) (the completion of (X, MM, mu)).

   If f, g are MM-measurable then they are MMbar-measurable.
   f = g a.e. in MM -> f = g a.e. in MMbar.

   If f is MMbar-measurable we know (proposition 2.12) that there is an
   MM-measurable g such that f = g a.e.; that is each MMbar equivalence class has
   a MM representative. Therefore there is a 1-1 correspondence between MM and
   MMbar.

   We also did the proof for the dominated convergence theorem. The general idea
   is to use lim inf and lim sup with Fatou's lemma.

   Look at Theorem 2.25 in the book (we did not prove it in class, but Dr. Day
   promises that Folland's proof is very nice).
