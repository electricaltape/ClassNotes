* General Tips

    Never compute the inverse!

    Use properties of special matricies.

    Proofs on homework - don't be afraid to go back.

    modern mathematics is about structure! We do not care about sets anymore;
    anything worthwhile has structure behind it, certain properties, etc

    In modern math, the disjoint-ness from earlier decades is becoming
    blurred. People work more on modeling than just 'Ax = b'. Once we model it,
    how do we solve it?

    More data, more trouble.

    Goal of the class is to learn general classes of methods, rather than
    particular techniques.

* 9/9/11
** Pivoting
s_i = max over 1 \leq j \leq n \abs{a_{ij}}
scale of the ith row - use for another pivoting policy

** LU by GESRP

Set A1 = A
    scan the first column of A1 : set a_{r1,1} / s_r1 := (max A[:,0])
    exchange that row r1 and row 1 of A1 to form A~1, or
    A~1 = I_r1,1 A1 .

for true numerical analysis, need to know/do:
    understand troublesome parts
    understand algorithm by hand
    use examples to check code.
    we like test-first coding practices - each part valid.

for A = 0 1 1
        1 2 3
        1 0 2

s1 = 1, s2 = 3, s3 = 2 (largest element in each row)
max [a_31 / s_3 , a_11 / s_1, a_21 / s2] = a_31/ s_3 so use r_1 = 3.

Therefore R1 -> R3 so carry out elimination.

Most stable is full pivoting, but that is generally difficult. generally
partial pivoting is as good (a belief, not a truth)

** Applications of LU factorization
    solve Ax = b - O(n^3/3) long operations (optional hw - find exact number)
    (should be less, as there is less division)
    Much worse strategy - compute determinants and solve by
        Cramer's rule - (n + 1)(n - 1)n! operations (far worse.)
    quickly compute the determinant of a matrix
        works because det(AB) = det(A)det(B)
        and the determinant of a triangular matrix is the trace
        so det(P)det(A) = det(L)det(U), det(L) = 1, det(U) = trace(U)
    find rank(A)
    solve Ax = b1, Ax = b2, Ax = b3, ... good for gaming calculations
        (moving polygons/vectors)
        compute inverse(A) - requires O(4n^3/3) long operations
        repeatedly solve for [1 0 0], [0 1 0], [0 0 1] etc to form columns

** solving LU factorization systems
    Ax    = b
    PAx   = Pb
    (LU)x = Pb
    so solve Ly = Pb -> Ux = y

    Algorithms are already coded and available. Nothing particularly new.
    matlab : [L,U,P] = lu(A)
             y = L \ (P * b)
             x = U \ y
    Google netlib


* 9/12/11
** LU factorization of RDD matricies
   Pay attention! important for scientific computing.
   diagonally dominant matrix
       - magnitude of diagonal entry \leq sum of magnitudes of other entries
       - strictly : use > not \leq
   Theorem 3
   - If GE is applied to fint the LU factorization of a strictly RDD matrix A,
     A^(k) = (A_(ij)^(k)) k in NN in the elimination procedure are strictly RDD
   - every RDD matrix has an LU factorization.
   - the LU based on GESRP for a RDD matrix is the same as the LU based on GE.

   kinda nice - for RDD we do not need pivoting. The greatest entry is already
   on the diagonal! We can handle LU faster, more accurately.

   Proof of Theorem 3
   Let A = (a_ij) nxn be a strictly RDD matrix. Prove R^(2) is strictly RDD and
   similar arguments can be shown for A^(3), A^(4), etc.

   so show that | a_ii^(2) | > sum off-diagonals. (slide 59)

   follow the algorithm for the second row and use the property of diagonal
   dominance from the first row.
** LU factorization of SPD matricies
   Lots of physical problems (like heat flow) are symmetrical.

   SPD <-> following properties:
   - A = transpose(A) (symmetrical)
   - (transpose x) A x > 0 for x /= 0 (positive definite, by test vector)

   SPD matricies are nice! Check out some properties:
   - A nonsingular, (inverse A) SPD
   - leading principle submatricies are SPD
   - principal minors of A are positive.
   - eigenvalues are positive.

   Lemma 1 - go backwards.
   - if all eigenvalues of A are positive then A is SPD
   - if all principal minors are positive then A is SPD

   SPD is symmetric - we should only need to look at half of it.

   Theorem 3.8 - (SPD? A) <-> exists L s.t. A = L(transpose L)

   (LLT is cholesky factorization!)
   Proof. assume A = L(transpose L), so A symmetric. (show A positive definite)
   let y = (transpose L)x so (nonzero x -> nonzero y). Then

   (transpose x) A x = (transpose x) L (transpose L) x
                     = (transpose y) y
                     > 0, for x /= 0.
   so A is SPD.

   Now assume A is SPD so A = LU. L is unit lower triangular. Note

   P_k(A) = P_k(L) P_k(U) (slide 64)




* 9/14/11
** Theorem 3.8
   Let A be symmetric, nxn. A is SPD iff A + L(transpose L)
   Better explanation in slides. Shows that if we have a symmetric matrix we
   don't need to do LU!
** Cholesky Factorization
   Given some SPD matrix we can compute L with the following algorithm:
   do k = 1,n
       l_kk = sqrt ( a_kk - sum (l_{ks})^2, for s = 1 to k-1)
       do i = k+1,n
           l_{ik} = 1/l_{kk} (a_{ik} - sum l_{is} l_{ks} for s = 1 to k-1)
       end do
   end do

   operation count is homework! show that it is n^3/6 + O(n^2).

   This algorithm is nice and stable - no need to pivoting. It also is faster
   than LU factorization (n^3/6 instead of n^3/3 multiplications)
** Sparse Matricies
   First challenge - storage space. Not as big a problem as it once was, but
   still nontrivial.

   More examples of sparse matricies - telephone grids. Represent with a matrix
   of ones and zeros - if two are connected, write 1. Otherwise write 0.

   Some references : Saad (sparse book), Meschach (noncommercial sparse
   package).

   We like sparse matricies - can skip operations on zero entries, save a lot
   of time.

*** Coordinate Format
    Goal - don't store zeros.
    Accordian format.

    one array - store nonzero entries in any order.
    two other arrays - store row and column indicies.

    This requires 3Nz entries, where Nz is the number of nonzero entries.

*** Compressed Sparse Row format
    Real array to store entries row by row
    Integer array J to store column indicies
    Integer array of pointers to where each row starts.

*** Tridiagonal matricies
    A square matrix A = (a_ij) is said to be tri-diagonal if a_ij = 0 for all
    i, j such that | i - j | > 1.

    Nice properties - principle minors are nonsingular if:
    |a_1| > |c_1|
    |a_k| \geq |b_k| + |c_k|
    |a_n| > |b_n|

    Proof by induction: show that the first 2x2 matrix is fine, assume nxn,
    show (n+1)x(n+1).

    How can we compute something that is tridiagonal? Factorize and solve.

**** Trick - Crout factorization = Thomas algorithm

     Crout requires about 2n operations for solution.

     Nice feature: superdiagonal entries of U are always less than 1.
     Nice feature: subdiagonal entries of L are bounded.

     Turns out Crout is very stable - we like it.

     Like 'undetermined matrix'
     Propose that we factor some tridiagonal matrix by
     A = (alphas on diagonal, bs on subdiagonal) times
         (ones on diagonal, gammas on superdiagonal)
     What happens when we multiply them?
     get a tridiagonal back.
     so row 1 = (alpha1, alpha1*gamma1, 0, 0, ...)
     so row 2 = (b2 , b2* gamma1 + alpha2, alpha2 * gamma2,    0, 0, ...)
     so row 3 = (0  , b3,                  b3*gamma2 + alpha3, alpha3*gamma3,
     0, 0, ...)

     set this equal to the LHS, so alpha1 = a1, alpha1*gamma1 = c1, etc

     follow in repetition, for k = 2 .. n - 1
     alphak = ak - bk*gamma(k-1)
     gammak = ck / alphak

     operation counts: 2n - 2 (nice!)


* 9/16/11
** Announcement
   Test in a week! May be take-home.
   Do exercises, read books, do research: we have to take tests too :(

** Features of Crout : Theorem 4
   For meeting the conditions (see slide 73/195):
   (diagonally dominant, last and first rows diagonally dominant)
   see slide 77/195

** Normed Linear Spaces
   We have to talk about error - that means norms.
   our solutions are vectors so we need norms - how do we compare?
   Another problem - how do we call them 'large' or 'small'?

   RR^n - set of length n arrays. What operation? Norm! Inner Product! Vector
   addition!

   Matricies - normally boring containers. Not interesting. We need operations
   to make them worthwhile. Matricies have structure and addition.

   On RR^2 : f(x1, x2) = e^x1 + e^x2 - not a norm on RR^2 as f(0,0) /= 0
   norms are just special functions of RR^n.

   We can also create a distance function : d(x,y) = || x - y ||
   (a metric, relates vectors)
   we want d(x,y) to be small to say that x ~ y : one number to gauge accuracy.

*** Some specific norms
    L_p norm : Holer's norm  on CC^n : for p geq 1, L_p norm is
    norm : ||x||_p = ( Sum |x_i|^p)^(1/p)

    so p is arbitrary, p can be a real. We use p=1, p=2, p=inf most often.




* 9/19/11
** Test
   Allowed to bring 1-page of information (open sheet, 8.5 x 11)
   In-class exam
   will have LU factorization (at most, 3x3; no huge problems)
   able to do 3x3 LU factorization by hand.

   A lot of error estimate style things, like chapter 1.

   Bring a calculator. Should not need it, but it is nice.
   Test covers all material up to Wednesday. Review slides!

** More on normed linear spaces
   Use the entries of two vectors to form a single number:
   Cauchy-Schwarz : sum of x_i bar(y_i) leq 2-norm(x) * 2-norm(y)
   This is similar to the triangle inequality:
   norm(x + y) leq norm(x) + norm(y)

** Vectors and matricies
   matricies tend to be functions and vectors tend to be 'variables'. We should
   study matricies too!
   Say that the norm of a matrix, norm(A) geq 0. norm(A) = 0 iff A = 0. All the
   usual properties of norms hold.

*** Compatible Norms
   We say that a matrix norm and a vector norm are compatible if
   norm(Ax) leq norm(A)norm(x), forall x, forall A.

**** Frobenius Norm
     not Natural/Induced/Subordinate : square root of the sum of squares
**** P norm
     equivalent of p norm for vectors.

*** Natural/Induced/Subordinate matrix norms
    for some vector norm norm(), we say it is Natural/Induced/Subordinate if

    norm(A) = sup(x /= 0) norm(Ax)/norm(A)

    not a nice definition - needs supremum. We can find a beter finite formula.

*** Proposition 3.4 - norm(A, inf)
    compute it by the max absolute row sum.
    so: find absolute values of all entries, sum each row, take the max rowsum

    Similarly: norm(A, 1) is the maximum absolute column sum.

**** TODO prove rest as informal HW
     for norm(A, inf) we have

     norm(Ax) = max of the absolute sums of of rows by definition. Rearrange
     with leq. See slide 87.

*** Spectral Radius
    Assume a matrix has eigenvalues lambda_i. Call

    rho(A) = max |(lambda_i)|

    and call it the spectral radius of A.

    why radius? Create a circle with radius max | lambda_i |. This will inclose
    all eigenvalues in the complex plane.

**** Theorem : norm(A,2) = sqrt (spectralRadius((conjugate-transpose A) * A))
     Proof : Note that (conjugate-transpose A) * A is symmetric and has n
     orthonormal independent eigenvectors. v_i, and

     lambda_i = (Av_i, Av_i) geq 0.



* 9/21/11
** Spectral Radius and Norms
   For any matrix norm we have that rho(A) leq norm(A)
   (spectral radius is always leq norm)

*** Proof
    Assume norm is induced and lambda is an eigenvalue. Then exists x /= 0
    s.t. Ax = lambda x.

    Then |lambda| || x || = ||lambda x|| = ||Ax|| leq || A || || x ||

    which implies that |lambda| leq || A ||

    note that the spectral radius is the largest eigenvalue so we are done.

*** Can we bound the matrix norm by the spectral radius? No.
    A = [0 2; 0 0] so || A || = 2, but rho(A) = 0.

*** Schur Decomposition - necessary details

    forall A, exists nonsingular matrix P and an upper triangular matrix T s.t.

    PAP^-1 = T

    additionally, given nonsingular B, ||x|| = ||Bx||2 is a norm.
    (useful only theoretically)

*** Proposition 3.6 - from above facts. (implicit homework)
    Given an nxn matrix A and epsilon > 0 :

    exists norm s.t. || A || leq rho(A) + epsilon

**** Proof
     Let P, T be the matricies of the Schur decomposition of A and let

     T = Lambda + U
     Lambda = diagonal matrix of eigenvalues of A
     U(i,j) = 0 for i geq j.

     Then for any delta > 0, can form

     D^-1 = diagonal matrix of 1, delta, delta^2, ...
     C = DTD^-1 = Lambda + E
     E = DUD^-1.

     Because DP is nonsingular, we introduce:

     vector norm || x || = || DPx ||2 = (x^H P^H D^H DPx)^(1/2)

     and further

     an induced matrix norm || A || = supremum over ||y|| = 1 ||Ay||.

     Let z = DPy. If ||y|| = 1 then ||z||2 = ||DPy||2 = ||y|| = 1.

     Note that A = P^-1 T P = P^-1 D^-1 CDP, DPAP^-1D^-1 = C.

     So, again for ||y|| = 1, we get

     that ||Ay||^2 = = ||DPAy||2^2 = || mess ||2^2 = ||Cz||2^2 = gets
     worse. See slide 93.

** Matrix Norms and Sequences
*** Proposition 3.7 - equivalency of matrix norms
    We can bound any two norms with constants c1 and c2 by something like

    c1 || A ||_alpha leq || A ||_beta \leq c2 || A ||

*** Definition 3.27 - Sequence of matricies
    sequence of matricies {A_k} converges to A iff

                    lim (k -> inf)|| A_k - A|| = 0.

    (this will be important in the second half of this chapter, iterative
    methods)

*** The following are Equivalent

    1. lim (k -> inf) A^k = 0.
    2. lim (k -> inf) A^k x = 0 forall x in CC^n.
    3. rho(A) < 1.
    4. exists norm s.t. norm(A) < 1.

    there are more that can go here.

**** Proof
     (1 -> 2) norm(A^k x) leq norm(A^k) norm(x)
     therefore
     0 leq lim (k -> inf) norm(A^k x) leq lim (k -> inf) norm(A^k) norm(x) = 0

     Therefore it is squeezed between 0 and 0; goes to 0.

     (2 -> 3) let lambda be an eigen value, so |lambda|^k must go to zero ->
     lambda < 1.

*** Gathering up the eigenvalues

    Call sigma(A) the set of all eigenvalues of A.

    If P(x) is a polynomial then we get something like

    sigma(P(A)) = {P(lambda), lambda in sigma(A)}

    we can even extend this beyond polynomials to analytical functions (exp(A),
    sin(A) all defined... interestingly)

    Even more interesting: sigma(A^-1) = P(1/x) evaluated at lambdas. We know
    the eigenvalues with out knowing the matrix! Wow. Magical!
**** Example

     P(x) = 1/sqrt(3) x^2 + pi x + \sqrt(2) I

     Hard to evaluate. However, easy to find eigenvalues.

** Geometric series

   I + A + A^2 + A^3 + ... converges iff lim (k -> inf) A^k = 0

   Also: if the limit exists then I - A is nonsingular and

           Sum (from k=0) (to infinity) A^k = (I - A)^(-1)

**** Proof
     Assume that lim (k -> inf) A^k = 0. Then

     rho(A) < 1 by Theorem 3.5 .
     then sigma(I - A) = stuff. See slide 97.


* Numerical Test
** Backward error
   Wikipedia : delta x s.t. f(x + delta x) = y* (what problem the algorithm
   actually solved)

   Sauer : Say that f(r) = 0 (a root), xc ~ r. backward error is f(xc). This is
   the amount we need to change the input by to get the approximation out.
*** Examples
**** Matrix
     Say we solve a matrix problem Ax = b and get result x*. Then the backward
     error is delta b in:

     Ax* = b + delta b, find delta b.

     Better example, from Purple Book : for backward error, show that x* is the
     exact solution of

     (A + F) x* = b.

**** Sine button
     (from Sauer) say that we find an approximation to sin(x) - x = 0, xc =
     0.001. Backward error is 1.6e-10 (amount we need to change f(xc) by)

** Forward error
   Wikipedia : difference between result and solution

   Sauer : Say that f(r) = 0, xc ~ r. Then forward error is r - xc. This is the
   amount we need to change the approximation by to make it correct.

**** Sine button
     Same example as above: sin(x) - x = 0, xc = 0.001. Plug it in and get r =
     xc = 0 - 0.001 = 0.001

** LU Factorization
   L unit lower -> Doolittle factorization (L(k,k) = 1 at kth step)
   U unit lower -> Crout factorization (U(k,k) = 1 at kth step)
*** Solving
    from A = LU, solve Ly = b -> Ux = y.
*** Building
    Create U by row operations (Gaussian elimination)
    Store the -1*(row multipliers) in L (with unit diagonal.)
    takes about n^3/3 multiplications/divisions.
*** Pivoting
    Solve PA = LU instead - put largest entries on diagonals.
    Find the largest entry in the 1st column and row swap so it is now at the
    pivot. Reduce.
    Repeat the same thing for each column.

** Elementary Matricies
   Describe the 3 elementary row operations
*** Constructor
    ElementaryMatrix E = ElementaryMatrix Vector u, Vector v, Float sigma
    ElementaryMatrix u v sigma = IdentityMatrix - sigma * u * (conj. trans. v)
**** Exchange rows
     u = v = e_i - e_j to interchange rows i and j
     E = I - (e_i - e_j)(e_i - e_j)^T

     also referred to as I_ij : I_ij A permutes ith and jth rows of A
     A I_ij permutes ith and jth COLUMNS of A
**** Scale row i by alpha
     u = v = e_i
     sigma = 1 - alpha
     so E = I - (1 - alpha)e_i e_i^T

     then EA scales the ith row by alpha.
**** alpha * R_i + R_j -> R_j
     u = e_j, v = e_i
     sigma = -alpha
     E = I + alpha e_j e_i^T

     then EA does what we want.
*** Nice properties
    u is an eigenvector of E if u /= 0 : eigenvalue is 1 - sigma v^H u.
    if v^H x = 0 then x is an eigenvector, eigenvalue 1.
    if sigma /= 0, v /= 0, and u^H v /= 0 then E has two eigenvalues:
    1. 1 n-1 times
    2. 1 - sigma v^H u one time

    If 1 - sigma v^H u /= 0 (the determinant, see above) then

    E^-1 = E(u, v, -sigma/det(E))
** Gaussian transformation matrix
   special case of the elementary matrix
   L_j(l_j) = E(l_j, e_j, -1)
   where l_j = [0, 0, .. l[j+1,j], .. , l[n,j]]^T.

   looks something like the indentity matrix where one column, below the main
   diagonal, is filled with l[j+1, j] etc
*** Properties
    L_j(l_j)^-1 = E(l_j, e_j, 1) = -1 * L_j(l_j)

    in particular, for a lower triangular matrix:

    L = L_1(l_j) L_2(l_2) .. (product of Gaussian transformation matricies)



** Things to put on cheat sheet
*** Chapter 1
    Lagrange form of interpolation error
    MVT for integrals, MVT plain
    IVT
    Taylor's Theorem
    something about the problem with error analysis in PS2 p3
    O/o notation
    Machine arithmetic fun facts
*** Chapter 3
    LU formula - GEPP, GESRP
    Cholesky formula
    Sparse matrix storage.
    Thomas algorithm
    Spectral radius and norms


* 9/26/11
** Powers of a matrix

   the geometric series

   I + A + A^2 + A^3 + ... (to infinity)

   converges iff lim (k -> infinity) A^k = 0.

   If the limit exists then I - A is nonsingular and

   sum (k from 0 to infinity) A^k = (I - A)^-1

*** Proposition 3.9
    if the matrix norm ||A|| < 1 then both I - A and I + A are nonsingular, and

        1/(1 + norm(A)) leq norm((I plusminus A)^-1) leq 1/(1 - norm(A))

    (proving the plusminus is homework.)

**** Proof
     by previous work : rho(A) leq norm(A) < 1. Then by definition of rho (and
     the fact that I +/- A is a polynomial) then the eigenvalues of (1 +/- A)
     are 1 +/- rho(A) /= 0 (as rho(A) is the max eigenvalue)

     Therefore both have nonzero eigenvalues, so they are invertible.

     next part: I = (I + A)(I + A)^-1

     so by norm properties (see slide 99, mostly triangular tango) we have that

     1 = norm((I + A)^-1) * norm(I + A)

     so 1/(1 + norm(A)) leq norm((I + A)^-1)

     other part: show that I + A has an inverse, then distribute and
     rearrange with more triangular tango - more slide 99.

     Works as (1 - norm(A)) > 0 by assumption.

     (this is usually combined with 3.8 and is called Banach's lemma)

**** Example
     See slide 100 - for a infinity norm of 0.6 then (I + A)^-1 and (I - A)^-1
     should exist. If we go and calculate the inverses and (I +/- A)^-1 we get
     that they do fall between the bounds.

** Roundoff Error and conditioning in Gaussian Elimination
   Numbers on computers - always some error. LU, GE, Cholesky - all corrupt.
   Ax = b -> computer sees A + dA, b + db. Start off badly - can't even enter
   the problem!

   so the computer handles what WE would call (A + da)(x + dx) = b + db

   how large is dx? forward error analysis.

*** Theorem 3.10 (more general version)
    Assume that some beta-norm is an induced matrix norm, A is nonsingular, and

    norm(dA, beta) norm(A^-1, beta) < 1.

    then

    norm(dx, beta)/norm(x,beta) leq combination of norms of A, B, dA, db,
    A^-1. Bounded!

    therefore, to a constant, the solution error is proportional to the data
    error.

**** Proof - standard things.
     Distribute (A + da)(x + dx) = b + db. Recall that Ax = b to simplify the
     result (every term should have a d on it).

     see slide 103 and use Banach's Lemma twice - just brute force field
     algebra from there on.
*** Conditioning
    We say that the problem is ill-conditioned if a slight change in A or b
    drastically changes x. The factor from the work before is called the
    condition number (again, it is very long. See slide 103).

    condition number = K_beta(A) = norm(A,beta) * norm(A^-1,beta)


* 9/28/11

** Computing Project 1
   coming up. Most code will be supplied.

** Condition Numbers
   If the solution Ax = b changes 'drastically' when A or B are perturbed we
   call it 'ill-conditioned'

   Call K_beta (A) = norm(A, beta) * norm(A^-1, beta) to be the condition
   number of A
   why? All the neat things we derived! See above slides.

   Computing the condition number is not easy, but important

   always have that the condition number is geq 1. homework - show that it can
   be one (namely identity, others work as well)

*** Condition number in 2-norm
    for A^H A has eigenvalues mu1 geq mu2 geq mu3 geq ...

    then (A^H A)^-1 has eigenvalues 0 leq 1/mu1 leq 1/mu2 leq ...

    As we know that norm(A,2) = sqrt(spectral radius of (A^H A)) = sqrt (mu1)
    so norm(A^-1,2) = sqrt (1/mu_n)

    so for a small matrix, we can calculate by finding eigenvalues of A^H A.

**** Hermitian A
     Then A^H = A, so we want eigenvalues of A^2. THerefore the eigenvalues are
     (mu)^2s. Therefore we get

     cond(A) = abs( max(eigenvalue)/min(eigenvalue) )

** Error in Gaussian Elimination
   solving Ax = b yields xhat. By backward analysis we get

   (A + F)xhat = b (the problem we actually solved)

   Ax - Axhat = -F xhat
   so x - xhat = -A^-1 F xhat.

   then (with some forward analysis)

   norm(x - xhat,inf) / norm(xhat,inf) leq norm(A^-1,inf)norm(F,inf)
                   = K_inf(A) norm(F,inf) / norm(A,inf)

   Someone showed something like
   norm(F,inf)/norm(A,inf) leq c_n g theta

   where c_n depends on the size of A
   g is a constant factor in Gaussian Elimination

   g = max over i,j,k of | a_ij^(k) | / max over i,j of | a_ij |

   theta is the roundoff error on the machine.

*** generally

    c_n = 1.01 n^3 + 5(n + 1)^2

    for complete pivoting:
    g leq (n 2 3^(1/2)4^(1/3)...n^(1/(n-1)))^(1/2) (thanks, Wilkinson)

    for partial pivoting:
    g leq 2^(n-1)

    no pivoting: g can be arbitrarily large (!!!)

    in most applications, the growth factor is much smaller. Good research area
    (finding tighter bounds based on applications)

** Iterative Methods for solving Ax = b

*** TODO : update with neat general info from slides.

** Classical methods
   Choose M such that solving My = g is easy.

   Let A = M - N. Then Ax = b -> Mx = Nx + b

   Choose a guess x^(0) and update by Mx^(k+1) = Nx^(k) + b.


* 9/30/11
 cm** Basic ideas for classical iterative methods
   Strategy - split so that My = g is easy to solve.

   Set A = M - N
   Ax = b -> Mx = Nx + b
   guess x0, find x1 by Mx1 = Nx0 + b, easy to solve by design.

   We can rewrite this for B = M^-1 N, c = M^-1 b, as
   x^k+1 = Bx + c (theoretical! can't invert.)

   this is useful for analyzing convergence. We will see why soon.
   (lots of homework discussing the convergence of this sort of thing)

*** How do we turn this in to a finite procedure?
    Usually artificial (best one - physics of the application)
    one way to do it - see if xk, xk+1 close. Not much improvement means we are
    not getting anywhere (so stop)

    another way to do it - look at the residual of the solution (substitute in
    Axk - b, solve and check residue)

    yet another way - set number of iterations (very artificial)

    can mix and match!

*** Classical ways to split A
    splitting - not factorizing
    say A = L + D + U (free to do)

**** Jacobi method

     set M = D, N = -1(C_L + C_U) (C_L is lower diagonal entries of A, C_U is
     upper)

     so for Jacobi - Dxk+1 = -(C_L + C_U)xk + b.

**** Gauss-Seidel method
     ALso calles successive relaxation.

     M = C_L + D, N = -C_U

     then (C_L + D) xk+1 = -C_U xk + b (solve and update! new vectors depend on
     the old vectors)

     this is were the 'successive' relaxation comes from.

*** What is the difference?

    Jacobi needs xk and xk+1, Gauss Seidel stores xk and sequentially
    overwrites.

    Jacobi, however, may be done in parallel! However in sequence GS is faster
    (we will see why soon)

*** Successive Overrelaxation
    Best for the 60s
    M = C_L + 1/sigma D
    N = - [ C_U + (1 - 1/sigma)D]

    relaxation parameter sigma.

    derivation - A = D + C_L + C_U
    therefore 1/sigma D x + (D + C_L + C_U)x = 1/sigma D x + b
    therefore (C_L + 1/sigma D)x = -[C_U (1 - 1/sigma) D]x + b
    (this is a fixed point form, so we may iterate)

    This is the same as Gauss-Seidel if we pick sigma = 1.

**** How do we pick a good sigma? Depends on each problem.
     Very fast for good sigma values. However, if we have an odd problem, no
     tuned sigma -> slow.

     sigma < 1 -> underrelaxed (generally bad, but not always)
     sigma = 1 -> Gauss-Seidel
     sigma > 1 -> overrelaxed (most common)

** Comparison of Jacobi, Gauss-Seidel, and SOR
   can we always compute? Will they always converge? (no)

*** Convergence
    say convergent if it works for all starting points.

    errork = x^k - x which implies that e^k+1 = B^k+1 error(0)

    rate of convergence depends upon the initial guess!

**** Theorem 3.14
     the following are equivalent:
     a. iterative method convergent
     b. rho(B) < 1
     c. Exists matrix norm such that norm(B) < 1.

*** Error bounds Slide 124
    From a lot of matrix math, we get that
    epsilonk = - (I - B)^-1 B^k(x(1) - x(0))


* 10/03/11
** More iterative methods
   Review : norm(e^(k+1)) leq norm(B^k+1) * norm(e^0)
   There are two ways to lower the error - better initial guess, get
   norm(B^k+1) closer to zero.

*** How can we get a more concrete bound?
    say that norm(B^k) * norm(epsilon^1) leq TOL * norm(epsilon^1)

    then (norm(B^k)^k)^(1/k) leq TOL, so, using logarithms

    (-1/k log(norm(B^k))) log(tol^-1) .LEQ. TOL

    which gives us that the number of iterations needed is proportional to the
    inverse of the log of the matrix norm.

    Then R_inf (B) + lim R_k(B) = - ln (rho(B)) where rho(B) < 1 for
    convergence.

    Where does this crazy condition come from?

*** Theorem 3.21 : bounding the matrix norm for B
    For any nxn matrix B and any matrix norm, we have
    lim (k to infty) norm(B^k)^(1/k) = rho(B)
**** Proof.
     (rho(B))^k = rho(B^k) leq norm(B^k)
     by proposition 3.3 -> rho(B) leq norm(B^k)^(1/k), for all k.

     Lets use an auxillary matrix. Let

     B(epsilon) = 1/(rho(B) + epsilon) B ->
     rho(B(epsilon)) = rho(B) / (rho(B) + epsilon) < 1

     therefore lim (k to infty) (B(epsilon))^k = 0. The matrix converges!

     this means that the norm of B(epsilon) goes to zero, so for some k > k0 we
     get that

     norm(B^k) / (rho(B) + epsilon)^k = norm(B(epsilon)^k) < 1, forall k
     .GEQ. k0

     Therefore if we take the limit for epsilon -> 0 we get that

     lim (k to infty) norm(B^k)^(1/k) leq rho(B) because epsilon is arbitrary.

*** Asymptotic rate of convergence
    We say that the asymptotic rate of convergence for B is R_inf(B).

*** Theorem 3.16 (Stein, Rosenberg)
    if B_J is nonnegative then B_J and B_GS can satisfy precisely one of the
    following relations:

    1. rho(B_GS) = rho(B_J) = 0
    2. 0 < rho(B_GS) < rho(B_J) < 1
    3. rho(B_GS) = rho(B_J) = 1 (if one spectral radius is 1, the other is as
       well)
    4. 1 < rho(B_J) < rho(B_GS) (if it converges, it converges faster. If it
       diverges, it diverges faster)

    useful - if the iteration matrix for Jacobi fails then the iteration matrix
    for Gauss-Seidel fails as well.

*** Special matricies
    Given some M, consider the absolute value of each entry; call this
    matrix |M|

    we say that M .GEQ. N iff M[i][j] > N[i][j]

    then | AB | .LEQ. |A| |B|.

**** Theorem 5
     if A is strictly diagonally dominant, then
     norm(B_GS) .LEQ. norm(B_J) < 1.
***** Proof
      A = D + C_L + C_U and
      B_J = -D^-1 (C_L + C_U) = -D^-1 (D + C_L + C_U - D) = I - D^-1 A
      (D inverse is guaranteed to exist because of strict diagonal dominance)

      norm(B_J,inf) = norm(I - D^-1 A,inf) = max ( sum (from j=1, j /= i, to j
      = n) abs( A[i][j] / A[i][i])) < 1.

      There are several steps for the first inequality. Know this for prelim.


* 10/05/11
** Recommendation - reread slide 131 onward
** Convergence of the SOR method
   We have two general ways to show convergence - norm(B) < 1 or rho(B) < 1.
*** Kahan Theorem
    spectral radius of the iteration matrix of the relaxation method satisfies

    rho(B_SOR(sigma)) .GEQ. abs(sigma - 1)

    Therefore the SOR method cannot converge if sigma is outside (0,2).
**** Proof
     B_SOR(sigma) = -(C_L + 1/sigma D)^-1 * [C_U + (1 - 1/sigma)*D]
     (we assume that it is convergent, so the matrix inverse already exists in
     the first bit)

     (note that the product of the eigenvalues equals the determinant)

*** Final version
    Theorem 3.15 - based on lemmas from notes that help us along
    Read it! Quick.

*** Optimal Relaxation Parameters
    Read it.


* 10/10/11
** Sparse methods
   Ax = b - nice and linear. But we can solve it faster if we make it
   nonlinear!

   so for the next few lectures, we will reduce Ax = b to a nonlinear problem.

*** Useful tool - inner products.
    We say that (x,Ay) = (A^Tx,y) (we can move linear operators around)
    this is useful for symmetric matricies.

    (x + y, z) = (x,z) + (y,z) among other properties.

    (modern mathematics is about structure! We do not care about sets anymore;
    anything worthwhile has structure behind it, certain properties, etc)

    also recall, for some standard multivariable function : q(x1,x2,x3) =
    q(bar(x))

    so 1/2 * (x, Ax) - (x, (4 5)) = 1/2(x1,x2) A (x1, x2)^T - (x1, x2) (4, 5)^T

    a nonlinear version of a linear equation.

    for A = [ [2,1], [1,3] ] we get

    0.5 * (2x1^2 + x1x2 + x1x2 + 3x2^2) - (4x1 + 5x2)

    why did we rewrite this as some awful nonlinear problem???

**** Theorem 3.22
     instead of direct solution, find a minimal solution:

     x* is the solution to Ax = b iff x* is a minimizer of q(x).

     (for some q(x) = 0.5(x, Ax) - (x,b))

     Proof. As A is SPD, A^-1 exists and is SPD as well. Consider some function

     F(x) = (b - Ax)^T A^-1 (b - Ax)

     which is nice as

     F(x) = (A^-1 (b - Ax), b - Ax)
          = (A^-1 b,b) - (x,b) - (A^-1b, Ax) +  (x, Ax)
          = ((x, Ax) - 2(x,b)) + (A^-1b, b)
          = 2q(x) + (A^-1b, b)

     so if x minimizes q, x minimizes F. Additionally the minimum should be
     unique.

     In general, GRAD q(x) = Ax - b = -r (residual)

*** Following the minimization algorithm
    say we have some initial guess x1. How can we find a better guess? (that
    is, decrease the residual)

    We can use the gradient! The gradient tells us what the steepest descent
    is. If we follow the negative gradient the function q(x) will
    decrease. Therefore the function is guaranteed to decrease on that line so
    we know on what line x2 lies.

    Therefore all we need is some constant, or

    x2 = x1 + t_1 r1 - some scalar variable t.

    Then Q(t)  = q(x1 + tr1) = mess
         Q'(t) = -(r1,r1) + t(r1,Ar1)

    so the best value for t is just (r1,r1)/(r1,Ar1)

**** Does this actually converge?
     consider the A-norm (TODO - show that this is a norm)

     norm(x,A) = sqrt((Ax,x))

     let e^k = x^k - x*, r^k = b - Ax^k

***** Theorem 9 - important results for analysis of convergence. This is also informal HW

      (r^k+1,r^k) = 0 (bad numerically - lot of traveling in parallel
      direction)
      e^k = -A^-1r^k
      r^(k+1) = r^k - t_kAr^k



* 10/12/11
** Review of steepest descent
   Find the optimal scalar for the remainder and go in that direction.
   Purely a minimization algorithm - easy to extend to nonlinear.

   Why do we like this method? Easy intuition, prompt discussion.

*** Analysis of Steepest Descent

    Kantorovich inequality - if A is SPD, then

    1 .LEQ. norm(x,A)^2 * norm(x,A^-1) / (x,x)^2
      .LEQ. (lambda_1 + lambda_n)^2 / (4 lambda_1 lambda_n)

    where lambda_n is the greatest eigenvalue and lambda_1 is the least
    eigenvalue.

**** Proof

     WLOG assume norm(x,2) = 1. Then

     A is SPD, so we have some Q s.t. A = Q^T D Q, A^-1 = Q^T D^-1 Q
     where D is a diagonal matrix of eigenvalues.

     then, for y = Qx, norm(x,A) = norm(y,D) so

     norm(x,A)^2 norm(x,A^-1)^2 = norm(y,D)^2 norm(y,D^-1)^2

     and, as Q is unitary, norm(y,2) = 1.

     TO show that norm(y,D)^2 norm(y,D^-1)^2 .GEQ. 1, we need

     y_D = sqrt(diag(D)) .* y
     y_D^-1 = transpose((1 ./ sqrt(diag(D))) .* y)

     This part needs four small lemmas. Check slide 160.


* 10/14/11
** More on the steepest descent method
   We have still assumed that everything is exact. We have also assumed that we
   have rather large matricies.

** Conjugate Gradient Method
   in Steepest Descent - used each new step is in the direction of the
   gradient. Follow the direction at each point (we have a specific formula to
   compute this)

   The search direction from SD made sense from Sophomore Calculus, but not so
   much for matricies. Locally, each choice is the best, but globally it is not
   that great.

   Now we search in the direction of the Conjugate Gradient instead (hence the
   name)

**** Conjugate Gradient
     If A is symmetric, we say that x and y are conjugate or A-orthogonal if

     (x, Ay) = x^T A y = 0

*** Algorithm Motivation

    Q(t) = q(x0 + t * p0)
         = 1/2 * (Ax0, x0) + t (Ax0, p0) + 1/2 t^2 (Ap0, p0)
         - 1/2 (b,x0) - t(b,p0)

    CG - use the previous information to generate a better vector.

    So, we find the direction at x1 conjugate to p0 :
    p1 = -r1 + mu1 * p0 s.t. (p0, Ap1) = 0.

    Then (p0, -Ar1 + mu A p0) = 0 -> mu1 = (r1, Ap0) / (p0, Ap0).

*** Algorithm

    r0 = Ax0 - b, p0 = -r0.

    for k in [0..M-1] :
        t_k = -(rk, pk) / (Apk, pk) and x_k+1 = x_k + t_k * p_k

        r_k+1 = r_k + t_k * A * p_k

        mu_k = (r_k+1, Ap_k) / (p_k, Ap_k)

        p_k+1 = -r_k+1 + mu_k * p_k

    endfor

    Uses two scalars and three vectors - more expensive than SD.

*** Properties

    Lots of properties. Check the book for lots of good information.

    -(r1, r0) = (r1, p0) = (r0 + t0 * A * p0, p0)
                         = (r0, p0) + t0 * (Ap0, p0)
                         = mess
                         = 0 woo

    therefore the remainders are orthogonal to each-other. Similarly, r_k and
    p_k-1 are also orthogonal.

**** Theorem : All the rks and pks are orthogonal
     for i /= j, (r_i, p_j) = 0 and (p_i, Ap_i) = 0. Done by induction from
     results above.

***** Important Corollary
      (p_k, r_i) = -(r_k, r_k) (simplification for calculating t_k and mu_k)


* 10/17/11
** More CG
   L_k  = span of p0, p1, p2, ...
   Pi_k = {x s.t. x = x0 + z, z in L_k} (shifted space)

*** Lemma
    The sequence x0, x1, ... xk is such that

    q(xk) = min q(x0 + z)

    iff xk in Pi_k, rk perpendicular to L_k where rk = Axk - b.

**** Proof

     Condition necessary - Review slides 176-177.

*** Theorem 11
    Asume that x0, x1, ... is the sequence generated by CG. Then

    q(xk) = min(q(x0 + z)) forall k .GEQ. 1.

**** Proof
     From the CG procedure we have that

     xk = xk-1 + tk-1pk-1
        = xk-2 + tk-2 * p_k-2 + tk-1 * p_k-1
        etc...

     therefore xk is in the span of the pks.

     Furthermore, we need that rk is perpendicular to L_k.

     for k = 1, this is valid as (r1,p0) = 0.
     Now assume that rj is perpendicular to Lj. then by induction (for k+1)

     r^k+1 = rk + t_k * A * p_k -> (rk+1, p_k) = 0.

** Krylov Subspaces
   Given some matrix A and a vector v, the mth Krylov subspace is

   {v, Av, ... A^m-1 v}

*** Theorem (Krylov from CG)

    if r^(m-1) /= 0 then we have

    Lm = span(r0,r1, ... , r^m-1) = Km (A,r0) for some m .LEQ. n.

    (so our L_k is the Krylov subspace)

**** Proof
     m = 1, holds as p0 = -r0.

     Assume that it is true for m = k. THen for m = k+1, we have

     r_k-1, p_k-1 in K_k so A * p_k-1 in K_k+1

     therefore r_k = r_k-1 + t_k A * p_k-1 in K_k+1

     so the span{r0, ... rk} subset K_k+1

     we know that the rs are linearly independent, so the dimension of that is
     k+1, or

     k+1 .LEQ. dim(K_k+1) .LEQ. k+1

     so the span of the rs = K_k+1 .

*** TODO Homework - show that the ps are linearly independent.


* 10/19/11

** Krylov Methods

   Getting more popular! CG is actually getting less popular.

   Krylov is a goldmine. So is Chebyshev!

** Chebyshev Polynomial

*** Recursive Definition

    T0(x) = 1, T1(x) = x
    T_n+1(x) = 2x * T_n(x) - T_n-1(x)

    More sophisticated - do not need recursion.

    Can show T_n(x) = 1/2 * ((x + sqrt(x^2 - 1))^n + (x - sqrt(x^2 - 1))^n)

*** Lemma 5

    Suppose that p(x) is a polynomial of degree k s.t.

    p(0) = 1, abs(p(x)) .LEQ. r

    then for any x0, the sequence {xk} generated by the CG method satisfies

    norm(ek, A) .LEQ. r * norm(e0,A)

    That is, error is bounded by initial error. We will show that r -> 0 later.

**** Proof

     (q(x) -- minimize energy.)
     q(x) = 1/2 (x, Ax) - (x, b) = 1/2( (x,Ax) - 2*(x,b))

     by previous work, we can work this down to

     q(xk) = min (x in x0 + K_k(A,r0) ) q(x)

     that is, minimizing over some x in the kth Krylov space.

     This gets a bit messy, but uses the Chebyshev polynomial's properties in
     order to get that the CG method error is bounded by the square root of the
     condition number.

     Next time - preconditioning.

* 10/21/11

** Review - CG versus SD

   CG performance dictated by A - number of steps bounded by condition number.

** Preconditioning and CG

   Given some Ax = b with large K(A), we want some preconditioner Q = E * E^T

   where R = A - Q is small.

*** Converting to a problem we would like to solve

    Ax = b, but we would really like to say x = A^-1 b.
    A(E^-T E^T ) x = b

    so E^-1 A (E^-T) (E^T x) = E^-1 b.

    We hope that K(E^-1 A E^-T) << K(A).

    This won't work for sparse matricies! It converts them back to dense.

    Algorithm for sparse-preserving CG given in slides -- approximately slide
    192

    This algorithm really just needs an input of some preconditioning matrix
    Q - therefore we can write a general solver with Q as an input!

*** Two examples of preconditioners

**** sigma > 0

     then calculate some E, where

     E = D^(1/2) + sigma * C_L D^(-1/2)

     then Q = EE^T

**** Incomplete cholesky factorization

     A = Q + R

     want to use some l_ij where if a_ii = 0 then set l_ij = 0. This preserves
     the sparsity and is almost A.
