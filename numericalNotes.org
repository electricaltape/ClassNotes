* General Tips
    Never compute the inverse!
    Use properties of special matricies.
    Proofs on homework - don't be afraid to go back.


* 9.9.11
** Pivoting
s_i = max over 1 \leq j \leq n \abs{a_{ij}}
scale of the ith row - use for another pivoting policy

** LU by GESRP

Set A1 = A
    scan the first column of A1 : set a_{r1,1} / s_r1 := (max A[:,0])
    exchange that row r1 and row 1 of A1 to form A~1, or
    A~1 = I_r1,1 A1 .

for true numerical analysis, need to know/do:
    understand troublesome parts
    understand algorithm by hand
    use examples to check code.
    we like test-first coding practices - each part valid.

for A = 0 1 1
        1 2 3
        1 0 2

s1 = 1, s2 = 3, s3 = 2 (largest element in each row)
max [a_31 / s_3 , a_11 / s_1, a_21 / s2] = a_31/ s_3 so use r_1 = 3.

Therefore R1 -> R3 so carry out elimination.

Most stable is full pivoting, but that is generally difficult. generally
partial pivoting is as good (a belief, not a truth)

** Applications of LU factorization
    solve Ax = b - O(n^3/3) long operations (optional hw - find exact number)
    (should be less, as there is less division)
    Much worse strategy - compute determinants and solve by
        Cramer's rule - (n + 1)(n - 1)n! operations (far worse.)
    quickly compute the determinant of a matrix
        works because det(AB) = det(A)det(B)
        and the determinant of a triangular matrix is the trace
        so det(P)det(A) = det(L)det(U), det(L) = 1, det(U) = trace(U)
    find rank(A)
    solve Ax = b1, Ax = b2, Ax = b3, ... good for gaming calculations
        (moving polygons/vectors)
        compute inverse(A) - requires O(4n^3/3) long operations
        repeatedly solve for [1 0 0], [0 1 0], [0 0 1] etc to form columns

** solving LU factorization systems
    Ax    = b
    PAx   = Pb
    (LU)x = Pb
    so solve Ly = Pb -> Ux = y

    Algorithms are already coded and available. Nothing particularly new.
    matlab : [L,U,P] = lu(A)
             y = L \ (P * b)
             x = U \ y
    Google netlib


* 9.12.11
** LU factorization of RDD matricies
   Pay attention! important for scientific computing.
   diagonally dominant matrix
       - magnitude of diagonal entry \leq sum of magnitudes of other entries
       - strictly : use > not \leq
   Theorem 3
   - If GE is applied to fint the LU factorization of a strictly RDD matrix A,
     A^(k) = (A_(ij)^(k)) k in NN in the elimination procedure are strictly RDD
   - every RDD matrix has an LU factorization.
   - the LU based on GESRP for a RDD matrix is the same as the LU based on GE.

   kinda nice - for RDD we do not need pivoting. The greatest entry is already
   on the diagonal! We can handle LU faster, more accurately.

   Proof of Theorem 3
   Let A = (a_ij) nxn be a strictly RDD matrix. Prove R^(2) is strictly RDD and
   similar arguments can be shown for A^(3), A^(4), etc.

   so show that | a_ii^(2) | > sum off-diagonals. (slide 59)

   follow the algorithm for the second row and use the property of diagonal
   dominance from the first row.
** LU factorization of SPD matricies
   Lots of physical problems (like heat flow) are symmetrical.

   SPD <-> following properties:
   - A = transpose(A) (symmetrical)
   - (transpose x) A x > 0 for x /= 0 (positive definite, by test vector)

   SPD matricies are nice! Check out some properties:
   - A nonsingular, (inverse A) SPD
   - leading principle submatricies are SPD
   - principal minors of A are positive.
   - eigenvalues are positive.

   Lemma 1 - go backwards.
   - if all eigenvalues of A are positive then A is SPD
   - if all principal minors are positive then A is SPD

   SPD is symmetric - we should only need to look at half of it.

   Theorem 3.8 - (SPD? A) <-> exists L s.t. A = L(transpose L)

   (LLT is cholesky factorization!)
   Proof. assume A = L(transpose L), so A symmetric. (show A positive definite)
   let y = (transpose L)x so (nonzero x -> nonzero y). Then

   (transpose x) A x = (transpose x) L (transpose L) x
                     = (transpose y) y
                     > 0, for x /= 0.
   so A is SPD.

   Now assume A is SPD so A = LU. L is unit lower triangular. Note

   P_k(A) = P_k(L) P_k(U) (slide 64)




* 9.14.11
** Theorem 3.8
   Let A be symmetric, nxn. A is SPD iff A + L(transpose L)
   Better explanation in slides. Shows that if we have a symmetric matrix we
   don't need to do LU!
** Cholesky Factorization
   Given some SPD matrix we can compute L with the following algorithm:
   do k = 1,n
       l_kk = sqrt ( a_kk - sum (l_{ks})^2, for s = 1 to k-1)
       do i = k+1,n
           l_{ik} = 1/l_{kk} (a_{ik} - sum l_{is} l_{ks} for s = 1 to k-1)
       end do
   end do

   operation count is homework! show that it is n^3/6 + O(n^2).

   This algorithm is nice and stable - no need to pivoting. It also is faster
   than LU factorization (n^3/6 instead of n^3/3 multiplications)
** Sparse Matricies
   First challenge - storage space. Not as big a problem as it once was, but
   still nontrivial.

   More examples of sparse matricies - telephone grids. Represent with a matrix
   of ones and zeros - if two are connected, write 1. Otherwise write 0.

   Some references : Saad (sparse book), Meschach (noncommercial sparse
   package).

   We like sparse matricies - can skip operations on zero entries, save a lot
   of time.

*** Coordinate Format
    Goal - don't store zeros.
    Accordian format.

    one array - store nonzero entries in any order.
    two other arrays - store row and column indicies.

    This requires 3Nz entries, where Nz is the number of nonzero entries.

*** Compressed Sparse Row format
    Real array to store entries row by row
    Integer array J to store column indicies
    Integer array of pointers to where each row starts.

*** Tridiagonal matricies
    A square matrix A = (a_ij) is said to be tri-diagonal if a_ij = 0 for all
    i, j such that | i - j | > 1.

    Nice properties - principle minors are nonsingular if:
    |a_1| > |c_1|
    |a_k| \geq |b_k| + |c_k|
    |a_n| > |b_n|

    Proof by induction: show that the first 2x2 matrix is fine, assume nxn,
    show (n+1)x(n+1).

    How can we compute something that is tridiagonal? Factorize and solve.

**** Trick - Crout factorization = Thomas algorithm

     Crout requires about 2n operations for solution.

     Nice feature: superdiagonal entries of U are always less than 1.
     Nice feature: subdiagonal entries of L are bounded.

     Turns out Crout is very stable - we like it.

     Like 'undetermined matrix'
     Propose that we factor some tridiagonal matrix by
     A = (alphas on diagonal, bs on subdiagonal) times
         (ones on diagonal, gammas on superdiagonal)
     What happens when we multiply them?
     get a tridiagonal back.
     so row 1 = (alpha1, alpha1*gamma1, 0, 0, ...)
     so row 2 = (b2 , b2* gamma1 + alpha2, alpha2 * gamma2,    0, 0, ...)
     so row 3 = (0  , b3,                  b3*gamma2 + alpha3, alpha3*gamma3,
     0, 0, ...)

     set this equal to the LHS, so alpha1 = a1, alpha1*gamma1 = c1, etc

     follow in repetition, for k = 2 .. n - 1
     alphak = ak - bk*gamma(k-1)
     gammak = ck / alphak

     operation counts: 2n - 2 (nice!)


* 9.16.11
** Announcement
   Test in a week! May be take-home.
   Do exercises, read books, do research: we have to take tests too :(

** Features of Crout : Theorem 4
   For meeting the conditions (see slide 73/195):
   (diagonally dominant, last and first rows diagonally dominant)
   see slide 77/195

** Normed Linear Spaces
   We have to talk about error - that means norms.
   our solutions are vectors so we need norms - how do we compare?
   Another problem - how do we call them 'large' or 'small'?

   RR^n - set of length n arrays. What operation? Norm! Inner Product! Vector
   addition!

   Matricies - normally boring containers. Not interesting. We need operations
   to make them worthwhile. Matricies have structure and addition.

   On RR^2 : f(x1, x2) = e^x1 + e^x2 - not a norm on RR^2 as f(0,0) /= 0
   norms are just special functions of RR^n.

   We can also create a distance function : d(x,y) = || x - y ||
   (a metric, relates vectors)
   we want d(x,y) to be small to say that x ~ y : one number to gauge accuracy.

*** Some specific norms
    L_p norm : Holer's norm  on CC^n : for p geq 1, L_p norm is
    norm : ||x||_p = ( Sum |x_i|^p)^(1/p)

    so p is arbitrary, p can be a real. We use p=1, p=2, p=inf most often.




* 9.19.11
** Test
   Allowed to bring 1-page of information (open sheet, 8.5 x 11)
   In-class exam
   will have LU factorization (at most, 3x3; no huge problems)
   able to do 3x3 LU factorization by hand.

   A lot of error estimate style things, like chapter 1.

   Bring a calculator. Should not need it, but it is nice.
   Test covers all material up to Wednesday. Review slides!

** More on normed linear spaces
   Use the entries of two vectors to form a single number:
   Cauchy-Schwarz : sum of x_i bar(y_i) leq 2-norm(x) * 2-norm(y)
   This is similar to the triangle inequality:
   norm(x + y) leq norm(x) + norm(y)

** Vectors and matricies
   matricies tend to be functions and vectors tend to be 'variables'. We should
   study matricies too!
   Say that the norm of a matrix, norm(A) geq 0. norm(A) = 0 iff A = 0. All the
   usual properties of norms hold.

*** Compatible Norms
   We say that a matrix norm and a vector norm are compatible if
   norm(Ax) leq norm(A)norm(x), forall x, forall A.

**** Frobenius Norm
     not Natural/Induced/Subordinate : square root of the sum of squares
**** P norm
     equivalent of p norm for vectors.

*** Natural/Induced/Subordinate matrix norms
    for some vector norm norm(), we say it is Natural/Induced/Subordinate if

    norm(A) = sup(x /= 0) norm(Ax)/norm(A)

    not a nice definition - needs supremum. We can find a beter finite formula.

*** Proposition 3.4 - norm(A, inf)
    compute it by the max absolute row sum.
    so: find absolute values of all entries, sum each row, take the max rowsum

    Similarly: norm(A, 1) is the maximum absolute column sum.

**** TODO prove rest as informal HW
     for norm(A, inf) we have

     norm(Ax) = max of the absolute sums of of rows by definition. Rearrange
     with leq. See slide 87.

*** Spectral Radius
    Assume a matrix has eigenvalues lambda_i. Call

    rho(A) = max |(lambda_i)|

    and call it the spectral radius of A.

    why radius? Create a circle with radius max | lambda_i |. This will inclose
    all eigenvalues in the complex plane.

**** Theorem : norm(A,2) = sqrt (spectralRadius((conjugate-transpose A) * A))
     Proof : Note that (conjugate-transpose A) * A is symmetric and has n
     orthonormal independent eigenvectors. v_i, and

     lambda_i = (Av_i, Av_i) geq 0.



* 9.21.11
** Spectral Radius and Norms
   For any matrix norm we have that rho(A) leq norm(A)
   (spectral radius is always leq norm)

*** Proof
    Assume norm is induced and lambda is an eigenvalue. Then exists x /= 0
    s.t. Ax = lambda x.

    Then |lambda| || x || = ||lambda x|| = ||Ax|| leq || A || || x ||

    which implies that |lambda| leq || A ||

    note that the spectral radius is the largest eigenvalue so we are done.

*** Can we bound the matrix norm by the spectral radius? No.
    A = [0 2; 0 0] so || A || = 2, but rho(A) = 0.

*** Schur Decomposition - necessary details

    forall A, exists nonsingular matrix P and an upper triangular matrix T s.t.

    PAP^-1 = T

    additionally, given nonsingular B, ||x|| = ||Bx||2 is a norm.
    (useful only theoretically)

*** Proposition 3.6 - from above facts. (implicit homework)
    Given an nxn matrix A and epsilon > 0 :

    exists norm s.t. || A || leq rho(A) + epsilon

**** Proof
     Let P, T be the matricies of the Schur decomposition of A and let

     T = Lambda + U
     Lambda = diagonal matrix of eigenvalues of A
     U(i,j) = 0 for i geq j.

     Then for any delta > 0, can form

     D^-1 = diagonal matrix of 1, delta, delta^2, ...
     C = DTD^-1 = Lambda + E
     E = DUD^-1.

     Because DP is nonsingular, we introduce:

     vector norm || x || = || DPx ||2 = (x^H P^H D^H DPx)^(1/2)

     and further

     an induced matrix norm || A || = supremum over ||y|| = 1 ||Ay||.

     Let z = DPy. If ||y|| = 1 then ||z||2 = ||DPy||2 = ||y|| = 1.

     Note that A = P^-1 T P = P^-1 D^-1 CDP, DPAP^-1D^-1 = C.

     So, again for ||y|| = 1, we get

     that ||Ay||^2 = = ||DPAy||2^2 = || mess ||2^2 = ||Cz||2^2 = gets
     worse. See slide 93.

** Matrix Norms and Sequences
*** Proposition 3.7 - equivalency of matrix norms
    We can bound any two norms with constants c1 and c2 by something like

    c1 || A ||_alpha leq || A ||_beta \leq c2 || A ||

*** Definition 3.27 - Sequence of matricies
    sequence of matricies {A_k} converges to A iff

                    lim (k -> inf)|| A_k - A|| = 0.

    (this will be important in the second half of this chapter, iterative
    methods)

*** The following are Equivalent

    1. lim (k -> inf) A^k = 0.
    2. lim (k -> inf) A^k x = 0 forall x in CC^n.
    3. rho(A) < 1.
    4. exists norm s.t. norm(A) < 1.

    there are more that can go here.

**** Proof
     (1 -> 2) norm(A^k x) leq norm(A^k) norm(x)
     therefore
     0 leq lim (k -> inf) norm(A^k x) leq lim (k -> inf) norm(A^k) norm(x) = 0

     Therefore it is squeezed between 0 and 0; goes to 0.

     (2 -> 3) let lambda be an eigen value, so |lambda|^k must go to zero ->
     lambda < 1.

*** Gathering up the eigenvalues

    Call sigma(A) the set of all eigenvalues of A.

    If P(x) is a polynomial then we get something like

    sigma(P(A)) = {P(lambda), lambda in sigma(A)}

    we can even extend this beyond polynomials to analytical functions (exp(A),
    sin(A) all defined... interestingly)

    Even more interesting: sigma(A^-1) = P(1/x) evaluated at lambdas. We know
    the eigenvalues with out knowing the matrix! Wow. Magical!
**** Example

     P(x) = 1/sqrt(3) x^2 + pi x + \sqrt(2) I

     Hard to evaluate. However, easy to find eigenvalues.

** Geometric series

   I + A + A^2 + A^3 + ... converges iff lim (k -> inf) A^k = 0

   Also: if the limit exists then I - A is nonsingular and

           Sum (from k=0) (to infinity) A^k = (I - A)^(-1)

**** Proof
     Assume that lim (k -> inf) A^k = 0. Then

     rho(A) < 1 by Theorem 3.5 .
     then sigma(I - A) = stuff. See slide 97.


