* General Tips
    + Never compute the inverse!
    + Use properties of special matricies.
    + Proofs on homework - don't be afraid to go back.
    + modern mathematics is about structure! We do not care about sets anymore;
    anything worthwhile has structure behind it, certain properties, etc
    + In modern math, the disjoint-ness from earlier decades is becoming
    blurred. People work more on modeling than just 'Ax = b'. Once we model it,
    how do we solve it?
    + More data, more trouble.
    + Goal of the class is to learn general classes of methods, rather than
    particular techniques.
    + Cameron Says : we take as much of the definition of the derivative that
    we need (Frechet, Gateaux, etc) so we are unburdened by the remaining
    constraints
    + Chinese Philosophy and Numerical Methods : If we have a linear problem,
    we should consider nonlinear techniques (e.g. Conjugate Gradient). If we
    have a nonlinear problem we should consider linear techniques
    (e.g. Newton's Method)
* 9/9/11
** Pivoting
s_i = max over 1 \leq j \leq n \abs{a_{ij}}
scale of the ith row - use for another pivoting policy
** LU by GESRP
   Set A1 = A
   scan the first column of A1 : set a_{r1,1} / s_r1 := (max A[:,0])
   exchange that row r1 and row 1 of A1 to form A~1, or
   A~1 = I_r1,1 A1 .
   for true numerical analysis, need to know/do:
   understand troublesome parts
   understand algorithm by hand
   use examples to check code.
   we like test-first coding practices - each part valid.
   for
       A = 0 1 1
           1 2 3
           1 0 2

   s1 = 1, s2 = 3, s3 = 2 (largest element in each row)
   max [a_31 / s_3 , a_11 / s_1, a_21 / s2] = a_31/ s_3 so use r_1 = 3.

   Therefore R1 -> R3 so carry out elimination.

   Most stable is full pivoting, but that is generally difficult. generally
   partial pivoting is as good (a belief, not a truth)
** Applications of LU factorization
   solve Ax = b - O(n^3/3) long operations (optional hw - find exact number)
   (should be less, as there is less division)
   Much worse strategy - compute determinants and solve by
   Cramer's rule - (n + 1)(n - 1)n! operations (far worse.)
   quickly compute the determinant of a matrix
   works because det(AB) = det(A)det(B)
   and the determinant of a triangular matrix is the trace
   so det(P)det(A) = det(L)det(U), det(L) = 1, det(U) = trace(U)
   find rank(A)
   solve Ax = b1, Ax = b2, Ax = b3, ... good for gaming calculations
   (moving polygons/vectors)
   compute inverse(A) - requires O(4n^3/3) long operations
   repeatedly solve for [1 0 0], [0 1 0], [0 0 1] etc to form columns
** solving LU factorization systems
    Ax    = b
    PAx   = Pb
    (LU)x = Pb
    so solve Ly = Pb -> Ux = y

    Algorithms are already coded and available. Nothing particularly new.
    matlab : [L,U,P] = lu(A)
             y = L \ (P * b)
             x = U \ y
    Google netlib
* 9/12/11
** LU factorization of RDD matricies
   Pay attention! important for scientific computing.
   diagonally dominant matrix
       - magnitude of diagonal entry \leq sum of magnitudes of other entries
       - strictly : use > not \leq
   Theorem 3
   - If GE is applied to fint the LU factorization of a strictly RDD matrix A,
     A^(k) = (A_(ij)^(k)) k in NN in the elimination procedure are strictly RDD
   - every RDD matrix has an LU factorization.
   - the LU based on GESRP for a RDD matrix is the same as the LU based on GE.

   kinda nice - for RDD we do not need pivoting. The greatest entry is already
   on the diagonal! We can handle LU faster, more accurately.

   Proof of Theorem 3
   Let A = (a_ij) nxn be a strictly RDD matrix. Prove R^(2) is strictly RDD and
   similar arguments can be shown for A^(3), A^(4), etc.

   so show that | a_ii^(2) | > sum off-diagonals. (slide 59)

   follow the algorithm for the second row and use the property of diagonal
   dominance from the first row.
** LU factorization of SPD matricies
   Lots of physical problems (like heat flow) are symmetrical.

   SPD <-> following properties:
   - A = transpose(A) (symmetrical)
   - (transpose x) A x > 0 for x /= 0 (positive definite, by test vector)

   SPD matricies are nice! Check out some properties:
   - A nonsingular, (inverse A) SPD
   - leading principle submatricies are SPD
   - principal minors of A are positive.
   - eigenvalues are positive.

   Lemma 1 - go backwards.
   - if all eigenvalues of A are positive then A is SPD
   - if all principal minors are positive then A is SPD

   SPD is symmetric - we should only need to look at half of it.

   Theorem 3.8 - (SPD? A) <-> exists L s.t. A = L(transpose L)

   (LLT is cholesky factorization!)
   Proof. assume A = L(transpose L), so A symmetric. (show A positive definite)
   let y = (transpose L)x so (nonzero x -> nonzero y). Then

   (transpose x) A x = (transpose x) L (transpose L) x
                     = (transpose y) y
                     > 0, for x /= 0.
   so A is SPD.

   Now assume A is SPD so A = LU. L is unit lower triangular. Note

   P_k(A) = P_k(L) P_k(U) (slide 64)



* 9/14/11
** Theorem 3.8
   Let A be symmetric, nxn. A is SPD iff A + L(transpose L)
   Better explanation in slides. Shows that if we have a symmetric matrix we
   don't need to do LU!
** Cholesky Factorization
   Given some SPD matrix we can compute L with the following algorithm:
   do k = 1,n
       l_kk = sqrt ( a_kk - sum (l_{ks})^2, for s = 1 to k-1)
       do i = k+1,n
           l_{ik} = 1/l_{kk} (a_{ik} - sum l_{is} l_{ks} for s = 1 to k-1)
       end do
   end do

   operation count is homework! show that it is n^3/6 + O(n^2).

   This algorithm is nice and stable - no need to pivoting. It also is faster
   than LU factorization (n^3/6 instead of n^3/3 multiplications)
** Sparse Matricies
   First challenge - storage space. Not as big a problem as it once was, but
   still nontrivial.

   More examples of sparse matricies - telephone grids. Represent with a matrix
   of ones and zeros - if two are connected, write 1. Otherwise write 0.

   Some references : Saad (sparse book), Meschach (noncommercial sparse
   package).

   We like sparse matricies - can skip operations on zero entries, save a lot
   of time.

*** Coordinate Format
    Goal - don't store zeros.
    Accordian format.

    one array - store nonzero entries in any order.
    two other arrays - store row and column indicies.

    This requires 3Nz entries, where Nz is the number of nonzero entries.

*** Compressed Sparse Row format
    Real array to store entries row by row
    Integer array J to store column indicies
    Integer array of pointers to where each row starts.

*** Tridiagonal matricies
    A square matrix A = (a_ij) is said to be tri-diagonal if a_ij = 0 for all
    i, j such that | i - j | > 1.

    Nice properties - principle minors are nonsingular if:
    |a_1| > |c_1|
    |a_k| \geq |b_k| + |c_k|
    |a_n| > |b_n|

    Proof by induction: show that the first 2x2 matrix is fine, assume nxn,
    show (n+1)x(n+1).

    How can we compute something that is tridiagonal? Factorize and solve.

**** Trick - Crout factorization = Thomas algorithm

     Crout requires about 2n operations for solution.

     Nice feature: superdiagonal entries of U are always less than 1.
     Nice feature: subdiagonal entries of L are bounded.

     Turns out Crout is very stable - we like it.

     Like 'undetermined matrix'
     Propose that we factor some tridiagonal matrix by
     A = (alphas on diagonal, bs on subdiagonal) times
         (ones on diagonal, gammas on superdiagonal)
     What happens when we multiply them?
     get a tridiagonal back.
     so row 1 = (alpha1, alpha1*gamma1, 0, 0, ...)
     so row 2 = (b2 , b2* gamma1 + alpha2, alpha2 * gamma2,    0, 0, ...)
     so row 3 = (0  , b3,                  b3*gamma2 + alpha3, alpha3*gamma3,
     0, 0, ...)

     set this equal to the LHS, so alpha1 = a1, alpha1*gamma1 = c1, etc

     follow in repetition, for k = 2 .. n - 1
     alphak = ak - bk*gamma(k-1)
     gammak = ck / alphak

     operation counts: 2n - 2 (nice!)

* 9/16/11
** Announcement
   Test in a week! May be take-home.
   Do exercises, read books, do research: we have to take tests too :(

** Features of Crout : Theorem 4
   For meeting the conditions (see slide 73/195):
   (diagonally dominant, last and first rows diagonally dominant)
   see slide 77/195

** Normed Linear Spaces
   We have to talk about error - that means norms.
   our solutions are vectors so we need norms - how do we compare?
   Another problem - how do we call them 'large' or 'small'?

   RR^n - set of length n arrays. What operation? Norm! Inner Product! Vector
   addition!

   Matricies - normally boring containers. Not interesting. We need operations
   to make them worthwhile. Matricies have structure and addition.

   On RR^2 : f(x1, x2) = e^x1 + e^x2 - not a norm on RR^2 as f(0,0) /= 0
   norms are just special functions of RR^n.

   We can also create a distance function : d(x,y) = || x - y ||
   (a metric, relates vectors)
   we want d(x,y) to be small to say that x ~ y : one number to gauge accuracy.

*** Some specific norms
    L_p norm : Holer's norm  on CC^n : for p geq 1, L_p norm is
    norm : ||x||_p = ( Sum |x_i|^p)^(1/p)

    so p is arbitrary, p can be a real. We use p=1, p=2, p=inf most often.



* 9/19/11
** Test
   Allowed to bring 1-page of information (open sheet, 8.5 x 11)
   In-class exam
   will have LU factorization (at most, 3x3; no huge problems)
   able to do 3x3 LU factorization by hand.

   A lot of error estimate style things, like chapter 1.

   Bring a calculator. Should not need it, but it is nice.
   Test covers all material up to Wednesday. Review slides!

** More on normed linear spaces
   Use the entries of two vectors to form a single number:
   Cauchy-Schwarz : sum of x_i bar(y_i) leq 2-norm(x) * 2-norm(y)
   This is similar to the triangle inequality:
   norm(x + y) leq norm(x) + norm(y)

** Vectors and matricies
   matricies tend to be functions and vectors tend to be 'variables'. We should
   study matricies too!
   Say that the norm of a matrix, norm(A) geq 0. norm(A) = 0 iff A = 0. All the
   usual properties of norms hold.

*** Compatible Norms
   We say that a matrix norm and a vector norm are compatible if
   norm(Ax) leq norm(A)norm(x), forall x, forall A.

**** Frobenius Norm
     not Natural/Induced/Subordinate : square root of the sum of squares
**** P norm
     equivalent of p norm for vectors.

*** Natural/Induced/Subordinate matrix norms
    for some vector norm norm(), we say it is Natural/Induced/Subordinate if

    norm(A) = sup(x /= 0) norm(Ax)/norm(A)

    not a nice definition - needs supremum. We can find a beter finite formula.

*** Proposition 3.4 - norm(A, inf)
    compute it by the max absolute row sum.
    so: find absolute values of all entries, sum each row, take the max rowsum

    Similarly: norm(A, 1) is the maximum absolute column sum.

**** TODO prove rest as informal HW
     for norm(A, inf) we have

     norm(Ax) = max of the absolute sums of of rows by definition. Rearrange
     with leq. See slide 87.

*** Spectral Radius
    Assume a matrix has eigenvalues lambda_i. Call

    rho(A) = max |(lambda_i)|

    and call it the spectral radius of A.

    why radius? Create a circle with radius max | lambda_i |. This will inclose
    all eigenvalues in the complex plane.

**** Theorem : norm(A,2) = sqrt (spectralRadius((conjugate-transpose A) * A))
     Proof : Note that (conjugate-transpose A) * A is symmetric and has n
     orthonormal independent eigenvectors. v_i, and

     lambda_i = (Av_i, Av_i) geq 0.


* 9/21/11
** Spectral Radius and Norms
   For any matrix norm we have that rho(A) leq norm(A)
   (spectral radius is always leq norm)
*** Proof
    Assume norm is induced and lambda is an eigenvalue. Then exists x /= 0
    s.t. Ax = lambda x.

    Then |lambda| || x || = ||lambda x|| = ||Ax|| leq || A || || x ||

    which implies that |lambda| leq || A ||

    note that the spectral radius is the largest eigenvalue so we are done.
*** Can we bound the matrix norm by the spectral radius? No.
    A = [0 2; 0 0] so || A || = 2, but rho(A) = 0.
*** Schur Decomposition - necessary details
    forall A, exists nonsingular matrix P and an upper triangular matrix T s.t.

    PAP^-1 = T

    additionally, given nonsingular B, ||x|| = ||Bx||2 is a norm.
    (useful only theoretically)
*** Proposition 3.6 - from above facts. (implicit homework)
    Given an nxn matrix A and epsilon > 0 :

    exists norm s.t. || A || leq rho(A) + epsilon
**** Proof
     Let P, T be the matricies of the Schur decomposition of A and let

     T = Lambda + U
     Lambda = diagonal matrix of eigenvalues of A
     U(i,j) = 0 for i geq j.

     Then for any delta > 0, can form

     D^-1 = diagonal matrix of 1, delta, delta^2, ...
     C = DTD^-1 = Lambda + E
     E = DUD^-1.

     Because DP is nonsingular, we introduce:

     vector norm || x || = || DPx ||2 = (x^H P^H D^H DPx)^(1/2)

     and further

     an induced matrix norm || A || = supremum over ||y|| = 1 ||Ay||.

     Let z = DPy. If ||y|| = 1 then ||z||2 = ||DPy||2 = ||y|| = 1.

     Note that A = P^-1 T P = P^-1 D^-1 CDP, DPAP^-1D^-1 = C.

     So, again for ||y|| = 1, we get

     that ||Ay||^2 = = ||DPAy||2^2 = || mess ||2^2 = ||Cz||2^2 = gets
     worse. See slide 93.
** Matrix Norms and Sequences
*** Proposition 3.7 - equivalency of matrix norms
    We can bound any two norms with constants c1 and c2 by something like

    c1 || A ||_alpha leq || A ||_beta \leq c2 || A ||

*** Definition 3.27 - Sequence of matricies
    sequence of matricies {A_k} converges to A iff

                    lim (k -> inf)|| A_k - A|| = 0.

    (this will be important in the second half of this chapter, iterative
    methods)

*** The following are Equivalent

    1. lim (k -> inf) A^k = 0.
    2. lim (k -> inf) A^k x = 0 forall x in CC^n.
    3. rho(A) < 1.
    4. exists norm s.t. norm(A) < 1.

    there are more that can go here.

**** Proof
     (1 -> 2) norm(A^k x) leq norm(A^k) norm(x)
     therefore
     0 leq lim (k -> inf) norm(A^k x) leq lim (k -> inf) norm(A^k) norm(x) = 0

     Therefore it is squeezed between 0 and 0; goes to 0.

     (2 -> 3) let lambda be an eigen value, so |lambda|^k must go to zero ->
     lambda < 1.

*** Gathering up the eigenvalues

    Call sigma(A) the set of all eigenvalues of A.

    If P(x) is a polynomial then we get something like

    sigma(P(A)) = {P(lambda), lambda in sigma(A)}

    we can even extend this beyond polynomials to analytical functions (exp(A),
    sin(A) all defined... interestingly)

    Even more interesting: sigma(A^-1) = P(1/x) evaluated at lambdas. We know
    the eigenvalues with out knowing the matrix! Wow. Magical!
**** Example

     P(x) = 1/sqrt(3) x^2 + pi x + \sqrt(2) I

     Hard to evaluate. However, easy to find eigenvalues.

** Geometric series

   I + A + A^2 + A^3 + ... converges iff lim (k -> inf) A^k = 0

   Also: if the limit exists then I - A is nonsingular and

           Sum (from k=0) (to infinity) A^k = (I - A)^(-1)

**** Proof
     Assume that lim (k -> inf) A^k = 0. Then

     rho(A) < 1 by Theorem 3.5 .
     then sigma(I - A) = stuff. See slide 97.

* Numerical Test
** Backward error
   Wikipedia : delta x s.t. f(x + delta x) = y* (what problem the algorithm
   actually solved)

   Sauer : Say that f(r) = 0 (a root), xc ~ r. backward error is f(xc). This is
   the amount we need to change the input by to get the approximation out.
*** Examples
**** Matrix
     Say we solve a matrix problem Ax = b and get result x*. Then the backward
     error is delta b in:

     Ax* = b + delta b, find delta b.

     Better example, from Purple Book : for backward error, show that x* is the
     exact solution of

     (A + F) x* = b.

**** Sine button
     (from Sauer) say that we find an approximation to sin(x) - x = 0, xc =
     0.001. Backward error is 1.6e-10 (amount we need to change f(xc) by)

** Forward error
   Wikipedia : difference between result and solution

   Sauer : Say that f(r) = 0, xc ~ r. Then forward error is r - xc. This is the
   amount we need to change the approximation by to make it correct.

**** Sine button
     Same example as above: sin(x) - x = 0, xc = 0.001. Plug it in and get r =
     xc = 0 - 0.001 = 0.001

** LU Factorization
   L unit lower -> Doolittle factorization (L(k,k) = 1 at kth step)
   U unit lower -> Crout factorization (U(k,k) = 1 at kth step)
*** Solving
    from A = LU, solve Ly = b -> Ux = y.
*** Building
    Create U by row operations (Gaussian elimination)
    Store the -1*(row multipliers) in L (with unit diagonal.)
    takes about n^3/3 multiplications/divisions.
*** Pivoting
    Solve PA = LU instead - put largest entries on diagonals.
    Find the largest entry in the 1st column and row swap so it is now at the
    pivot. Reduce.
    Repeat the same thing for each column.

** Elementary Matricies
   Describe the 3 elementary row operations
*** Constructor
    ElementaryMatrix E = ElementaryMatrix Vector u, Vector v, Float sigma
    ElementaryMatrix u v sigma = IdentityMatrix - sigma * u * (conj. trans. v)
**** Exchange rows
     u = v = e_i - e_j to interchange rows i and j
     E = I - (e_i - e_j)(e_i - e_j)^T

     also referred to as I_ij : I_ij A permutes ith and jth rows of A
     A I_ij permutes ith and jth COLUMNS of A
**** Scale row i by alpha
     u = v = e_i
     sigma = 1 - alpha
     so E = I - (1 - alpha)e_i e_i^T

     then EA scales the ith row by alpha.
**** alpha * R_i + R_j -> R_j
     u = e_j, v = e_i
     sigma = -alpha
     E = I + alpha e_j e_i^T

     then EA does what we want.
*** Nice properties
    u is an eigenvector of E if u /= 0 : eigenvalue is 1 - sigma v^H u.
    if v^H x = 0 then x is an eigenvector, eigenvalue 1.
    if sigma /= 0, v /= 0, and u^H v /= 0 then E has two eigenvalues:
    1. 1 n-1 times
    2. 1 - sigma v^H u one time

    If 1 - sigma v^H u /= 0 (the determinant, see above) then

    E^-1 = E(u, v, -sigma/det(E))
** Gaussian transformation matrix
   special case of the elementary matrix
   L_j(l_j) = E(l_j, e_j, -1)
   where l_j = [0, 0, .. l[j+1,j], .. , l[n,j]]^T.

   looks something like the indentity matrix where one column, below the main
   diagonal, is filled with l[j+1, j] etc
*** Properties
    L_j(l_j)^-1 = E(l_j, e_j, 1) = -1 * L_j(l_j)

    in particular, for a lower triangular matrix:

    L = L_1(l_j) L_2(l_2) .. (product of Gaussian transformation matricies)


** Things to put on cheat sheet
*** Chapter 1
    Lagrange form of interpolation error
    MVT for integrals, MVT plain
    IVT
    Taylor's Theorem
    something about the problem with error analysis in PS2 p3
    O/o notation
    Machine arithmetic fun facts
*** Chapter 3
    LU formula - GEPP, GESRP
    Cholesky formula
    Sparse matrix storage.
    Thomas algorithm
    Spectral radius and norms
* 9/26/11
** Powers of a matrix

   the geometric series

   I + A + A^2 + A^3 + ... (to infinity)

   converges iff lim (k -> infinity) A^k = 0.

   If the limit exists then I - A is nonsingular and

   sum (k from 0 to infinity) A^k = (I - A)^-1

*** Proposition 3.9
    if the matrix norm ||A|| < 1 then both I - A and I + A are nonsingular, and

        1/(1 + norm(A)) leq norm((I plusminus A)^-1) leq 1/(1 - norm(A))

    (proving the plusminus is homework.)

**** Proof
     by previous work : rho(A) leq norm(A) < 1. Then by definition of rho (and
     the fact that I +/- A is a polynomial) then the eigenvalues of (1 +/- A)
     are 1 +/- rho(A) /= 0 (as rho(A) is the max eigenvalue)

     Therefore both have nonzero eigenvalues, so they are invertible.

     next part: I = (I + A)(I + A)^-1

     so by norm properties (see slide 99, mostly triangular tango) we have that

     1 = norm((I + A)^-1) * norm(I + A)

     so 1/(1 + norm(A)) leq norm((I + A)^-1)

     other part: show that I + A has an inverse, then distribute and
     rearrange with more triangular tango - more slide 99.

     Works as (1 - norm(A)) > 0 by assumption.

     (this is usually combined with 3.8 and is called Banach's lemma)

**** Example
     See slide 100 - for a infinity norm of 0.6 then (I + A)^-1 and (I - A)^-1
     should exist. If we go and calculate the inverses and (I +/- A)^-1 we get
     that they do fall between the bounds.

** Roundoff Error and conditioning in Gaussian Elimination
   Numbers on computers - always some error. LU, GE, Cholesky - all corrupt.
   Ax = b -> computer sees A + dA, b + db. Start off badly - can't even enter
   the problem!

   so the computer handles what WE would call (A + da)(x + dx) = b + db

   how large is dx? forward error analysis.

*** Theorem 3.10 (more general version)
    Assume that some beta-norm is an induced matrix norm, A is nonsingular, and

    norm(dA, beta) norm(A^-1, beta) < 1.

    then

    norm(dx, beta)/norm(x,beta) leq combination of norms of A, B, dA, db,
    A^-1. Bounded!

    therefore, to a constant, the solution error is proportional to the data
    error.

**** Proof - standard things.
     Distribute (A + da)(x + dx) = b + db. Recall that Ax = b to simplify the
     result (every term should have a d on it).

     see slide 103 and use Banach's Lemma twice - just brute force field
     algebra from there on.
*** Conditioning
    We say that the problem is ill-conditioned if a slight change in A or b
    drastically changes x. The factor from the work before is called the
    condition number (again, it is very long. See slide 103).

    condition number = K_beta(A) = norm(A,beta) * norm(A^-1,beta)

* 9/28/11

** Computing Project 1
   coming up. Most code will be supplied.

** Condition Numbers
   If the solution Ax = b changes 'drastically' when A or B are perturbed we
   call it 'ill-conditioned'

   Call K_beta (A) = norm(A, beta) * norm(A^-1, beta) to be the condition
   number of A
   why? All the neat things we derived! See above slides.

   Computing the condition number is not easy, but important

   always have that the condition number is geq 1. homework - show that it can
   be one (namely identity, others work as well)

*** Condition number in 2-norm
    for A^H A has eigenvalues mu1 geq mu2 geq mu3 geq ...

    then (A^H A)^-1 has eigenvalues 0 leq 1/mu1 leq 1/mu2 leq ...

    As we know that norm(A,2) = sqrt(spectral radius of (A^H A)) = sqrt (mu1)
    so norm(A^-1,2) = sqrt (1/mu_n)

    so for a small matrix, we can calculate by finding eigenvalues of A^H A.

**** Hermitian A
     Then A^H = A, so we want eigenvalues of A^2. THerefore the eigenvalues are
     (mu)^2s. Therefore we get

     cond(A) = abs( max(eigenvalue)/min(eigenvalue) )

** Error in Gaussian Elimination
   solving Ax = b yields xhat. By backward analysis we get

   (A + F)xhat = b (the problem we actually solved)

   Ax - Axhat = -F xhat
   so x - xhat = -A^-1 F xhat.

   then (with some forward analysis)

   norm(x - xhat,inf) / norm(xhat,inf) leq norm(A^-1,inf)norm(F,inf)
                   = K_inf(A) norm(F,inf) / norm(A,inf)

   Someone showed something like
   norm(F,inf)/norm(A,inf) leq c_n g theta

   where c_n depends on the size of A
   g is a constant factor in Gaussian Elimination

   g = max over i,j,k of | a_ij^(k) | / max over i,j of | a_ij |

   theta is the roundoff error on the machine.

*** generally

    c_n = 1.01 n^3 + 5(n + 1)^2

    for complete pivoting:
    g leq (n 2 3^(1/2)4^(1/3)...n^(1/(n-1)))^(1/2) (thanks, Wilkinson)

    for partial pivoting:
    g leq 2^(n-1)

    no pivoting: g can be arbitrarily large (!!!)

    in most applications, the growth factor is much smaller. Good research area
    (finding tighter bounds based on applications)

** Iterative Methods for solving Ax = b

*** TODO : update with neat general info from slides.

** Classical methods
   Choose M such that solving My = g is easy.

   Let A = M - N. Then Ax = b -> Mx = Nx + b

   Choose a guess x^(0) and update by Mx^(k+1) = Nx^(k) + b.

* 9/30/11
 cm** Basic ideas for classical iterative methods
   Strategy - split so that My = g is easy to solve.

   Set A = M - N
   Ax = b -> Mx = Nx + b
   guess x0, find x1 by Mx1 = Nx0 + b, easy to solve by design.

   We can rewrite this for B = M^-1 N, c = M^-1 b, as
   x^k+1 = Bx + c (theoretical! can't invert.)

   this is useful for analyzing convergence. We will see why soon.
   (lots of homework discussing the convergence of this sort of thing)

*** How do we turn this in to a finite procedure?
    Usually artificial (best one - physics of the application)
    one way to do it - see if xk, xk+1 close. Not much improvement means we are
    not getting anywhere (so stop)

    another way to do it - look at the residual of the solution (substitute in
    Axk - b, solve and check residue)

    yet another way - set number of iterations (very artificial)

    can mix and match!

*** Classical ways to split A
    splitting - not factorizing
    say A = L + D + U (free to do)

**** Jacobi method

     set M = D, N = -1(C_L + C_U) (C_L is lower diagonal entries of A, C_U is
     upper)

     so for Jacobi - Dxk+1 = -(C_L + C_U)xk + b.

**** Gauss-Seidel method
     ALso calles successive relaxation.

     M = C_L + D, N = -C_U

     then (C_L + D) xk+1 = -C_U xk + b (solve and update! new vectors depend on
     the old vectors)

     this is were the 'successive' relaxation comes from.

*** What is the difference?

    Jacobi needs xk and xk+1, Gauss Seidel stores xk and sequentially
    overwrites.

    Jacobi, however, may be done in parallel! However in sequence GS is faster
    (we will see why soon)

*** Successive Overrelaxation
    Best for the 60s
    M = C_L + 1/sigma D
    N = - [ C_U + (1 - 1/sigma)D]

    relaxation parameter sigma.

    derivation - A = D + C_L + C_U
    therefore 1/sigma D x + (D + C_L + C_U)x = 1/sigma D x + b
    therefore (C_L + 1/sigma D)x = -[C_U (1 - 1/sigma) D]x + b
    (this is a fixed point form, so we may iterate)

    This is the same as Gauss-Seidel if we pick sigma = 1.

**** How do we pick a good sigma? Depends on each problem.
     Very fast for good sigma values. However, if we have an odd problem, no
     tuned sigma -> slow.

     sigma < 1 -> underrelaxed (generally bad, but not always)
     sigma = 1 -> Gauss-Seidel
     sigma > 1 -> overrelaxed (most common)

** Comparison of Jacobi, Gauss-Seidel, and SOR
   can we always compute? Will they always converge? (no)

*** Convergence
    say convergent if it works for all starting points.

    errork = x^k - x which implies that e^k+1 = B^k+1 error(0)

    rate of convergence depends upon the initial guess!

**** Theorem 3.14
     the following are equivalent:
     a. iterative method convergent
     b. rho(B) < 1
     c. Exists matrix norm such that norm(B) < 1.

*** Error bounds Slide 124
    From a lot of matrix math, we get that
    epsilonk = - (I - B)^-1 B^k(x(1) - x(0))

* 10/03/11
** More iterative methods
   Review : norm(e^(k+1)) leq norm(B^k+1) * norm(e^0)
   There are two ways to lower the error - better initial guess, get
   norm(B^k+1) closer to zero.
*** How can we get a more concrete bound?
    say that norm(B^k) * norm(epsilon^1) leq TOL * norm(epsilon^1)

    then (norm(B^k)^k)^(1/k) leq TOL, so, using logarithms

    (-1/k log(norm(B^k))) log(tol^-1) .LEQ. TOL

    which gives us that the number of iterations needed is proportional to the
    inverse of the log of the matrix norm.

    Then R_inf (B) + lim R_k(B) = - ln (rho(B)) where rho(B) < 1 for
    convergence.

    Where does this crazy condition come from?

*** Theorem 3.21 : bounding the matrix norm for B
    For any nxn matrix B and any matrix norm, we have
    lim (k to infty) norm(B^k)^(1/k) = rho(B)
**** Proof.
     (rho(B))^k = rho(B^k) leq norm(B^k)
     by proposition 3.3 -> rho(B) leq norm(B^k)^(1/k), for all k.

     Lets use an auxillary matrix. Let

     B(epsilon) = 1/(rho(B) + epsilon) B ->
     rho(B(epsilon)) = rho(B) / (rho(B) + epsilon) < 1

     therefore lim (k to infty) (B(epsilon))^k = 0. The matrix converges!

     this means that the norm of B(epsilon) goes to zero, so for some k > k0 we
     get that

     norm(B^k) / (rho(B) + epsilon)^k = norm(B(epsilon)^k) < 1, forall k
     .GEQ. k0

     Therefore if we take the limit for epsilon -> 0 we get that

     lim (k to infty) norm(B^k)^(1/k) leq rho(B) because epsilon is arbitrary.

*** Asymptotic rate of convergence
    We say that the asymptotic rate of convergence for B is R_inf(B).

*** Theorem 3.16 (Stein, Rosenberg)
    if B_J is nonnegative then B_J and B_GS can satisfy precisely one of the
    following relations:

    1. rho(B_GS) = rho(B_J) = 0
    2. 0 < rho(B_GS) < rho(B_J) < 1
    3. rho(B_GS) = rho(B_J) = 1 (if one spectral radius is 1, the other is as
       well)
    4. 1 < rho(B_J) < rho(B_GS) (if it converges, it converges faster. If it
       diverges, it diverges faster)

    useful - if the iteration matrix for Jacobi fails then the iteration matrix
    for Gauss-Seidel fails as well.

*** Special matricies
    Given some M, consider the absolute value of each entry; call this
    matrix |M|

    we say that M .GEQ. N iff M[i][j] > N[i][j]

    then | AB | .LEQ. |A| |B|.
**** Theorem 5
     if A is strictly diagonally dominant, then
     norm(B_GS) .LEQ. norm(B_J) < 1.
***** Proof
      A = D + C_L + C_U and
      B_J = -D^-1 (C_L + C_U) = -D^-1 (D + C_L + C_U - D) = I - D^-1 A
      (D inverse is guaranteed to exist because of strict diagonal dominance)

      norm(B_J,inf) = norm(I - D^-1 A,inf) = max ( sum (from j=1, j /= i, to j
      = n) abs( A[i][j] / A[i][i])) < 1.

      There are several steps for the first inequality. Know this for prelim.
* 10/05/11
** Recommendation - reread slide 131 onward
** Convergence of the SOR method
   We have two general ways to show convergence - norm(B) < 1 or rho(B) < 1.
*** Kahan Theorem
    spectral radius of the iteration matrix of the relaxation method satisfies

    rho(B_SOR(sigma)) .GEQ. abs(sigma - 1)

    Therefore the SOR method cannot converge if sigma is outside (0,2).
**** Proof
     B_SOR(sigma) = -(C_L + 1/sigma D)^-1 * [C_U + (1 - 1/sigma)*D]
     (we assume that it is convergent, so the matrix inverse already exists in
     the first bit)

     (note that the product of the eigenvalues equals the determinant)

*** Final version
    Theorem 3.15 - based on lemmas from notes that help us along
    Read it! Quick.

*** Optimal Relaxation Parameters
    Read it.

* 10/10/11
** Sparse methods
   Ax = b - nice and linear. But we can solve it faster if we make it
   nonlinear!

   so for the next few lectures, we will reduce Ax = b to a nonlinear problem.

*** Useful tool - inner products.
    We say that (x,Ay) = (A^Tx,y) (we can move linear operators around)
    this is useful for symmetric matricies.

    (x + y, z) = (x,z) + (y,z) among other properties.

    (modern mathematics is about structure! We do not care about sets anymore;
    anything worthwhile has structure behind it, certain properties, etc)

    also recall, for some standard multivariable function : q(x1,x2,x3) =
    q(bar(x))

    so 1/2 * (x, Ax) - (x, (4 5)) = 1/2(x1,x2) A (x1, x2)^T - (x1, x2) (4, 5)^T

    a nonlinear version of a linear equation.

    for A = [ [2,1], [1,3] ] we get

    0.5 * (2x1^2 + x1x2 + x1x2 + 3x2^2) - (4x1 + 5x2)

    why did we rewrite this as some awful nonlinear problem???

**** Theorem 3.22
     instead of direct solution, find a minimal solution:

     x* is the solution to Ax = b iff x* is a minimizer of q(x).

     (for some q(x) = 0.5(x, Ax) - (x,b))

     Proof. As A is SPD, A^-1 exists and is SPD as well. Consider some function

     F(x) = (b - Ax)^T A^-1 (b - Ax)

     which is nice as

     F(x) = (A^-1 (b - Ax), b - Ax)
          = (A^-1 b,b) - (x,b) - (A^-1b, Ax) +  (x, Ax)
          = ((x, Ax) - 2(x,b)) + (A^-1b, b)
          = 2q(x) + (A^-1b, b)

     so if x minimizes q, x minimizes F. Additionally the minimum should be
     unique.

     In general, GRAD q(x) = Ax - b = -r (residual)

*** Following the minimization algorithm
    say we have some initial guess x1. How can we find a better guess? (that
    is, decrease the residual)

    We can use the gradient! The gradient tells us what the steepest descent
    is. If we follow the negative gradient the function q(x) will
    decrease. Therefore the function is guaranteed to decrease on that line so
    we know on what line x2 lies.

    Therefore all we need is some constant, or

    x2 = x1 + t_1 r1 - some scalar variable t.

    Then Q(t)  = q(x1 + tr1) = mess
         Q'(t) = -(r1,r1) + t(r1,Ar1)

    so the best value for t is just (r1,r1)/(r1,Ar1)

**** Does this actually converge?
     consider the A-norm (TODO - show that this is a norm)

     norm(x,A) = sqrt((Ax,x))

     let e^k = x^k - x*, r^k = b - Ax^k

***** Theorem 9 - important results for analysis of convergence. This is also informal HW

      (r^k+1,r^k) = 0 (bad numerically - lot of traveling in parallel
      direction)
      e^k = -A^-1r^k
      r^(k+1) = r^k - t_kAr^k


* 10/12/11
** Review of steepest descent
   Find the optimal scalar for the remainder and go in that direction.
   Purely a minimization algorithm - easy to extend to nonlinear.

   Why do we like this method? Easy intuition, prompt discussion.

*** Analysis of Steepest Descent

    Kantorovich inequality - if A is SPD, then

    1 .LEQ. norm(x,A)^2 * norm(x,A^-1) / (x,x)^2
      .LEQ. (lambda_1 + lambda_n)^2 / (4 lambda_1 lambda_n)

    where lambda_n is the greatest eigenvalue and lambda_1 is the least
    eigenvalue.

**** Proof

     WLOG assume norm(x,2) = 1. Then

     A is SPD, so we have some Q s.t. A = Q^T D Q, A^-1 = Q^T D^-1 Q
     where D is a diagonal matrix of eigenvalues.

     then, for y = Qx, norm(x,A) = norm(y,D) so

     norm(x,A)^2 norm(x,A^-1)^2 = norm(y,D)^2 norm(y,D^-1)^2

     and, as Q is unitary, norm(y,2) = 1.

     TO show that norm(y,D)^2 norm(y,D^-1)^2 .GEQ. 1, we need

     y_D = sqrt(diag(D)) .* y
     y_D^-1 = transpose((1 ./ sqrt(diag(D))) .* y)

     This part needs four small lemmas. Check slide 160.

* 10/14/11
** More on the steepest descent method
   We have still assumed that everything is exact. We have also assumed that we
   have rather large matricies.

** Conjugate Gradient Method
   in Steepest Descent - used each new step is in the direction of the
   gradient. Follow the direction at each point (we have a specific formula to
   compute this)

   The search direction from SD made sense from Sophomore Calculus, but not so
   much for matricies. Locally, each choice is the best, but globally it is not
   that great.

   Now we search in the direction of the Conjugate Gradient instead (hence the
   name)

**** Conjugate Gradient
     If A is symmetric, we say that x and y are conjugate or A-orthogonal if

     (x, Ay) = x^T A y = 0

*** Algorithm Motivation

    Q(t) = q(x0 + t * p0)
         = 1/2 * (Ax0, x0) + t (Ax0, p0) + 1/2 t^2 (Ap0, p0)
         - 1/2 (b,x0) - t(b,p0)

    CG - use the previous information to generate a better vector.

    So, we find the direction at x1 conjugate to p0 :
    p1 = -r1 + mu1 * p0 s.t. (p0, Ap1) = 0.

    Then (p0, -Ar1 + mu A p0) = 0 -> mu1 = (r1, Ap0) / (p0, Ap0).

*** Algorithm

    r0 = Ax0 - b, p0 = -r0.

    for k in [0..M-1] :
        t_k = -(rk, pk) / (Apk, pk) and x_k+1 = x_k + t_k * p_k

        r_k+1 = r_k + t_k * A * p_k

        mu_k = (r_k+1, Ap_k) / (p_k, Ap_k)

        p_k+1 = -r_k+1 + mu_k * p_k

    endfor

    Uses two scalars and three vectors - more expensive than SD.

*** Properties

    Lots of properties. Check the book for lots of good information.

    -(r1, r0) = (r1, p0) = (r0 + t0 * A * p0, p0)
                         = (r0, p0) + t0 * (Ap0, p0)
                         = mess
                         = 0 woo

    therefore the remainders are orthogonal to each-other. Similarly, r_k and
    p_k-1 are also orthogonal.

**** Theorem : All the rks and pks are orthogonal
     for i /= j, (r_i, p_j) = 0 and (p_i, Ap_i) = 0. Done by induction from
     results above.

***** Important Corollary
      (p_k, r_i) = -(r_k, r_k) (simplification for calculating t_k and mu_k)

* 10/17/11
** More CG
   L_k  = span of p0, p1, p2, ...
   Pi_k = {x s.t. x = x0 + z, z in L_k} (shifted space)

*** Lemma
    The sequence x0, x1, ... xk is such that

    q(xk) = min q(x0 + z)

    iff xk in Pi_k, rk perpendicular to L_k where rk = Axk - b.

**** Proof

     Condition necessary - Review slides 176-177.

*** Theorem 11
    Asume that x0, x1, ... is the sequence generated by CG. Then

    q(xk) = min(q(x0 + z)) forall k .GEQ. 1.

**** Proof
     From the CG procedure we have that

     xk = xk-1 + tk-1pk-1
        = xk-2 + tk-2 * p_k-2 + tk-1 * p_k-1
        etc...

     therefore xk is in the span of the pks.

     Furthermore, we need that rk is perpendicular to L_k.

     for k = 1, this is valid as (r1,p0) = 0.
     Now assume that rj is perpendicular to Lj. then by induction (for k+1)

     r^k+1 = rk + t_k * A * p_k -> (rk+1, p_k) = 0.

** Krylov Subspaces
   Given some matrix A and a vector v, the mth Krylov subspace is

   {v, Av, ... A^m-1 v}

*** Theorem (Krylov from CG)

    if r^(m-1) /= 0 then we have

    Lm = span(r0,r1, ... , r^m-1) = Km (A,r0) for some m .LEQ. n.

    (so our L_k is the Krylov subspace)

**** Proof
     m = 1, holds as p0 = -r0.

     Assume that it is true for m = k. THen for m = k+1, we have

     r_k-1, p_k-1 in K_k so A * p_k-1 in K_k+1

     therefore r_k = r_k-1 + t_k A * p_k-1 in K_k+1

     so the span{r0, ... rk} subset K_k+1

     we know that the rs are linearly independent, so the dimension of that is
     k+1, or

     k+1 .LEQ. dim(K_k+1) .LEQ. k+1

     so the span of the rs = K_k+1 .

*** TODO Homework - show that the ps are linearly independent.

* 10/19/11

** Krylov Methods

   Getting more popular! CG is actually getting less popular.

   Krylov is a goldmine. So is Chebyshev!

** Chebyshev Polynomial

*** Recursive Definition

    T0(x) = 1, T1(x) = x
    T_n+1(x) = 2x * T_n(x) - T_n-1(x)

    More sophisticated - do not need recursion.

    Can show T_n(x) = 1/2 * ((x + sqrt(x^2 - 1))^n + (x - sqrt(x^2 - 1))^n)

*** Lemma 5

    Suppose that p(x) is a polynomial of degree k s.t.

    p(0) = 1, abs(p(x)) .LEQ. r

    then for any x0, the sequence {xk} generated by the CG method satisfies

    norm(ek, A) .LEQ. r * norm(e0,A)

    That is, error is bounded by initial error. We will show that r -> 0 later.

**** Proof

     (q(x) -- minimize energy.)
     q(x) = 1/2 (x, Ax) - (x, b) = 1/2( (x,Ax) - 2*(x,b))

     by previous work, we can work this down to

     q(xk) = min (x in x0 + K_k(A,r0) ) q(x)

     that is, minimizing over some x in the kth Krylov space.

     This gets a bit messy, but uses the Chebyshev polynomial's properties in
     order to get that the CG method error is bounded by the square root of the
     condition number.

     Next time - preconditioning.

* 10/21/11

** Review - CG versus SD

   CG performance dictated by A - number of steps bounded by condition number.

** Preconditioning and CG

   Given some Ax = b with large K(A), we want some preconditioner Q = E * E^T

   where R = A - Q is small.

*** Converting to a problem we would like to solve

    Ax = b, but we would really like to say x = A^-1 b.
    A(E^-T E^T ) x = b

    so E^-1 A (E^-T) (E^T x) = E^-1 b.

    We hope that K(E^-1 A E^-T) << K(A).

    This won't work for sparse matricies! It converts them back to dense.

    Algorithm for sparse-preserving CG given in slides -- approximately slide
    192

    This algorithm really just needs an input of some preconditioning matrix
    Q - therefore we can write a general solver with Q as an input!

*** Two examples of preconditioners

**** sigma > 0

     then calculate some E, where

     E = D^(1/2) + sigma * C_L D^(-1/2)

     then Q = EE^T

**** Incomplete cholesky factorization

     A = Q + R

     want to use some l_ij where if a_ii = 0 then set l_ij = 0. This preserves
     the sparsity and is almost A.

* 10/24/11

** New Unit - solving nonlinear equations.

   Nonlinear Equations

   Given some function f :: RR^n -> RR^n, find a zero.

   we have n scalar, multivariable functions. Not easy.

   Goal - find some x* s.t. f(x*) = 0.

   'zeros of functions, roots of equations'

** Easiest solver - bisection method

   always guaranteed; works for scalar functions.

*** Root Interval

    contains a root! Like [a,b] in bisection.

*** Theorem 2.1

    Suppose that f in C^0[a,b] and f(a)f(b) < 0. Then there exists some x* in
    [a,b] such that f(x*) = 0.

*** Theorem 2.2

    Given root interval [a,b] there is an x* s.t.

    abs(x_k - x*) .LEQ. (b - a) / 2^n (that is, we have convergence)

    This is useful. We already have existence and now we also have convergence
    (but we do not have uniqueness).

    Does it converge to some root? Yes!

    f(x*)^2 = lim f(a_k)*f(b_k) .LEQ. 0 (as f(a_k) and f(b_k) have opposite
    signs)

    therefore as f(x*) .GEQ. 0, f(x*) = 0. Done.

* 10/26/11

** Overview of project

   Already a matlab function? use it!

   Type up the report.

   1. various formulae for evaluating derivatives numerically.

   2. Solve by LL^T and GEPP by LU (so find LU by matlab).

   3. Sparse Matricies! solve them with various methods.

   4. Implement Crout factorization.

   5. Iterative methods. for CG and PCG - Matlab has CG built-in. PCG -
      preconditioned CG.

      inchol - incomplete cholesky factorization.

      also experiment with DROPTOL.

*** What is the difference between using sparse LU and normal LU?

    do not count the assembly time.

    do something like A = sparse(I, J, V) and pipe it through the LU
    solver. Matlab is smart enough to do the right thing when given a sparse
    matrix.

*** TODO where does that crazy imaginary finite difference formula come from?


** Fixed Point Iteration

   G(z) = z

   One way to solve (the dumb way) Picard method. Pick a guess x and set x1 =
   G(x), etc.

*** Why are we interested in fixed point?

    reduce f(x) = 0 to some G(x) = x : now we have an algorithm for solving
    these problems.

* 10/28/11

** More Fixed-Point

   we want to convert some f(x) = 0 to x = f(x)

*** Problems with Picard's Method

    What are the conditions for convergence? How fast does it converge?

    There are lots of different fixed point iteration schemes for any given
    problem. Which one?

**** Lipschitz condition

     We say that a function G satisfies the _Lipschitz condition_ in a set S if
     there exists some L .GEQ. 0 where

     norm(G(x) - G(y)) .LEQ. L norm(x - y)

     (we want to bound the function by the arguments)

     where L is called a Lipschitz constant (not unique)

     if L < 1, then we say G is _contractive_



*** Theorem 2.3

    (Contractive mapping theorem)

    Assume that S is a closed subset of RR^n and G is a contractive function
    from S INTO S. Then

    1. G has a unique fixed point p in S

    2. Any sequence generated by the picard iteration of G with x0 in S will
       converge to P.

    3. The error of the

**** Proof

     x^(k+1) = G(x^k), for k = 0,1,2,3,... in S.

     by the contractive property, exists L in (0, 1) s.t.

     norm(x^(k + j + 1) - x^(k + j))
     = norm (G(x^(k+j)) - G(x^(k+j-1))) . LEQ. L norm(x^(k+j) - x^(k+j - 1))

     so we can keep generating L, L^2, ... and as L < 1 we get convergence.

     Alternatively, we may use the telescoping property to delete a lot of
     terms and end up with something like

** Review of Multivariable Calculus

*** Gateaux Differentiability
    if

        lim (t -> 0) (f(x + tz) - f(x))/t

    exists, we say that f is _Gateaux differentiable_ at x along the direction
    of z.

    we call Df(x)(z) that Gateaux derivative of f at x along z.
*** Frechet Differentiability
    If there is a linear mapping f'(x) :: RR^n -> RR^m where

    lim (norm(Delta x) -> 0)
    norm( f(x + Delta x) - f(x) - f'(x) * Delta x)/norm(Delta x) = 0

    we say that f is _Frechet Differentiable_ at x and we call F'(x) the
    Frechet Derivative of f at x.

    We can show that f'(x) is also the Jacobian matrix of f(x). In particular,
    for f(x) we have

    f'(x) = Df(x) = (Gradient f(x))^T
* 10/31/11

** Review of Fancy Derivatives

*** Frechet Derivatives

    for some vector x and vector function f (and matrix A)
    f(x + Delta x) = f(x) + A Delta x + HOT (Taylor expansion)

    We can show that f'(x) = Jacobi matrix of f(x) (see definition of Frechet
    derivative)

** Determine if a function is a contractive mapping

   We would like to borrow tools from calculus. We say that a set

   D0 `subset` RR^n

   is convex provided

   lambda x + (1 - lambda) y in D0 for all x,y in D0 and lambda in [0,1]
   (often called 'convex combination')

   for lambda = 0, we get y, lambda = 1 we get x, otherwise some weighted
   average.

*** Example

    Triangle - can draw a chord between any two points that is contained inside
    the boundary.

    Ball - same trick.

    Star - Doesn't work (can draw between points on ends)

*** Theorem 8.3

    Let D0 in RR^n be convex and G :: D0 -> RR^n be such that all of its
    entries g_i(x) have continuous and bouneded partial derivatives of 1st
    order on D0. Then the

    norm(G(x) - G(y)) .LEQ. L norm(x - y)

    for L = sup (w in D0) norm(G'(w)) (Lipschitz function)

**** Proof

     focus on each of the entries (multivariable functions).

     Let Psi_j(s) = g_j (x + s(y - x)) for s in [0,1], j in [1..n]
     (this is another way to write convexity)

     Then d Psi_j / ds = SUM (k = 1 to n) dg_j(x + s(y - x)) / dx_k (y_k - x_k)

     the summation occurs because we have the product of the Jacobian with [y1 -
     x1, y2 - x2, y3 - x3, ... , x_n - y_n]

     There are a few more steps - see slides 18/19.

*** More general - Theorem 8.4 (Ostrowski)

    Assume that p is a fixed point of G(x) and G(x) is Frechet differentiable
    at p with rho(G'(p)) < 1.

    Then there exists a ball B_r(p) such that for any x0 in B_r(p), the
    sequence x_k generated by Picard iteration converges to p.

    Therefore if we start in the ball we are guaranteed to converge - we don't
    know the details of the ball, however.

**** Proof

     As the spectral radius < 1 we know that there is a norm where

     norm(G'(p)) .LEQ. rho(G'(p)) + epsilon = sigma + epsilon,

     where epsilon > 0 and L = sigma + 2 * epsilon < 1.

*** Theorem 2.4

    Assume that the fixed point iteration converges (lim (k to infty) x_k = x*)
    where x_k+1 = g(x_k) (picard iteration)

    Assume that q is the first positive integer for which g^q(x*) /= 0 (if q =
    1 then abs(g'(x*)) < 1)

    Assume g in C^q(S) where S is an open set s.t. x* in S.

    Then x_k converges to x* with order q.

**** Proof

* Fixed Point Iteration

** Easy 2D example

   g(x)  = (x^2 + 6) / 5

   g'(x) = 2x / 5
   Since g'(2) = 4/5 (the fixed point) it will converge rather slowly. Why?


** Another Example

   say that g(x) = x/2 + 2/x

   Then g(x) = x - f(x) / f'(x) - f''(x)/2f'(x) (f(x)/f'(x))^2 - cubic convergence!

* Newton's Method

** Overview

   Say we can approximate the solution x* of f(x) = 0 by x_k. Then:

   f(x) = f(x_k) + f'(x_k) (x - x_k) + 1/2 f''(x_k)(x - x_k)^2 + HOT.

   We know that

   f(x*) = 0 -> 0 = f(x_k) + f'(x_k)(x* - x_k) + ...
                  ~ f(x_k) + f'(x_k)(x* - x_k)

   we just say that x_k is close enough to x* s.t. that approximation
   works. Therefore we can solve for x* given some x_k, and call that our new
   approximation.

** Newton-Kantorovich

   Purely for single-variable, scalar function. The analysis here is easier to
   understand (we get the same results but the analysis is harder)


*** Lemma 2.1 - boundedness of Newton's method.

    for f in C2, we can bound the newton iteration.

    f(y) = f(x) + f'(x) (y - x) + R(y,x)

    where R(y,x) = (y - x)^2 integral (0 to 1) (1 - s(f''(x + s(y - x)))) ds

*** Theorem 2.5 - Existence of unique intervals.

    for f in C2 and m,M > 0 s.t. |f'(x)| > m, |f''(x)| .LEQ. M

    Then, for each zero z of f in G, we have some [z-p,z+p] subset G s.t.

    1. z is the only zero of f in [z - p, z + p]

    2. The sequence starting in [z - p, z + p] will remain in the interval and
       converge to the zero.

    3. We can estimate the error in three ways:

       1. |x_k - z| .LEQ. 2m/M q^(2^k) with q = M / (2m) | x0 - z |

       2. |x_k - z| .LEQ. 1/m |f(x_k)| .LEQ. M/(2m) |x_k - x_(k-1)|^2

       3. |x_(k+1) - z| .LEQ. M/2m |x_k - z|^2 (quadratic convergence)

**** Proof

     Use the mean value theorem to bound ((f(x) - f(x~))/(x - x~)) by f'(xi),
     which is bounded below by m.

     See the slides for more! Builds a lot on the sequential property of
     newton's method (that is the linearization)


** Vector Newton

   Newton's method for vector equations - requires inverting a matrix. Bad!



*** Theorem 8.6 - Convergence of Vector Newton

    Assume that f : D subset RR^n -> RR^n is Frechet-differentiable on some
    ball and that f(x*) = 0.

    Also assume that f'(x) is continuous at x* and f'(x*) is nonsingular (that
    is, the Jacobi matrix near x*)

    Then there exists some r > 0 such that the sequence generated by Newton's
    method with and x0 in the ball will converge to x* (Newton-Kantorovich, in
    n-dimensions).

*** Proposition 8.1

    If we have 8.6, then we have superlinear convergence.

** Methods with-out taking derivatives

   We want fast convergence, but we do not want to take derivatives.

   In particular, we don't want to keep calculating derivatives - finding J,
   J^-1 is hard.

*** Updating the Jacobi matrix

    Don't update the Jacobi matrix at each step. Instead, use the same one 'm'
    times and then update.

    in 1D we can pretty easily calculate the slope, just by

    (y1 - y0)/(x1 - x0).

*** Secant Methods

    We now depend on both x_k-1 and x_k-2 to find x_k. We can't examine
    convergence by fixed-point analysis methods (since we depend on two
    previous values).

    Instead, we can use Lagrangian interpolation to do it.

*** Broyden Methods

    x^k+1 = x^k - (inverse A)f(x^k) or
    x^k+1 = x^k - H_k * f(x^k)

    Assume that the change in x is small, so

    f(x^k+1) - f(x^k) approx f'(x^k+1) delta x (quasi newton condition)

    The quasi-newton equation only provides n equations, but we need n^2
    conditions for building the matrix.

    Therefore we can form the matrix A^k+1 by 'correcting' the previous matrix
    in n entries.

**** Broyden 1 (1965)

     Let E_k be a matrix of rank 1. We can represent this as a product of two
     vectors v and u

     E^k = v^ku^k

     Let v^k = Delta x^k. Then the quasi-newton method is

     (A^k + u^k (v^k)^T) (Delta x)^k = f(x^k+1) - f(x^k)

     We can multiply it out, moving around A^k

     u^k (v^k)^T (Delta x)^k = f(x^k+1) - f(x^k) - A^k (Delta x)^k

     Therefore

     u^k = 1/( (v^k)^T(Delta x)^k ) (f(x^k+1) - f(x^k) - A^k Delta x^k)

     which gives us a way to solve for some u^k.

* Eigenvalue Calculations
** Definition of the Problem
   If

   (A - lambda I)x = 0

   has a nontrivial solution then lambda is an eigenvalue of A. A nontrivial
   solution x is an eigenvector of A corresponding to the eigenvalue lambda.

   clearly x = 0 vector is a solution, but no one cares about that.

   difficulty - two unknowns, one equation.

   geometric interpretation : A rescales eigenvectors, they still point in the
   same direction.
** Characteristic/eigen polynomial
   Delta_A(lambda) = determinant(A - lambda I) is called the _characteristic
   polynomial_ or _eigenpolynomial_ of A.

   Let lambda1, lambda2, ... be the distinct eigenvalues of A. Then

   the characteristic polynomial:
   Delta_A(lambda) = (-1)^n (lambda - lambda1)^a1 (lambda - lambda2)^a2 ...

   where each a1,a2,... is the multiplicity of lambda1, lambda2, ...
*** Subspace Properties
    we may form a subspace from each eigenvalue.

    The subspace {x s.t. (A - lambda_i I) x = 0}

    is a subspace of RR^n. It has dimension dim(L(lambda_i)).
*** Relating m_g(lambda_i) and m_a
    m_g is the geometric multiplicity of the eigenvalues - given some lambda,
    how many eigenvectors can we find that are not multiples of each-other?
    (that is, how many free variables are there when we solve (A - lambda I) x
    = 0 beyond the first)

    1 .LEQ. m_g(lambda_i) .LEQ. m_a(lambda_i)

    the geometric multiplicity must be less than or equal to the algebraic
    multiplicity. We generally want equality.

    We say that A is _defective_ if if has an eigenvalue such that

    m_g(lambda_i) < m_a(lambda_i)

    this is because the dimension of the subspaces will not sum to the dimension
    of the matrix.
    Proof - Kincaid-Cheney.
** Similar Matricies
   B = P^-1 A P has the same eigenpairs as A. (invariant under conjugation)
** Gershgorin's Disk Theorem
   Shows us where the eigenvalues are, or at least a neighborhood of each
   eigenvalue.

   Statement - all eigenvalues are contained in the following n Gershgorin
   disks in the complex plane:
*** Proof

    Let lambda be an eigenvalue of A and let x be the corresponding
    eigenvector. Assume that the kth entry of x is such that

    abs(x_k) = norm(x,inf) /= 0

    (one of the entries must be the largest). This suggests a way to index.

    From the kth equation of Ax = lambda x we get

    (lambda - a_kk)x_k = SUM (j = 1, j /= k to n) a_kj x_j
    (due to the matrix-vector product)

    this leads to

    abs(lambda - a_kk) abs(x_k) .LEQ. SUM (j = 1, j /= k to n) abs(a_kj)
    abs(x_j)
    abs(lambda - a_kk) abs(x_k) .LEQ. abs(x_k) SUM (j = 1, j /= k to n)
    abs(a_kj)

    which allows us to cancel out the x_ks and we get

    abs(lambda - a_kk) .LEQ. SUM (j = 1, j /= k to n) abs(a_kj)

    which allows us to bound the radii.

*** Counting intersections

    Let S be the union of m connected Gershgorin disks of A that has no
    intersection with the remaining disks. Then S contains m eigenvalues of A.

** Schur Decomposition
   For an square matrix A, exists unitary matrix U and upper triangular matrix
   R such that

       A = URU^H

   This means that every square matrix is _unitarily similar_ to some upper
   triangular matrix. If we desire, we can do this for some L instead
   (construct U differently).
*** Important Result - Get All Eigenvalues
    We know that similar matricies have the same eigenvalues, so if we have R
    (eigenvalues are diagonal entries) then we have all the eigenvalues!

    This is not a very good way to compute eigenvalues.
** Power Method
*** Overview
    'Business of Computation'

    Find the dominant (largest magnitude) eigenvalue. (structural engineering
    application - largest eigenvalue of some building must not cause resonance
    with earthquakes)

    Let max(x) = x_j if |x_j| = norm(x,inf) (largest in absolute value)

    Note that max(a*x) = a*max(x). We can also take limits:

    lim (k -> inf) max(x_k) = max(x) if x_k -> x.

*** Algorithm

    We compute the largest eigenvalue by guessing a vector x0 s.t.
    norm(x0, inf) = 1.

    Repeat until x_k converges:

    xhat_k+1 = Ax_k
    sigma_k+1 = max(xhat_k+1)
    x_k+1 = 1/sigma_k+1 xhat_k+1 (normalize)

*** Where does the name come from? Theorem 3!

    Assume that A has a dominant eigenvalue with algebraic multiplicity
    one. Assume that A has n linearly independent eigenvectors (that is, it is
    not defective).

    Then the sequences {sigma_k} and {x_k} generated by the power method s.t.

    x0 = SUM alpha_i v_i alpha1 /= 0 (for linearly independent eigenvectors v1,
    v2, ...)

    Then sigma_k -> lambda_1 and x_k converges to the correlated eigenvector.

*** Proof

    x_k = 1/sigma_k xhat_k = 1/sigma_k Ax_k-1 (x_k is derived from x_k-1)

    we can recur and keep writing these:

    x_k = ... = 1/(sigma_k * sigma_k-1 * sigma_k-2 ...) A^k x0. (hence the
    name power method - we use a power of A)

    hmmm, looks like Krylov! On the other hand, we have that

    max(A^kx0) = max(A^k-1Ax0) = max(A^k-1xhat1) = sigma1 * max(A^k-1x1)
    (last step: factor out the normalization constant)
    = sigma1sigma2 * max(A^k-2 x2) = ... = sigma1sigma2...sigma_n max(xhat_k)
    (definition of xhat_k)
    = sigma1 * sigma2 * ... * sigma_n

    so xk = 1/PRODUCT(sigmas) * x0

***** Convergence Results: calculating the eigenvector

      x0 = alpha1 * v1 + alpha2 * v2 + ... with a1 /= 0. Then
      A^kx0 = A^k(alpha1 * v1 + alpha2 * v2 + ...)
            = alpha1*lambda1^k*v1 + alpha2*lambda2^k*v2 + ...

      so x^k = (alpha1*v1 + alpha2(lambda2/lambda1)^k + ...) /
      (Max (alpha1*v1 + alpha2*(lambda2/lambda1)^k*v2 + ...))

      Therefore x^k converges to some normalized eigenvector.

***** Calculating the eigenvalue

      xhat^k = Ax^(k-1) = A * (1/(max(A^(k-1)x0)) A^(k-1) x0)
                        = a mess. The factors on lambda1 go to 1.

      therefore sigma_k goes to lambda1.

*** Comments

    Not so nice for clustered eigenvalues at the top (recall that the error is
    proportional to (lambda2/lambda1)^(k-1), for second largest eigenvalue)

    Poor performance for a defective matrix (we need full eigenspace)

    How do we compute other eigenvalues? Not with this technique!

** Inverse Power Method

*** Overview

    (with help) can compute any eigenvalue, may converge faster. The main trick
    is to find some estimate of an eigenvalue.

*** Theory

    Assume that A is not defective and has the following eigenpairs:

    (lambda1, v1), (lambda2, v2), ... , (lambdan, vn)

    Then the matrix (A - lambda I)^-1 has the following eigenpairs:

    ((lambda1 - lambda)^-1, v1), ...

    This implies that if m_a(lambda_1) = 1 and if we choose lambda such that

    abs(lambda1 - lambda) < abs(lambda_i - lambda), i /= 1,

    then (lambda1 - lambda)^-1 is the dominant eigenvalue of (A - lambda I)^-1.

    Therefore we apply the power method to the inverse matrix: hence the name,
    _inverse power method_.

*** Algorithm

    Choose some lambda ~ lambda1, x0, tolerance, and maximum iterations. Then
    find some factorization of A - lambda I.

    solve (A - lambda I)xhat^k+1 = xhat^k.
    sigma_k+1 := max(xhat^k+1), x^k+1 = 1/(sigma_k+1) xhat^k+1

    post process - sigma_k+1 is approximately (lambda1 - lambda)^-1, solve for
    lambda1.

**** Problem

     How did we pick lambda1 ? We had to find it somehow (Gershgorin or physics
     of problem). This is a difficulty because we need a good parameter to
     find the eigenvalue.

*** Convergence

    Very fast! Why? We will see later.

** Symmetric Power Method

*** Overview

    Given some symmetric matrix we can exploit its properties to calculate the
    dominant eigenvalue. We may perform the calculation of the eigenvalue in
    half of the steps that we may use for another technique.

*** Algorithm

    Set some xhat0 := 1/norm(x0) x0.

    Do the following until the eigenvector converges:

    xhat^k+1 = Ax^k
    x^k+1    = 1/norm(xhat^k+1) xhat^k+1
    sigma_k  = (xhat^k+1)^T * x^k.

    By an example, we see that this takes about half as many steps as the power
    method. Can we prove that? Yes!

*** Proof

    Since A is symmetric, it has a set of n orthonormal
    eigenvectors. Therefore, for each eigenvector

    Av_i = lambda_i v_i

**** Lemma

     Assume that the SPM starts from some x0 = SUM alpha_i v_i, for not all
     alpha_i = 0.

     Then the vectors generated by the symmetric power method are

     xk = 1 / (sqrt alpha1^2 + SUM alpha_i^2 (lambdai/lambda1)^2(k-1)) *
     (alpha1 * v1 + SUM alpha_i^2 (lambdai/lambda1)^2(k-1))

     This proof needs a lot of steps; see the slides.

** Calculating Other Eigenvalues

*** Overview

    Two techniques: shift and deflation.

    Where do we need this? Building blocks for spectral methods (solve PDEs by
    eigenspaces)

*** Shift Technique

*** Deflation Technique

    We can always find at least one eigenpair (lambda1, v1) (the dominant one).

    Suppose we have S1 v1 = t * e1 (for t /= 0, some nonsingular S1)

    so Av1 = lambda1 v1 -> S1 A (S1^-1 S) v1 = lambda1 S1 v1.
    -> S1AS1^-1 e1 = lambda1 e1

    as eigenvalues are invariant under conjugation.

    A2 = S1 * A * S1^-1 = [lambda1, c^T;
                           0,       B_2]

    and lambda_2, ..., lambda_n must be eigenvalues of B2.

**** How do we find eigenvalues of S1?

     Read page 37!

*** Householder Matrix: Important Tool

    some elementary matrix, where H(u) = E(u,u,sigma) = I - sigma u u^H

    this is a special elementary matrix which is Hermitian, unitary, and has
    a determinant of -1.

**** Properties

     Householder matricies are unitary, which is nice. As they are also
     symmetric they are their own inverses.

***** Theorem 5 : x /= y, exists H s.t. Hx = y

****** Statement

       For two distinct vectors x /= y such that norm(x,2) = norm(y,2) then
       there exists some householder matrix H such that Hx = y.

****** Proof

       *Geometrical property of the Householder matrix* : some plane S exists
       such that x and y are symmetric with respect to S.

       Say we have y and x vectors. Then y - x is normal to the plane S (if y
       is dimension n, then S is dimension n+1). Therefore we may construct the
       symmetric plane, and any normal vector of S must be parallel to x - y

       set w = 1/(norm(x-y,2)) (x - y), so
           H = I - 2/(norm(x-y,2)^2) (x - y) (x^T - y^T)

       This should be H(w). Then Hx = mess = y. Done!

*** How do we choose some t such that H(u) (1/t) x = e^1?

    Pick t such that t^2 = norm(x,2)^2. Then we meet the conditions for
    Theorem 5. This is all we need mathematically, but not computationally.

    To avoid division by something close to zero (which we got by subtracting
    close numbers), pick the t with the same sign as -x(1).
** QR Factorization

*** Definition

    A is m x n (need not be square) and A = QR, where Q is m x r matrix with
    orthonormal columns and R is upper triangular r x n matrix with rank r.

*** Why?

    It is the standard orthogonalization technique for many modern applications
    like Krylov methods and eigenvalue-finding methods.

*** Existence (for full rank)

    Let A be an m x n matrix. If rank(A) = n then there exist Q (m x n) and R
    (n x n) matricies such that

    1. A = QR

    2. Q has orthonormal columns

    3. R is upper triangular.

**** Proof

     Since A is full rank, B = A^H A must be HPD and has the Cholesky
     factorization

     A^H A = R^H R

     where R is nonsingular, upper triangular, n x n. Let Q = AR^-1. Then A =
     QR where

     Q^H Q = I

     so the columns of Q are orthonormal.

*** Existence (for *any* rank)

    Assume that rank(A) = r. Let A0 = [a1^0, a2^0, ..., an^0]. Let a_j1^0 be
    the first nonzero column of A.

    Construct an m x m householder matrix H1 such that

    H1 aj1^0 = t1 e^1, for t1^2 = norm(aj1^0,2)^2

    and let

    A1 = H1 A0 = [a1^1, a2^1, ..., an^1] where a_j1^1 = (t1, 0, 0, ..., 0)^T.

    Let a_j2^1 be the first column of A1 such that a~j2^1 /= 0, where

    a~j2^1 = [a2,j2^1 , aj3^1, ...] (the tail of aj2 is nonzero). Clearly
    j1 < j2. Then use a~j2^1 to form H~2 with the same properties as before
    (but it is 1 dimension smaller than H1). Let H2 = (1, [0 ]; [0 ],
    H~2). THerefore A2 = H2 A1 = H2 H1 A0.

    Therefore we can keep going and form some upper right triangular matrix
    A^(r) = Hr Hr-1 Hr-2 ... H2 H1 A0. This generates two cases.
**** Case 1 : all entries below some row r~ .LEQ. m - 1 are zero
     Then A^r~ = some upper right matrix above some matrix of zeros. Chop off
     the bit on the botton and call it R. Call the product

     H1 H2 ... Hr~ = Q~

     so A = Q~ A^(r~) = [Q, P] * [R; 0] = QR woohoo
**** Case 2 : r~ = m - 1 and mth row nonzero
     Then we have rank(A^r~) .GEQ. m. Therefore rank(A) = m (see slide 50).
*** Upper Hessenberg or tridiagonal form
**** Definition
     A is *upper hessenberg* provided a[i][j] = 0 for i > j + 1.
**** Existence
     _Theorem 7_: for any matrix A, there exists a unitary matrix U and an
     upper Hessenberg matrix H such that H = UAU^H. In particular, H is
     tridiagonal when A is hermitian.
***** Proof
      Let A1 := A. Then, from A_k, we may get

      A_k = [B_k, C_k;
             D_k, E_k]

      such that B_k is of size k x k. Assume that D_k has zeros everywhere
      except for its kth column. We may then construct the Householder matrix
      H~k such that

      H~k d= t e^1, dim(e^1) = n - k.
*** QR method with Origin Shifts
**** Overview
     1. Choose M and A0 to be the upper Hessenberg matrix generated from A with a
       unitary transformation.

       2. for k = 0 .. M do the following:

          u_k chosen;
          find QR factorization of A_k - u_k I and QR factorize it.
          form A_k+1 := R_k Q_k + u_k I.

       Note that every matrix A_i is similar to the previous A_i; therefore they
       are all similar and must contain the same eigenvalues. We really want A_i
       to converge to a diagonal (or upper triangular) matrix.
**** How do we choose u_k?
     The algorithm does not care about u_k : features are preserved regardless
     of choice.
**** Convergence
     Qk~ := Q0 * Q1 * ... * Qk
     Rk~ := Rk * Rk-1 * ... * R0

     Then Ak+1 = Q^H_k A_k Q_k
     and we may expand the Qs as far as we like. Then, as Q is unitary,

     Ak+1 - uk I = Q~^Hk (A - uk * I) Q~k

     Claim : Q~kR~k = (A - uk * I)(A - uk-1 * I)(A - uk-2 * I) * ...

     Proof : true for 0 case, and show that is works by induction. This means
     that we are using some variation of the power method.
**** Theorem 9
     Suppose that A is nonsingular and that A has n distinct eigenvalues (that
     is, n unique eigenvalues) such that abs(lambda_1) > abs(lambda_2) >
     ... > 0.

     Also assume that the matrix Y = X^-1 has the LU factorization, where X is
     the matrix of eigenvectors. Then the matrix sequence in the QR method will
     converge nicely.
**** How do we choose the optimal parameter?
     pick the bottom right corner of the matrix. This plays nicely with the
     Rayleigh quotient.
** Givens Matrix
*** Overview
    The famous 2D rotation matrix.

                 G(t) = [cos t,  sin t;
                         -sin t, cos t]

    This is a standard item in analytical geometry. How about higher dimension
    analogs?
*** Givens Matrix - Definition
    for k .LEQ. I, we have
    G(k, l, theta) = l + s(e^k (e^l)^T - e^l (e^k)^T)
                   + (c - 1)(e^k (e^k)^T + e^l (e^l)^T)

    simpler way:
    G(k, l, theta)_i,i = 1 if i /= k or l
                       = c if i = k or i = l.
**** Properties
     Feature-rich transformation!

     1. Orthogonal matrix.

     2. (vector properties) y = G(k,l,theta)x then y_i = x_i for i /= k, l

        y_k = c * x_k + s * x_l, l_l = -s*x_k + c * x_l.

     3. if A~ = G(k,l,theta) A then we manipulate two rows or two columns.
**** Constructing Givens Matricies
     Given x, k, l there exists a Givens matrix G such that y = Gx is such that

     y_i = x_i for i /= k, i /= l
     y_k = sqrt(x_k^2 + x_l^2), y_l = 0.
***** Proof
      if x_k = x_l = 0 then G = G(k,l,0). Therefore G is the identity matrix.

      Otherwise,

      c = cos(theta) = x_k / (sqrt(x_k^2 + x_l^2))
      and s = sin(theta) = x_l / (sqrt(x_k^2 + x_l^2))

      works by the pythagorean theorem.
*** Using Givens to form QR
    In particular - if A is upper Hessenberg, then we may construct n - 1 Givens
    matricies such that

    G(n-1, n, theta_n-1) G(n-2, n-1, theta_n-2) ... A = R.

    We can use the first Givens matrix to change the first two rows - the first
    one is what we want for R, the second one is something else. We can specify
    what we want based on a previous theorem (as we know what the shape of A
    is). Therefore the product of the G^Hs in opposite order is Q.
