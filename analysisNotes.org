* 9/12/11
  Can I type faster than I write by hand? We will find out.

** Review of the last homework problem from last time
*** Prove E' = ( E U E')'
    easy version:
    in general we have that (A U B)' = A' U B'
    if x is a limit point of A, it is a limit point of the union.

    Therefore the union of the limit points (A' U B') is a subset of (A U B)'

    harder version:
    if x in (A U B)' then every neighborhood Nr(x) containes some point of A U B distinct
    from x. If we can always take y in A, then

    x in A' -> x in A' U B' .

    suppose that

    exists Nr(x) such that (Nr(x) \ {x} ) intersect A = nullset

    then forall r > 0, we must have (Nr(x) \ {x} ) intersect B /= nullset
    therefore x in B'

    To be in A', we need the neighborhood. Some rs, point in A; others, point in B.

    Application : (E U E')' = E' U E'' = E' U E' = E'

    Part a

    Suppose that x in (E')'. To show: x in E'

    By definition, (write definitions and follow your nose), forall r > 0

    (Nr(x) \ {x} ) intersect E' /= nullset

    choose y in (Nr(x) \ {x} ) \{x} intersect E'

    choose r' > 0 s.t. Nr'(y) subset Nr(x) \ {x}

    so y in E' so there exists z in Nr'(y) with z in E, z /= y.

    Therefore z in Nr(x) \ {x}, which is true for r > 0 -> x in E'.


*** Number 8
    E subset R^2, E open -> E subset E'

    E open -> exists r > 0 with Nr(x) subset E

    for x in E subset R^2, Nr(x) infinite -> for r small enough,
    (Nr(x) \ {x}) intersect E /= nullset

*** Actual lecture connectedness
    for subsets A and B of some metric space X, A and B are separated if

    A intersect (closure B) = nullset and (closure A) intersect B = nullset

**** Example:
     A = (0,1) and B = (1,2) not separated
     A set E subset X is connected if E is not disconnected
    E is disconnected -> E has a disconnection -> E = A U B and A,B separated.

    Equivalently - E is disconnected if E = A (disjoint union) B (write it as
    such, union of two sets minus the intersection)
    where A and B are both (E-open OR E-closed OR A is both E-open and E-closed)

*** Now lets go to R^1.

    Theorem 2.42 - A nonempty subset E subset R1 is connected iff E has the following properties:
    (for a < b)

    1. x in E, y in E, x < z < y -> z in E
    2. E is an interval, i.e. E = one of (-inf, b) (-inf, b], [a,b], (a,b], [a,b), R, etc
3. ???

**** Proof
    Let a = inf E
    Let b = sup E
    proceed to show that one of the cases of (2) is valid.

***** Connectedness proof

      negate both conditions - prove that E in R disconnected iff not 2.42

      (trick - set defined by a property. Negate step by step to show x `notin` A)

      not [(forall x, y in E with x < y) (and forall z in R with x < z < y), then z in E]

      Therefore we need to produce a z such that x < z < y but z `notin` E

      Set Az = E intersect (-inf, z), Bz = E intersect (z, +inf)

      for x in Az, Az /= nullset
      for y in Bz, Bz /= nullset

      z notin E, Az union z union Bz = R

      Note that Az intersect (closure Bz) = nullset, and (closure A) intersect Bz = nullset
      so E is disconnected (as z `notin` E)

      (other direction) assume E disconnected, so E = A (disjoint union) B with A, B
      nonempty and separated. Pick x in A, y in B : WLOG x < y.

      (we will use completeness of the reals)

      define z = sup (A intersect [x,y])
      then z in (closure A)
         (by a previous result, sup f in (closure F) for F nonempty, bounded above, subset of R)

    so z `notin` B as the sets are separated.
    so x \leq z < y

    if z notin A then must have x < z < y (strictness) and we are done.

    If z in A, then z notin (closure B) implies (exists z_1 with z < z_1 < y) and z_1 notin B
       Then x < z_1 < y and z_1 notin B. As z_1 > x, z_1 notin A so z_1 notin E and we are done.


* 9/14/11
** Hints for hw5
   G open, V1 open -> G intersect V1 /= nullset

   G1 open -> V1 intersect G1 = new (??) open

   Choose V2 open, (closure V2) subset V1 intersect G1.

   G2 dense -> G2 intersect V2 /= nullset (it is a subset of G1)

   #17

   Use Baire's theorem
   for some E1, take the real number line and throw out everything that is not
   of the form 0.4... and 0.7... and use uniqueness of decimal expansions (keep
   it closed at each set)
   Hard part - invent suitable notation for dealing with it. The intersection
   should be nonempty, but nowhere dense, but perfect, any point in the
   intersection (neighborhood around it) for fine enough the

   property - always have endpoints (like Cantor) so we have
   perfect/uncountable (strings of 4s and 7s is the same as strings of 1s and
   0s, which is uncountable.)

   # 18
   There exists a perfect set with no rationals (answer! it does exist.)
   so rationals Q intersect [0,1] is countable (as Q is countable)

   call this list r_1, r_2, ...

   create an interval around each and remove them, but leave some space between
   each (irrationals between rationals)


** Upcoming Dates
   HW5 due on Monday (extra credit as well)
   HW6 due on Friday (connectedness)

** Definition : K in X is compact means:
   given any open cover {G_alpha : alpha in A} of K (so K is in the union of
   all G_alpha) there is a finite subcover consisting of a finite collection of
   alphas such that K subset union over alpha of G_alpha

*** Examples:

   K = (0,1) is covered by {(0,3/4),(1/2,1)} (finite cover)

   another cover: (1/n, 1) for n in NN, has no finite subcover.

   Not hard to show not compact just from creating a cover with no finite
   subcover.

   For the homework problem {0} U {1/n}, we can use Heine-Borel, but it is more
   instructive to use the definition.

*** TODO Archimedes Corollary
    all but finitely many something something. Fill this out.

*** Nested interval theorem

    for nullset /= In = [a_n, b_n] superset Im
*** Heine-Borel

    Not closed implies not compact.
    Alternatively, we can say that if K is closed and bounded in R then it is
    compact.

*** Compactness of [0,1]

**** Classic version

     Take [0,1] - it is compact (proof : by contradiction : suppose that there
     exists an open cover of [0,1] {G_alpha} with no finite subcover. Note that
     then {G_alpha} covers [0,1/2] and [1/2,1].

     Step 1: assumption implies that {G_alpha} has a cover of at least one of
     [0,1/2] and [1/2,1] . Therefore we have a finite subcover for each, and
     their union is finite and covers [0,1]: a contradiction.

     Part 2: Pick I1 = one of the two so {G_alpha} has no finite subcover of
     I1.

     Get I1 superset I2 ... superset In - by nested interval theorem -> the
     intersection is nonempty - in fact, it must be a single point, x0, since
     the length of In -> 0 (cannot be two things in the intersection, the
     length between them always decreases)

     but x0 in G_alpha for some alpha. and I subset G_alpha0 since the length
     goes to zero. Therefore there is a finite subcover of In.

     Therefore for n large enough, we have a finite subcover of In after all; a
     contradiction.)

**** Other version - not in our book.

     How do we distinguish between [0,1] intersect Q and [0,1] ? completeness.

     Suppose {G_alpha} is an open cover of [0,1]. Our goal is to find some
     finite subcover. Define a set

     E = {x in [0,1] | {G_alpha} has a finite subcover of [0,x]}

     Check : E /= nullset. We are allowed to take x = 0 in E as 0 in G_alpha0
     (as {G_ALPHA} covers [0,1]) so [0,0] has a finite subcover.

     E is bounded above by 1, so by the Least Upper Bound property for RR,

     x0 = sup E exists, 0 leq x0 leq 1

     Claim : x0 exists and x0 = 1. Then we are done.

     x0 exists -> If x0 = 0 we are done.

         Assume x0 > 0 so x0 in [0,1] -> some G_alpha0prime in {G_alpha} such
         that G_alpha0prime open -> exists delta > 0 s.t.

         N_delta(x_0) = (x0 - delta, x0 + delta) subset of G_alpha0prime

         Then x0 - delta < x0 -> exists s in E s.t. x0 - delta leq s leq x0

         Then {G_alpha} has a finite subcover over [0,s]. Then throw in to that
         union of sets G_alpha0prime which covers [0,x0].

         In conclusion : x0 in E (used t < x0 -> t not an upper bound)

     Part 2: Suppose that x0 /= 1, so x0 < 1.

     x0 in G_alpha0prime and (x0 - s, x0 + s) in G_alpha0prime. For x0 in E
     there exists some union of Gs s.t. their union covers [0,x0] then that
     union with G_alpha0prime is a superset of [0, x + delta] -> 1 in E and 1
     is least upper bound -> {G_alpha} has a finite subcover of [0,1].




* 9/16/11
** 2.16, last problem on HW4
   p in QQ s.t. 2 < p^2 < 3
   E = (-srt(3), -sqrt(2)) U (sqrt(2), sqrt(3)) intersect Q

*** E is closed in Q
    show that E closed in RR, so closed in R intersect QQ -> QQ closed

*** E is bounded
    E is bounded as d(p,0) < sqrt(3)

*** E is not compact in QQ
    since E is not closed in RR, E is not compact in R
    so by Heine-Borel E is not compact in QQ.
**** Directly
     Gn = {(-sqrt(3) + 1/n, sqrt(3))} intersect Q
        = open cover with no finite subcover.

*** E is QQ-open:
    E = [-sqrt(3), -sqrt(2)] U [sqrt(2), sqrt(3)] intersect QQ
    so RR-open intersect QQ -> E is QQ-open

** Hint for next homework - correction
*** for 4,7 problem
    4, 7 decimal expansions
    at each step, end points go away
    when proving perfect : take point in the intersection of the layers, find a
    point in the interval (in the generalized Cantor set)
    Show that we have a fractal thing - same no matter where we start (same
    thing at every scale)
    Fun problem once understood!

*** Announcement - HW6 and Test 1, Friday September 23
    HW not too hard.
    Test - everything up to connectedness (basic topology, sets, neighborhoods,
    completeness)

** Next Chapter - Sequences in Metric Spaces
   We will start in RR and many ideas hold for general metric spaces.
   for metric space (X,d)
   some function NN -> X is called a sequence (n -> pn)
   Say {p_n} = [p_1, p_2, p_3, ...]

*** Fundamental Notion : Convergence.
    Given traditional epsilon:
    limit as n -> inf of p_n = P means
    given epsilon > 0, exists N s.t. n > N -> d(p_n, p) < epsilon

*** Example
    1/n has limit p_n = 0 (by archimedes)
    given epsilon, exists N s.t. if n > N -> 0 < 1/n < epsilon
    which implies d(1/n, 0) < epsilon for n > N

*** Elementary Facts
    Convergence: {p_n} -> P <-> every neighborhood of P contains all but
    finitely many terms {p_n}
    Uniqueness: if {p_n} -> P and {p_n} -> P' then P = P'
    Convergence: {p_n} convergent -> {p_n} bounded.
**** Proof of Uniqueness
     (show the distance between P and P' is less than epsilon, for epsilon > 0)
     We do equality here by exhaustion
     Given epsilon > 0, exists N_1 s.t. n > N_1 -> d(p_n, P) < epsilon/2
     Given epsilon > 0, exists N_2 s.t. n > N_2 -> d(p_n, P') < epsilon/2

     Then by the triangle inequality, for m > max(N_1, N_2) we get
     d(P, P') leq d(P, p_m) + d(p_m, P') < epsilon/2 + epsilon/2
                                         < epsilon
     so 0 leq d(P, P') < epsilon for all epsilon
     implies d(P, P') = 0 -> P = P' (metric space definition)
**** Proof of Convergence
     choose epsilon = 1
     so there is some N_1 s.t. d(p_n, p) < 1 for n geq N
     Let M = max(d(p_1, p), d(p_2, p) ... d(p_n-1, p), 1) < inf
     so d(p_n, p) leq M for all n -> {p_n} bounded.

*** Complex Numbers
    X = CC
    congruent to RR
    x + iy equivalent to (x,y)
    d(x + iy, x' + iy') = sqrt( (x - x')^2 + (y - y')^2)

    Theorem : {s_n}, {t_n} complex, s_n -> s, t_n -> t implies:
    1. s_n + t_n -> s + t
    2. s_n t_n -> s t
    3. For c in CC, c s_n -> c s
    4. If s_n /= 0 and s /= 0 then 1/s_n -> 1/s.
**** Proof of Laundry List
***** 1. | s_n + t_n - s - t| = | (s_n - s) - (t_n - t)|
      \leq | (s_n - s) | + |(t_n - t)| < eps
      \leq | (s_n - s) | + |(t_n - t)| < epsilon/2 + epsilon/2
      and of course individually | (s_n - s) | < epsilon/2, etc, so done
***** 2. s_n t_n -> s t
      | s_n t_n - s t | = | (s_n - s) t_n + s(t_n - t)|
      \leq | (s_n - s) | | t_n | + | s | | (t_n - t) |
      both bounded.

     Others similar.

*** RR^k
    Theorem: in RR^k, X_n = (alpha_(1, n) ... alpha_(E,n)) implies X = vector
    of converged alphas

    Reason: comprable norms. Can generally just interchange them with absolute
    value signs from RR.

    x_n dot y_n -> x dot y.
    book proof : brute force
    elegant proof : Cauchy-Schwarz

    for | xn dot yn - x dot y | =    (xn - x) dot yn + x dot (yn - y) |
                                leq | (xn - x) dot yn | + | x dot (yn - y) |
                                leq | (xn - x) | | yn | + ...
                                all bounded!



* 9/19/11
** Limits
*** Limit Review
    lim over (n -> inf) p_n = p means:
    forall epsilon > 0 exists N in NN s.t. n geq N -> d(p_n,p) < epsilon.

    this is the same as saying that every neighborhood of p contains all but
    finitely many terms of the sequence p_n.

*** Limit Properties
    {p_n} convergent to p -> {p_n} bounded.

    we proved that exists M < inf s.t. d(p_n,p) leq M for all n (bounded)

    Pick any p_o in X. then exists M' s.t. d(p_n, p_o) leq M': (use the
    triangle inequality) Since the distance

    d(p_n, p_o) leq d(p_n, p) + d(p_o, p)
                leq M + d(p, p_o) = M'

*** Connection between sequential limits and limit points
    If E subset X, p in E -> {p_n} subset E, lim p_n = p.
**** Limit Points and Limits
     Suppose that {p_n} is a sequence, lim p_n = p.

     set E = Ran {p_n} = {p_1, p_2, ...} subset X

     is p in E' ?
     Suppose that p_n = p for all n. Then lim p_n = p but p notin (Ran {p_n})'

     If Ran {p_n} is infinite and lim p_n = p, then p in (Ran {p_n})'.

     (limit points and limits of sequences do not always line up in an
     intuitive manner)
*** Subsequences of {p_n}
    Suppose the subsequences of {p_n} subset X, a metric space. Consider some
    sequence k -> n_k of positive integers (so a function :: NN -> NN) which
    is strictly increasing. : n_1 < n_2 < ... etc. Thus necessarily n_k geq k.

    Perhaps lim (n -> inf) p_n does not exist, but lim (k -> inf) p_(n_k) does
    exist for some subsequences {p_(n_k)}.
**** Example
     x_n = (-1)^n : fails for n = 1,2,3... but converges to 1 for n = 2,4,6,...
     (a good example of bounded, but not convergent)
**** Fact : {p_n} -> p iff every subsequence converges to p
***** Proof (<-) trivial. Every subsequences converges means that p_n -> p.

      (->) need some definitions. p_n -> p means that we can use the epsilon
      definition. So pick some {p_(n_k)}, any subsequence.

      k geq N -> n_k geq N, so d(p_(n_k), p) < epsilon
      which is equivalent to lim (k -> inf) p_(n_k) = p.

***** Usefulness
      This is a useful theorem. Why:

      Theorem 3.6 - if {p_n} some sequence in a compact set (in a metric space)
      then there exists some {p_(n_k)} that converges to some p in X.

****** Proof
       Let E = range {p_n}, if E is finite, we can choose a constant subsequence
       and therefore it is convergent.

       Suppose E is infinite. X compact -> E' /= nullset (earlier theorem).
       Pick some x in E'.  Construct {p_(n_k)} subset E s.t. lim (k -> inf)
       {p_(n_k)} = x.

       step 1 : choose n_1 s.t. p_(n_1) /= x and d(p_n, x) leq 1. Choose each n
       s.t. n_1 < n_2 < ... where

       d({p_(n_k)}, p) < 1/k . Find n_(k+1) > n_k s.t.
       d({p_(n_(k + 1))}, p) < 1 / (k+1).

       This is possible since there are infinitely many p_ns in N_(1/(k+1))(p).

       Now choose some K(epsilon) s.t. 1/K(epsilon) < epsilon (by archimedes)

*** TODO Every bounded sequence in RR^k contains a convergent subsequence.
       so {x_n} bounded -> {x_n} subset of a k-cell I, which is compact by
       Heine-Borel. Rest of proof hard to read

** Theorem 3.9

   Given some {p_n}, (if some function n = p_n) then it is a subset of a metric
   space X.

   Let E* = {p in X : exists subsequence {p_(n_k)} s.t. lim (k -> inf) p_n_k ->
   p} (sequential limit set)

   Then E* is closed. This is similar to the (E')' subset E'.
*** Proof (different from book version)
    suppose q is the limit point of (E*)' (to show: q in E*).

    (i.e. we must cook up some n_1 < n_2 < ... s.t. lim (k -> inf) p_n_k = q)

    step 1: given q, there is some p1* in E* s.t. d(p1*, q) < 1. Therefore as
    p1* in E* we can say that p1* = lim (i -> inf) p_(n_i), where {p_(n_i)} is
    some subsequence of {p_n}. Then

    exists k_1 in {n_1, n_2, ...} s.t. d(p^(1)_(k_1), p1*) < 1 - d(p1*, q) > 0

    this gives us p_(k_1) = p^(1)_(k_1).

    Note that d(p^(1)_(k), q) leq d(p^(1)_(k_1) p1*) + d(p1*, q) < 1.

    INDUCTIVE STEP - assume that we have k_1 < k_2 < ... < k_n with distance
    d(p_(k_j), q) < 1/j. Choose k_(n+1) as follows:

    choose p*_(n+1) in E* with d(p^*_(n+1), q) < 1 / (n + 1)

    then p*_(n+1) in E* -> p*_(n+1) = lim (k -> inf) p^(n+1)_k

    so {p^(n + 1)_k} subset {p_n}

    choose p_n_(k + 1) in {p^(n + 1)_k}, n_(k + 1) > n_k.

    so d(p_n_(k+1), p*_(n+1)) < 1/(n+1) - d(p^*_(n+1), q) ->


* 9/21/11
** Review of homework 5
*** Problem 17
    Uncountable - show that it is perfect or use Cantor diagonalization.
    (as nonempty perfect -> uncountable)

    Dense : contains no intervals, so 'nowhere dense'

    compact : Yes, by decimal expansions.

    x in Ec -> (x - eps, x + eps) subset Ec

**** Countable

     E = infinite intersection of En, where
     En = disjoint union of 2^n closed intervals, length 1/10^n

     Yet another way : x = 0.alpha1 alpha2 alpha3 etc, 4 or 7.  Pick n
     s.t. 1/10^n < delta.

     Let x' = 0. phi1 phi2 etc where if k /= n then phi_k = alpha_k else flip to
     other digit.

     Then x' in E, x' /= x, so uncountable.

*** Problem 18
    Is there a nonempty perfect set with no rationals? Yes!

    we went over a sketch in class - here is Sara's version.

    Know that QQ is countable, = {r1, r2, r3, ...} and use 'a touch of measure
    theory'

    Let epsilon > 0, let In = (r_n - epsilon/(2*2^n), r_n + epsilon/(2*2^n))
    and use these to 'cover up' intervals.

    so the sum of all In is epsilon.

    E = RR \ union of In - uncountable, closed (uncountable because the
    measure, epsilon, is positive)

    (now show that it is perfect - no isolated points)
    so delete countably many isolated points (number of isolated points must be
    countable because each has some interval around it; injection of isolated
    points in to rationals) so we still have an uncountable set and i is
    perfect.

*** Problem on Baire's Theorem
    Baire's Theorem - if Fn closed and nowhere dense subset of RR^k then

                                union Fn /= RR^k.

    Important to have RR^k - if we used QQ we could write QQ as a countable
    union of sets (each set has exactly one point of QQ).

    Therefore we expect our proof of a special case to use some special feature
    of RR^k to work (completeness).

** Extra Credit Answers

   Was fun. Let (X,d) be a metric space then the following are equivalent:

   1. every open cover has a finite subcover (definition of compactness)

   2. Every collection of closed sets {F_alpha : alpha in A}  with the FIP has
      the property intersection of F_alpha /= nullset

   3. Every infinite subset has a limit point.

   4. (next chapter!) every sequence has a convergent subsequence.

   Things related to these may be on the test.

*** harder part: 3 -> 1

    break up in to three pieces.

    X separable -> X has a countable base (#23)

    V_alpha, alpha in A

    base means that for each G open, x in G, there is a V_alpha with x in
    V_alpha (so G is the union of V_alpha s)

**** Solution
     V = { N_q(s) : s in S, q in QQ+} (S is the separable set)
     V is indexed by S x QQ+, so it is countable.

     triangle inequality 'tap dance' follows

     Let x in G, G open. Choose r in QQ+, N_r(x) subset G.
     Choose s in S with s in N_(r/2)(x). Then x in N_(r/2)(x) and y in the same
     neighborhood implies thad d(y,x) leq d(y,s) + d(s,x) < r

     so x in N_(r/2)(s) subset G : this gives us our V!

*** Number 24
    X - metric space such that every infinite subset has a limit point -> X is
    separable.

**** Proof
     Fix delta > 0. Pick x1 in X.

     having chosen x1, ... , xj in X choose (if possible) x_j+1 in X

     with d(x_j+1, x_k) geq delta for k = 1 .. j. Then: if this is always
     possible we get an infinite set s.t. E = {x1, x2, ...} s.t. d(x, x') geq
     delta for x, x' in E, x /= E'.

     If p = limit point of E, then the distance d(x,x') leq d(x,p) + d(p,x')
     leq delta for any distinct x, x' in E intersect N(p) -> a
     contradiction. Therefore this stops as X subset N(x_j^delta)

     so for each delta, we repeat this and get finitely many per delta.

     This notation is really bad. Can we improve it?

     set x^(n)_j = x_j^(1/n) for n in NN delta = 1 / n

     then let S = {x_j^(n) : j = 1 .. K(1/n), n in NN} which is a countable
     set. Details : more triangle inequality.

*** Number 26
    Let X be a metric space where every infinite subset has a limit
    point. Prove that X is compact.

**** Proof

     #24 -> X is separable.
     #23 -> X has a countable base.

     Let G_alpha, alpha in A be any open cover of X. Then:
     1. {G_alpha} has a countable subcover (as we have a countable base) How
        does this work (namely, countable base -> countable subcover)? Each x
        in X is in some G_alpha, so there exists some V_n(x, alpha) in the
        countable base with x in V_n(x, alpha) and V_n(x, alpha) subset
        G_alpha.

        Define n -> alpha(n) by G_alpha(n) as any choice of G_alpha where V_n
        subset G_alpha(n) (if no such n, leave alpha(n) undefined)

        Then what we did above implies that X subset {G_alpha(n), n in NN} so a
        subcover.

        If {G_alpha(n)} has no finite subcover then

        F_n = (union G_alpha(j))^c /= nullset (cannot cover everything with
        finite set)

        and the intersection of all F_ns is the nullset.

        By hypothesis, every set has a limit point, so x in E' exists, so x is
        in the intersection of all F_n -> a contradiction.

** Test mechanics
   mostly definitions, things we have seen before (easier than the extra
   credit)

   Do all the homework, get experience, know the 'lay of the land' of analysis.


* Test 1 news reverse bell curve: 5 below 80, 5 above 90, 2 inbetween.


* Review of connectedness HW
** Problem 20
   pick the 'eyeglasses' in RR^2 - circle at -1, circle at 1:
   connected, but E^' disconnected.
   the definition of connected is too hard to work with - use disconnected.
**** Show that (closure E) disconnected -> E disconnected.
     Say that closure E has a separation, or
     (closure E) = A disjoint union B

     so (closure A) intersect B = nullset.
     so A intersect (closure B) = nullset.

     THen show that E = (A intersect E) disjoint union (B intersect E) is a
     disconnection of E.

     The nontrivial part is to show A intersect E /= nullset.
** Problem 19
**** Show A, B closed , A intersect B = nullset -> (closure A) intersect B = nullset
     (and that (closure B) intersect A = nullset)

     make it easier - E disconnected <-> E is a disjoint union of A and B.
     Therefore for A and B disjoint, A = E (where E is open and E is closed)
**** Show that A = (fix p) {q | d(p,q) < delta } = open on X.
     and that B = { q | d(p,q) > delta} open on X.

     E = disjoint union of A and B -> E disconnected.

     A and B clearly disjoint.
**** X has two points (at least), X disconnected -> X uncountable.
     For each alpha, 0 < alpha < d(p,q), if there is no q_alpha s.t.

     d(p,q_alpha) = alpha -> X disconnected (we know that)

     Therefore for each alpha choose some q_alpha in X with s.t. d(p,q_alpha) =
     alpha.

     (show that q_alpha is one-to-one) cannot have two values for distance.
     we can do a similar trick to show that q_alpha works for
     alpha in some [0, d(p,q)] - an uncountable interval.

* Limits

** Definition

   lim (n -> inf) p_n = p implies :

   forall epsilon > 0, exists N(epsilon) in NN s.t.
   if n .GEQ. N then d(p_n, p) < epsilon.
   lim s_n = s means : ABS(s_n - s) < epsilon

   Limits do not always exist! Take X = RR. We have seen that cauchy ->
   bounded, but the converse fails in general (remember (-1)^n, nice and
   bounded). However, bounded and 'something else' -> cauchy -> convergence (as
   RR is complete).

   This something else is 'monotonic'.

** Monotone Convergence Theorem

   Let {x_n} subset RR. Then if x_n leq x_(n+1) for all n in NN, we say it is
   monotonically increasing. If {x_n} is bounded as well then it is
   convergent.

*** Proof, for RR
    Our candidate for the limit is the sup. Now we just show that this is the
    case.
    Given epsilon > 0, there is N such that n geq N implies | s_n - s | <
    epsilon, so

    -epsilon < s_n - s < epsilon

    by definition of the sup, sup - epsilon is not an upper bound, so

    for some N in NN, s - epsilon < s_NN.

    For n geq N, s - epsilon < s_N leq s_n leq s < s + epsilon Done!

    (this is valid by completeness of RR)


* Lim Sup and Lim Inf

** Why do we need it?

   Given {s_n}, lim s_n may not exist. That would be a problem.

   Fix it by: lim sup s_n and over{RR} = RR U {inf, -inf}
   Don't subtract inf and -inf, or multiply them by zero. Instead 'throw up
   your hands' (this isn't really a field)

   Expand the definition of lim (n -> inf) s_n to include inf and -inf.

   lim (n -> inf) = inf means that given M < inf there exists N in NN such
   that n geq N -> M < s_n.

   lim (n -> inf) = -inf means that given M in RR then N in NN exists such

** Proof of b

   We don't really need to prove this. There is a subsequence that goes to
   s*.

   Uniqueness : two distinct such numbers p and q satisfying (a,b) then say x
   (say that p < x < q) then all s_n with n geq N have s_n < x and infinitely
   many s_ns including some bigger than N have s_n > x -> contradiction. This
   determines s* uniquely.

   that n .GEQ. N implies s_n < -M.

** Another Theorem
   Lim inf {s_n} = inf E({s_n}) = lim (n -> inf) infimum (k geq n) {s_k}
                                          sup over n inf (k geq n) {s_k}

*** Proof by 'erasure' - same as before.

    exists some N(epsilon) such that for all n past a certain point,
    s_n > s* - epsilon for n geq N.

*** Examples

    s_n = (-1)^n n -> the subsequential limit set is inf, -inf
    therefore lim sup is inf, lim inf is -inf

    s_n = r_n where r_1,r_2 is an enumeration of QQ
    so E({s_n}) = bar(RR) (we can get anything we please out of it.)

    s_n = (-1)^n / (1 + 1/n)

    therefore E({s_n}) = {1, -1} (we can approach both.)

    How about something more interesting - recursive formula

    #4 on page 78 - good luck.

** Relation with lim s_n = s

   means that forall epsilon > 0, exists N(epsilon) s.t. n geq N(epsilon)
   implies s - epsilon < s_n < s + epsilon.

** Advanced Calculus Theorem, 4225 equivalent

   LIM t_n exists, equals t, s_n .LEQ. t_n -> LIM s_n .LEQ. t.

   better formulation - for s_n .LEQ. t_n forall n geq N (some N) then

   a. LIM inf s_n .LEQ. LIM inf t_n

   b. LIM sup s_n .LEQ. LIM sup t_n.

*** Proof
    (b) via lim sup of s_n = sup ({s_n})
    let x in E({s_n}) so x = lim s_n_k

    by the limit theorem : x leq lim t_n_k exists.
    pick some {t_n_j_k} subset {t_n_k} with
    t_n_j_k exists and the limit is t. So this is a subsequence of t_n, and
    therefore t in E({s_n}), t leq lim sup of t_n.

    but also : lim s_n_j_k = s.

    also know that s_n_k_j leq t_n_k_j, so by the advanced calculus theorem,
    we have that x leq t leq lim sup of t_n forall x in E({s_n}) that
    lim sup s_n leq lim sup t_n


* Sequences

** Convergence

   forall epsilon > 0, if n .GEQ. N then d(p_n, p) < epsilon
   (means {p_n} converges to p)

** Convergence Theorem (3.2)

   Let {p_n} be a sequence in a metric space X.

   1. {p_n} converges to p iff every neighborhood of p contains all but
      finitely many p_n.

   2. p in X, p' in X, {p_n} -> p, {p_n} -> p' then p = p'.

   3. {p_n} convergent then {p_n} bounded.

   4. E `subSet` X and p in E' then there is a sequence {p_n} in E s.t. p = LIM
      {p_n}.

** Subsequences

   Function k in NN, n_k in NN
   with n_1 < n_2 < ... < n_k_1 < ...
   subsequences still converge to the original limit.

*** Theorem 3.6

    1. if {p_n} is a sequence in a compact metric space X then some subsequence
       of {p_n} converges to a point of X.

    2. Every bounded sequence in RR^k contains a convergent subsequence.

** Theorem 3.7 - sequence {p_n} in metric space X

   Let E = subsequential limit set, or
   E = { q | exists {n_k} s.t. q = lim (k -> inf) p_(n_k) }
   then E is closed (limit points of subsequential limits themselves
   subsequential limits)

*** Proof

    Two-step tango - use triangle inequality. In other news, Cameron's parents
    are professional ballroom dancers.

** Cauchy Sequences

   A sequence {p_n} in a metric space X is _cauchy_ <-> given epsilon > 0,
   exists N(epsilon) in NN s.t.

   m,n .GEQ. N -> d(p_n, p_m) < epsilon

   (the terms get close to each other)

*** Theorem 3.11 : {p_n} convergent -> {p_n} cauchy

    for some compact metric space the converse is also true.
    X subset RR^k also implies the converse.

    suggests a definition : X is _complete_ if its cauchy sequences converge.

**** Proof 2

     Lemma required - show that if {p_n_k} -> p then {p_n} -> p (#20)

     Variation on #20:
     Suppose that {p_n} is a sequence and contained in the metric space. Every
     subsequence {p_n_k} has, in turn, a subsequence {p_n_k_j} with limit as (j
     -> inf) p_n_k_j = p (limit independent of choice of subsequence). Then as a
     subsequence converges the whole sequence converges.

     This is used in the presence of compactness (we know that any sequence has
     a convergent subsequence). If we can show that two subsequences converge to
     the same thing we are okay.

     Therefore, for reformulation, use the contrapositive:

     not (lim p_n = p) implies exists {p_n_k} such that no subsequence {p_n_k_j}
     can converge to p.

     (therefore cook up a subsequence such that all subsequences cannot converge
     to p)

     a.k.a find p_n_k s.t. d(p_n_k,p) geq epsilon (all terms stay away from p,
     so subsequences cannot converge to p)

*** Corollary to 3.11

    X is compact, or X = RR^k -> X complete.
    (however the reverse is not true. Complete does not imply compact.)

**** Proof

     part one - easy - use a delta epsilon arguement.
     Say lim (n -> inf) p_n = p, so we know the usual things about epsilon and
     N(epsilon).
     then for n, m geq N(epsilon/2) we get
     d(p_n,p_m) leq d(p, p_n) + d(p, p_m) leq epsilon/2 + epsilon/2 leq epsilon

     part two - {p_n} cauchy -> {p_n} convergent

     {p_n} cauchy -> {p_n} bounded. Choose N = N(1). Then n geq N ->
     d(p_m, p_N) < 1. Let M = max(d(p_1,p_N), d(p_2, p_N), ... , d(p_N-1, p_N))

     Specialize : pick X = RR^k. By Heine-Borel we get that closed k-cells are
     compact. Therefore for p_n bounded -> {p_n} subset J = closed k-cell in
     RR^k. By Heine-Borel J is compact. Therefore this implies that {p_n} has a
     convergent subsequence with a limit in J.

     The guts chunk (X = RR^k : {p_n} cauchy -> {p_n} convergent)

     Say that {p_n} is cauchy in a compact metric space X.
     X compact -> exists {p_n_k} subsequence with limit p, p in X.
     Claim : lim p_n = p.
     (show that if the subsequence coverges to p, everything converges to p)
     (different from Rudin : Rudin reproves that 2 -> 3 from earlier. Not
     optimal.)
     (we will use 4 - every sequence has a convergent subsequence.)

     Let epsilon > 0 . Then choose N_1 s.t. k geq N_1 -> d(p_n_k, p) <
     epsilon/2.

     Similarly, choose N_2 s.t. n, m geq N_2 -> d(p_n, p_m) < epsilon/2.

     Suppose that n > N_2. Then choose k_0 s.t. n_k_0 >  N_2 and k0 > N1

     then d(p_n, p) leq d(p_n, p_k0) + d(p_nk0, p)
                    =   epsilon/2 + epsilon/2

** special sequences

   p > 0 -> lim 1/n^p = 0
   p > 0 -> lim nth root of p = 1
   lim nth root of n = 1
   p > 0, alpha real -> lim n^alpha / (1 + p)^n = 0
   abs(x) < 1 -> lim x^n = 0

*** Proofs

    1. take the logarithm:
       ln n^(-p) = -p ln n -> - inf
       which implies the limit is exp(-inf) = 0
    2. again, take the logarithm:
       ln nth root of p = 1/n ln p -> 0 so exp(0) = 1.
    3. ln n root n = ln n / n approx 1/n / 1 -> 0, so we get exp(0) = 1.
    4. n^alpha = exp(alpha ln n)
       goes to 0 for alpha < 0
       goes to 1 for alpha = 0
       goes to inf for alpha > 0
    5. abs(x^n) = abs(x)^n suffices to assume 0 < x < 1
       ln x^n = n ln x -> - inf

*** We are agnostics and don't know how to do any of that.
    What do we know? Combinatorics, binomial theorem, not calculus.

*** Proving stuff with just the binomial theorem
    3224 proof, epsilons and Ns

    1. take n > (1/epsilon)^(1/p). Then 1/n^p < epsilon
       for n > N(epsilon) = (1/epsilon)^(1/p)
    2. If p > 1, let x_n = (nth root of p) - 1. Then
       (binomial theorem) 1 + n * x_n leq (1 + x_n)^n = p
       (keep only the first two terms: this part is an art.)
       Therefore 0 < x_n leq (p - 1)/n
       by the squeeze theorem x_n must go to zero, as it is bounded by
       sequences that converge to zero.

       some more cases:
       p = 1 : trivial (everything zero)
       p < 1 : take the reciprocals.
       (nth root of p) = 1 / (nth root of 1/p) and 1/p > 1.

    3. Put x_n = (nth root of n) - 1 geq 0
       so again by the binomial theorem we have
       n = (1 + x_n)^n geq n(n-1)/2 x_n^2 geq (n-1)x_n^2 valid for n geq 2.

       a clearer way : 2n geq n(n-1)x_n^2 or 2/(n-1) geq x_n^2

       (by 1. x_n leq (sqrt 2 / (n-1)), which goes to zero)

       therefore we are squeezed again as x_n leq something that goes to zero


    4. p > 0 and alpha real : pick integer k, k > alpha, k > 0. For n > 2k,

       (1 + p)^n > (n k)p^k

       where (n k) is permutations (n * (n-1) * (n - 2) ... (n - k + 1))/k!

       so (n k) p^k > (n^k/2^k) * (p^k / k!)
       so if we take reciprocals of everything :
       0 < n^alpha / (1 + p)^n < (2^k k!) / (p^k)  * (n^(alpha - k))
       where the RHS is some constant times a decreasing sequence (n^(alpha
       -k))

       therefore by the squeeze theorem we get it.

    5. Also binomial theorem!

* Compact Metric Spaces

** Useful TFAE Theorem
   if X is a metric space, then the following are equivalent:
   1. X is compact.
   2. F_n closed then {F_n} has the finite intersection property (intersection
      of all nonempty)
   3. Every countably infinite subset of X has a limit point in X.
   4. Every sequence P_n contained in X has a convergent subsequence.

** Corollaries

   Infinite subsets E of RR^k have limit points (since E subset closed k-cell,
   which is compact)

   Bounded sequences have convergent subsequences.

   (these imply completeness)

** Example
   (-1)^n is a good example of lim (n -> inf) (-1)^n = 1. We can pick the
   subsequence (-1)^odd, which is never one, so all of its subsequences are
   never 1 as well.

  Any sequence {s_n} contained in bar{RR} has a convergent subsequence.
  (we no longer need boundedness, if we are in bar{RR})
  (most of our sequences are real valued, but can converge to +/- inf)

  bar{RR} is a metric space by d(s,t) = | artcan s - arctan t |
  where arctan inf  = pi / 2
        arctan -inf = -pi / 2

** Consequences
   So let {s_n} be a subsequence in RR (recall that we are allowed now to
   converge to things in bar{RR}). Then

   E = subsequential limits of {s_n} in bar{RR}

   is nonempty by the refined Bolzano-Weirstrass.

   Refine the least upper bound property for some S subset bar{RR}. in
   particular, S subset RR has a least upper bound in bar{RR}

   (the upper bound on the empty set is -inf. Weird.)

** Lim Sup
   {s_n} in RR, define bar{lim} (also known as lim sup) as

   sup E({s_n}), where E is the set of subsequential limits of its
   arguements.

   nice - this always exists.

   Homework problem - show that lim sup s_n + t_n leq lim sup s_n + lim sup
   t_n (provided that the sum is not inf - inf; assume one is finite).

   add part b : give an example where it is a strict inequality.

   Where does this notation come from? 'Supremum of the subsequential limit
   set' of this sequence.

   equivalency theorem : lim sup s_n = lim (n -> inf) (sup (k geq n) {s_k})

   also called the sequence of the tail. As n increases, the number of elements
   in {s_k} decreases, so we expect the sup to not 'blow up'.

*** Proof
    Suppose x in E = (subsequential limit set of {s_n}), so

    x = limit (j -> inf) s_n_j for some s_n_j in {s_n}.

    We will do the finite case (do the infinite case for practice).

    choose some epsilon > 0, so x - epsilon < s_n_k (n_k strictly increasing)
    for k geq K(epsilon).

    Therefore x - epsilon < sup (k geq n) s_k for all n.

    therefore this implies that for x = any element of E, x - epsilon leq inf
    (n -> inf) (sup (k geq n) s_k) which implies that

    x leq inf sup s_k

    which implies that x leq upper bound for E, so the sup of E leq inf sup
    s_k.

    next time : show inf sup in E.

*** A set of definitions
    overbar lim (m -> inf) s_n := (subsequential limit set for s_n)
                                = sup E({s_n})
                                = lim (n -> inf) (sup, k geq n) of {s_k}

   We can split them apart! Compose it with two more 'fundamental' operations
   (lim and sup)

   last time - showed that x in s_n -> x leq infimum (sup {s_k}, k geq n)
   call this the Rudin definition. Then

   overbar lim s_n leq lim (sup {s_k})

   how about the reverse? Suffices to show that

   lim (n -> inf) sup (k geq n) s_k is in E({s_n})
   (so of course it is less than or equal to the sup)

**** Supremum Side

     Say that L := lim (n -> inf) sup (k geq n) s_k

     Then this guy (sup k geq n s_k) is a sequence; call it t_n
     Choose some n_1 with L leq t_n_1 leq  L + 1, so choose k_1 > n_1 with

     L - 1 < s_k+1 leq t_n1 < L + 1

     as t_n1 was a supremum larger than L, so find something less than L.

     Then:
     L - 1/i < s_k_i < L + 1/i

     so choose some n_j+1 > k_1 with L leq t_j < L + 1/(i+1)

     we get layers of subscripts!

     L - 1/(j+1) < s_k_(j+1) leq t_j

     In this way we can get some {s_k_j} such that

     L - 1/j < S_k_j < L + 1/j implies s_k_j -> L.

**** Another way to look at the problem - 3.17 - real valued sequences

     {s_n} subset RR (could also use bar(RR))

     Let s* = lim sup of s_n. Then

     s* is a subsequential limit and we can skip to the last few steps.

     If x > s* then exists N s.t. n geq n implies s_n < x. Therefore we have a
     subsequential limit.

     Therefore s* = lim sup s_n, which means :

     a. forall epsilon, exists infinitely many ns with s* - epsilon < s_n, so
     there is a subsequence whose limit is greater than s* - epsilon, so there
     are subsequences with limits greater than s*,

     b. Exists N(epsilon) so that s_n < s* + epsilon for all n geq N(epsilon).
     No matter how far you go, you could still go farther. This tears apart
     sequences.


* Some Old Homework for Chapter 3
** Homework hints 3.3

   Monotone convergence theorem, define {s_n} by s_1 specified, s_n+1 =
   f(s_n)

   and s_n bounded and monotonically decreasing -> convergent.

** 3.4

   Recursively defined lim sup condition

   or rather, s_n defined recursively. Find the lim sup

   strategy : show that the limit of s_2m = s_even (exists)
   and limit of s_(2m-1) = s_odd (exists)
   therefore the limit of the infimum, we get to subsequential limits (s_even
   and s_odd)

   so lim s_n overbar = max (s_e, s_o)
   so lim s_n underbar = min (s_e, s_o)

** Homework hints 3.16 - Newton's method for x^2 = alpha.

   Everyone loves Newton's method!
   instead of the picture we use monotone convergence theorem. Show that the
   subsequence converges, or something.


   (if this is satisfied, then lim sup = lim inf (it is iff))

   always true : lim inf s_n = inf E ({s_n}) leq sup E({s_n}) leq lim sup.

*** sneaky proofs - showing that certain limits exist.

    to show s_n convergent (to something) it suffices to show that

    lim sup of s_n leq lim inf s_n.

    (as lim sup geq lim inf, so we have sandwiching, so the limit exists)

    this is a useful trick! We will need it.


* Series - sequence with recursion.
  Some summation that forms a sequence of partial sums.
  s_n = sum (from k = 1 to n) a_k
  so form s_n+1 = s_n + a_n+1

  challenge - knowing a_n, understand s_n.

  we say that the sum converges if lim (n -> inf) s_n in RR

** Cauchy Criterion

   The summation SUM (n from 1 to inf) a_n converges if the series of partial
   sums {s_n} is convergent, namely given epsilon > 0, exists N s.t.

               m > n .GEQ. N -> ABS(s_m - s_n) < epsilon.

   put another way, ABS(SUM (k = n+1 to m) a_k) < epsilon.

   in particular, pick some m = n + 1; then ABS(a_{n+1}) < epsilon.
   or lim (n to infinity) a_n = 0. (recall that the converse is not true.)

** Monotone convergence theorem

   Assume that s_n is monotonically increasing. so s_n+1 geq s_n for a_n geq 0.

   Then sum (from k = 1 to infinity) a_k convergent iff the partial sums are
   bounded.

** Famous Series'

   SUM (n=0 to inf) x^n

   SUM (n=1 to inf) 1/n

*** Harmonic Series
    Famous counter-example to everything.

    generalization of the 'easy' proof (Cauchy)

    Suppose that a_1 .GEQ. a_2 .GEQ. a_3 .GEQ. ... .GEQ. 0
    then SUM (n=1 to inf) a_n converges iff SUM (k=0 to inf) 2^k a_(2^k)
    converges.

    Therefore the divergence of the Harmonic series is a special case of
    Cauchy's work.

**** Proof for Cauchy
     Going backward - assume that SUM 2^k (a_(2^k)) converges.
     s_n = nth partial sum of a_n
     t_k = a_1 2a_2 + ... + 2^k a_(2^k)

     then for n < 2^k,

     s_n .LEQ. a_1 + (a_2 + a_3) + ... + (a_(2^k) + ... + a_(2^(k+1)-1))
         .LEQ. a_1 + 2*a_2 + ... + 2^k a_(2^k) = t_k

     Thus {t_k} is bounded (by assumption) so s_n is bounded.

     Going forward - assume that SUM a_n converges. Then

     s_n .GEQ. a_1 + a_2 + ...
         .GEQ. 1/2*a_1 + a_2 + 2a_4 + ... + 2^(k-1)a_(2^k)
             = t_k.

*** Application to p-series
    another sophomore calculus topic

    SUM (n = 1 to inf) 1/n^p converges for p > 1, diverges p .LEQ. 1.
    if p .LEQ. 0, nth terms 'blow up'

    Assume p > 0. Then use the integral test. We don't have integrals yet, so
    use Cauchy's thing instead.

**** Proof
     (via Cauchy)

     look at SUM (k=0 to inf) 2^k 1/(2^k)^p, like geometric series.
           = SUM (k=0 to inf) 2^((1-p)k) converted!


*** First application - geometric series.
    for 0 .LEQ. x .LEQ. 1, SUM (n = 0 to inf) x^n = 1 + x + x^2 + ...
    so s_n = 1 + x + ... + x^n
       xs_n = x + x^2 + ... + x^(n+1)

    therefore (1-x)s_n = 1 - x^(n+1)
    so if x .NEQ. 1 then s_n = (1 - x^(n+1))/(1-x)
    where if abs(x) = 1 then it diverges, as does abs(x) > 1.

    when this does converge, we get
    LIM (n from 1 to inf) = 1 / (1 - x).

** Tests

*** Divergence comparison test

    if a_n .GEQ. d_n .GEQ. - for all n > N0 and

    SUM d_n diverges then SUM a_n diverges.

    Why? s_n - s_(n-1) = SUM (from k = N_0 to m) a_k .GEQ. SUM (from N_0 to m)
    d_k, which diverges. Therefore SUM a_n diverges.

*** Alternating Series Test
    if the terms alternate in sign and the a_n goes to zero, and abs(a_n)
    monotone, then the summation converges.

*** Generalized Alternating Series Test
    Tool - summation by parts.
    Given some {a_n}, {b_n} in CC, set

    A_n : SUM (from k = 0 to n) a_k (for n < 0, a_n = 0)

    Then for 0 .LEQ. p .LEQ. q, we show that

    SUM (from n = p to q) a_n b_n =
    SUM (from n = p to q-1) A_n(b_n - b_(n+1)) + (A_q b_p - A_(p-1) b_p)

**** Aside - calculus analogue
     same as INTEGRAL u dv = - INTEGRAL v du + uv | bounds (uv at bounds)

**** Proof
     Shift the indicies in a clever fashion.

     Let a_n = A_n - A_(n-1)

     then SUM (from n = p to q) a_n b_n = SUM (from n=p to q)(A_n - A_(n-1))b_n

     = SUM (from n=p to q) A_n*b_n - SUM (n=p-1 to q-1)A_n*b_(n+1)

     = SUM (n=p to q-1) A_n (b_n - b_(n+1)) + A_qb_q - A_(p-1)b_p

**** Application to Alternating Series Test

     Suppose that A_n is bounded, and b_0 .GEQ. b_1 .GEQ b_2 .GEQ ... and that
     the limit of b_n is 0. Then SUM a_nb_n converges.

     Consequence: abs(c_1) .GEQ. abs(c_2) .GEQ. ...

     and if c_2m-1 .GEQ. 0, while c_2m .LEQ. 0, and lim c_n = 0, then SUM c_n
     converges.

***** Example
      SUM (-1)^k / k converges, though 1/k does not.

**** Telescoping Series
     for N .LEQ. p .LEQ. q we get

     abs(SUM a_nb_n) = abs(SUM (n=p to q-1) A_N (b_n - b_n+1) + A_Q b_q - A_p
     b_p)

     APPLICATION TO #8 - shuffle the hypotheses. Show that if SUM a_n converges
     then SUM a_n b converges. and SUM a_nb_n = SUM a_n b - SUM a_n b_n

     (say that b_n bounded below by b, also say b*_n = b_n - b).

*** Comparison Test
    If abs(a_n) leq b_n and sum (from n = 1 to inf) b_n converges then sum
    (from n = 1 to inf) a_n converges.

**** Proof - two-step tango
     step 1 : show that the summation of abs(a_n) converges by showing some t_n
     = partial sums of a_k is bounded.

     Note that by definition sum abs(a_k) leq sum b_k < inf (termwise, finite)
     Therefore by the monotone convergence theorem -> the summation abs(a_k)
     converges (the sequence of partial sums is monotone, each non-negative)

     step 2 : show that the summation of abs(a_k) convergent -> summation of
     a_k converges. (absolute converge -> convergence)

     Trick - use Cauchy. abs(s_n - s_m) = abs(sum (from k=n+1 to m) a_k) <
     epsilon. THen

     abs(sum (from n+1 to m) a_k) leq sum (from k=n+1 to m) abs(a_k) < epsilon
     for m > n geq N(epsilon)
     as {t_n} is Cauchy -> done.

*** Refined Root Test
    We can do better than Sophomore calculus. say that

    alpha = LIM SUP (nth-root abs(a_n)) always exists in [0, +inf]
     if alpha < 1 then converges absolutely
     if alpha > 1 then diverges (as abs(a_n_ -> inf))
     no conclusion for alpha = 1.

**** Comparison to Geomeric Series
     say that alpha < 1. Then for k, alpha < k < 1. Then by a property of LIM
     SUP, exists N s.t. n .GEQ. N -> (nth-root abs(a_n)) < k.

     Therefore for any value of n, we have that (nth-root abs(a_n)) < some beta.
     Therefore abs(a_n) < beta^n so by the comparison test abs(a_n) converges,
     so a_n converges.

     Now say that alpha > 1. Choose some subsequence {a_n_k} with (nkth-root
     abs(a_n_k)) -> alpha, where each (nkth-root abs(a_n_k)) > some beta for 1 <
     beta < alpha. Therefore as beta^nk diverges, NOT [LIM (n to inf) a_n == 0]
     implies that the summation diverges.

*** Ratio test

    Assume that a_n /= 0. Given some SUM a_n, then

    a) LIM SUP (n to inf) abs(a_(n+1)/a_n) < 1 -> SUM a_n converges absolutely.

    b) abs(a_n+1 / a_n) .GEQ. 1 -> for all n .GEQ. n0 SUM a_n diverges.

    Prove by the root test!

**** Example
     Take some 1/2 + 1/3 + 1/2^2 + 1/3^2 (we secretly know this converges)

     a_(2n + 1)/a_(2n) = (1/2)^n / (1/3)^n goes to infinity - fails ratio test.

     a_2n / a_2n-1 = 2(2/3)^n -> 0 same problem.

     Root test:

     (2nth-root abs(a_{2n})) = 1/sqrt(3) < 1.

     and similarly, the 1/2^n works. Therefore the root test gives the correct
     answer when the ratio test does not.



** Convergence

   We can also use the Cauchy Criterion!

   we say that SUM (n=1 to inf) a_n = s means

   s = LIM (n to inf) s_n (partial sums converge)

   put another way, the partial sums converge to some s. If a_n > 0 then the
   series converges iff s_n is bounded.

   Combine:
   abs(a_n) .LEQ. b_n, sum b_n converges -> SUM abs(a_n) converges.

** Euler's number
   e is the constant such that INTEGRAL (from 1 to e) 1/x dx = 1.
   (we don't know calculus yet so we cannot use this definition)

*** Our definition
    e = SUM (n = 1 to inf) 1/n! (use this to prove other things)
    convergent as each n .GEQ. 2 has 1/n! .LEQ. 1/2^{n-1}

*** Limit-based definition
    e = LIM (1 + 1/n)^n

    we can also do e^x = LIM (1 + x/n)^n

**** Sophomore proof
     LN ( (1 + 1/n)^n) = n LN (1 + 1/n) = (LN(1 + 1/n))/(1/n)
     goes to 0/0 so L'Hopital valid - can equate it to
     LIM (x to 0) ln(1 + x)/x = 1

**** Rudin Proof
     to show - SUM (1/n!) = LIM (1 + 1/n)^n = e
     let s_n = SUM (from k=0 to n) 1/k!
     technique : show that LIM SUP t_n .LEQ. e .LEQ. LIM INF t_n
     By the binomial theorem:

     t_n = SUM (k from 0 to n) (n k) 1/n^k
         = SUM (k from 0 to n) (n(n-1) ... (n - k + 1)) /
                               (k(k-1)(k-2)...(2)(1))
         = SUM (1 - 1/n)(1 - (k-1)/n) / (k!)
         .LEQ. SUM (1/k!) = s_n

     next, for n .GEQ. m,
     t_n .GEQ. SUM (k from 0 to m) ( 1 (1 - 1/n) ... (1 - (k-1)/n)) / k!

     fix m, and let n to inf:
     LIM INF t_n .GEQ. SUM (from k = 0 to m) 1/k! = s_m for all m

     therefore LIM INF .GEQ. LIM s_m = e. Done!

**** Rationality of e
     By contradiction - assume that e = p/q. Then

     0 < q! (e - s_q)
       = q! SUM (k = q+1 to N) 1/k!

    so e - s_n = 1/(n+1)! + 1/(n+2)! + 1/(n+3)! + ...
               = 1/(n+1)! (1 + 1/(n+1) + 1/(n+1)^2)
               = 1/(n+1)! 1/n (geomeric series converges to ...)

    so therefore 0 < q!(e - s_q) < 1/q = q! SUM (k = q+1 to N) 1/k!

    and additionally q!s_q = q!(1 + 1 + 1/2! + ... + 1/q!) = some integer.

    Contradiction - cannot have an integer strictly between 0 and 1. Therefore
    e is irrational.

   f(z) = SUM (n=0 to inf) c_n z^n
        = c_0 + c_1 z + c_2 z^2 + ...

*** Theorem (for convergence)
    put alpha = LIM SUP (nth-root abs(c_n)) and R = 1/alpha(0, 1)
    then SUM (c_n z^n) converges absolutely for abs(z) < R and diverges for
    abs(z) > R. No information for abs(z) = R.

    conclusion: R is the 'radius of convergence'.

** Review

   various series tests
   if a sum converges absolutely, it converges.
   geometric series - SUM x^n converges iff abs(x) < 1.
   p-series - SUM 1/n^p converges iff p > 1
   we can reduce p-series to geometric series by a_n .GEQ. 0, a_n decreasing
   iff SUM 2^k a_{2^k} converges.

** Addition and Multiplication of Series

   Sequences - we know that LIM (a_n + b_n) = LIM a_n  + LIM b_n
                       and  LIM (a_n * b_n) = LIM a_n  * LIM b_n

   Look at some finite sum - s_n = SUM(from k=0 to n) a_k,
   t_n = SUM(from k=0 to n) b_k

   and u_n = s_n + t_n = SUM(from k=0 to n) (a_k + b_k)
   so u_n = SUM a_k + SUM b_k (they exist.)

   or u = A + B.

   For products, similar nice things happen:

   s_n*t_n = SUM(from k=0 to n) a_k * SUM(from k=0 to n) b_k
   s_n*t_n = SUM(from k,k'=0 to n,n) a_k * b_k

   so we can separate them the same way by introducing a dummy index.

*** Convolution product
    product of two power series - what happens?

    get some new summation for new constant c_r, where
    c_r = SUM(l=0 to r) a_l b_(r-l).

    This sum is a convolution! A more general idea that we will see later.

** power series

   does it follow that SUM(from n=0 to inf) (SUM(from k=0 to n) a_k a_(n-k)
   z^k) converges?

***** Example

      a_n = b_n = (-1)^n / (sqrt(n+1)), set c_n = SUM(from k=0 to n) a_k b_(n-k)

      so c_n = SUM(from k=0 to n) (-1)^k / sqrt(k + 1) * (-1)^(n-k) /
      (sqrt(n-k+1))

             = (-1)^n SUM(from k=0 to n) 1 / sqrt((n-k+1)(k+1))

      where (n-k+1)(k+1) = (n/2 + 1)^2 - (n/2 - k)^2
                         = n + 1 + k(n-k)
                         .LEQ. (n/2 + 1)^2

      Therefore abs(c_n) .GEQ. SUM(from k=0 to n) 2/(n+2), a divergent sum!

      Therefore we had a product of convergent coefficients that does not
      converge.

      What did we do? We summed over the lattice of natural integers (1,1),
      (1,2), etc.

      More exactly, due to the SUM from k=0 to n-k + 1 we are doing repeated
      triangles.

      Let t_n = SUM(from k,k' in S_n) a_kb_k'
      and v_n = SUM(from l=0 to n) (SUM(from k=0 to l) a_k b_l-k)


      from the example - neither SUM a_n nor SUM b_n converge absolutely.

****** Theorem (Menten's Theorem)
       Assume that SUM a_n converges absolutely and SUM b_n converges. Then SUM
       c_n converges correctly (that is, to AB).

       Proof. Say that A = SUM a_n, B = SUM b_n, beta_n = B_n - B (error term)

       Then c_n = a0b0 + (a0b1 + a1b1) + ...
                = a0Bn + a1B_n-1 + ... + a_n B0.
                = a0(B + beta_n) + a1(B + beta_n-1) + ... + a_n(B + beta_0)
                = A_n B + a_0b_n a_1 beta_n-1 + ...

       Now set gamma_n = a_0beta_n + ... + a_nb_0.
       (we have not yet used the absolute convergence condition yet - here it is)

       We need that gamma_n -> 0.

       Set alpha = SUM(from n=0 to inf) abs(a_n) < inf

       for some eps > 0 we have that beta_n -> 0.

       Choose some N s.t. n .GEQ. N, so abs(beta_n) .LEQ. eps..

       therefore abs(gamma_n) .LEQ. abs(beta_0 a_n + ... + beta_n * a_n-N) +
                                    abs(beta_N+1a_n-N + ... + beta_na_0)

       so lim sup abs(gamma_n) .LEQ. alpha * eps -> done.

** Exercise 13
   Cauchy product of absolutely convergent series converges absolutely.

   strategy - 'crash through with absolute values'
   Proof Show that SUM(from n=0 to N) abs(c_n)
   = SUM(from n=0 to N) abs(SUM(k=0 to n) a_k b_(n-k))
   .LEQ. SUM(n=0 to N) SUM(k=0 to n) abs(a_k) abs(b_n-k)

   .LEQ. SUM(k,k' in Sn) abs(a_k)abs(b_k')
   = SUM(abs(a_k)) SUM(abs(b_k'))
   .LEQ. infinite summation products < infinity.

** Alternating Series'
   Can show - SUM (n=1 to inf) = 1 - 1/2 + 1/3 + ... = ln 2 = inf - inf (bad!)

   we can't really add and subtract inf, but we can do rearrangements.

*** Subsequences and Rearrangements
    Take {k_n} = sequence of positive integers, where each positive integer
    appears exactly once.

    in short - the function n -> k_n is a bijection from NN to NN. We do not
    skip any numbers, just rearrange them.

    For example, we could do a_n = (-1)^n / n
    and a_k_n = positive terms = 1, 1/3, 1/5, etc. up until SUM a_k_n > 10,000
    and then take a_k_N+1 = -1/2, -1/4, ... after that point. Take points until
    the sum of sums

    SUM (from n = 1 to N) a_k_n (this one is > 10,000) + SUM(from N+1 to N2)
    a_n_k (negative terms) < -10,000

    Therefore we can start where ever we want and get -10,000 as our
    limit. This is bad.

*** Theorem 3.54
    Let SUM a_n be a series of real numbers that converges but SUM abs(a_n)
    diverges (a_n converges conditionally).

    Choose any two numbers alpha and beta in overbar(RR) s.t.
    -inf .LEQ. alpha .LEQ. beta .LEQ. +inf.

    Then there is a rearrangement {a_k_n} (where n -> k_n is a bijection) such
    that

    limsup (n to inf) SUM a_k_j = beta
    liminf (n to inf) SUM a_k_j = alpha.

**** Proof
     Details in book - follow the sketch given above for the 10,000 case.

*** However - Theorem 3.55
    If SUM a_n converges absolutely and SUM a_n = A then we cannot change A by
    rearrangement. (a_k_n must converge to the same thing)

    Moral of the story - sums that do not converge absolutely are 'weird' and
    sums that converge absolutely are 'nice'

**** Proof
     Say that m .GEQ. n .GEQ. N so SUM (i=n to m) abs(a_i) .LEQ. epsilon.

     Choose p s.t. 1,2,...,N included in k1,k2, ..., kp. Then n > p implies
     that

     s_n = SUM (j=1 to n) a_j, s_n' = SUM (j=1 to n) a_k_j

     where a_1, a_2, ... a_N cancel in forming s_n - s_n'.

     Therefore abs(s_n - s_n') .LEQ. SUM (j .GEQ. N) abs(a_j) .LEQ. eps

     lim s_n = A, so lim s_n' exists and equals A.


* Continuity

  Given some metric spaces X and Y, E subset X, f :: E -> Y, p in E',

  then we say that lim (x -> p) f(x) = q means that

  forall eps > 0, exists delta > 0 s.t. x in E and 0 < d(x,p) < delta implies
  d(f(x), q) < epsilon (for each appropriate metric)

  This definition is not vacuous - given some delta there always exists some x
  in E as p is a limit point.

** Sequential version (equivalent):
   forall {p_n} subset E with p_n /= p forall n, such that p_n -> p, then
   f(p_n) -> q in Y.

   Why are these equivalent?

   show that one definition implies the other.

   Given some {p_n} in E, p_n /= p, p_n -> p:
   show that f(p_n) -> q.

   Let epsilon > 0. Must find some N s.t. n .GEQ. N -> d(f(p_n), q) < epsilon.

   we know : exists delta > 0 s.t. x in E, 0 < d(x,p) < delta -> d(f(x), q) <
   epsilon.

   We also know : p_n -> p, so exists N s.t. n .GEQ. N -> 0 < d(p_n, p) <
   delta. All we must do now is apply the previous statement with x = p_n and
   we are done.

   Other direction is harder : show that the second implies the first.

   'Contrapositive overcomes all'

   Show that not 1 implies not 2.

   so not(1) = exists epsilon > 0 s.t. forall delta > 0, exists x in E with 0
   < d(x,p) < delta -> d(f(x),q) .GEQ. epsilon.

   This is like the additional problem for the homework.

   Therefore we can write p_n rather than x_delta_n.

   d(p_n,p) < 1/n (-> p_n -> p in X) and d(f(p_n), q) .GEQ. epsilon. Done.


** Routine Facts

   lim (x -> p) f(x) = A, lim (x -> p) g(x) = P

   'things get foggy up in the clouds'

   For f,g(x) :: E subset X -> Y
   fact 1 : lim (f(x) + g(x)) = A + B

   fact 2 : lim (f g)(x)      = AB (as long as Y is RR or CC)

   fact 3 : lim (f / g)(x)    = A/B (B /= 0, Y is RR or CC)


** Continuous functions

   given f :: E -> Y (for E subset X)

   we say that f is continuous at p <-> forall epsilon > 0, exists delta > 0
   so that x in E, 0 .LEQ. d(x,p) < delta -> d(f(x), f(p)) < delta.

   examples - if f :: NN -> RR then f is continuous! (all isolated points)

   if p is a limit point of E, then

   f continuous at p <-> lim f(x) = f(p) (as x -> p)

   Suppose that E = X. Then for f :: X -> Y, we say that f is continuous if f
   is continuous at all p in X.

   Theorem (or 'profound reformulation') Given that f :: X -> Y, then f is
   continuous iff V open in Y -> f^-1(v) = {x in X : f(x) in V } is open in X.


* Homework 9
  6a - Can show that it telescopes.
  6b -

  (sqrt(n + 1) - sqrt(n)) / n = 1 / (n*sqrt(n+1) + sqrt(n)) (multiply by
  conjugate)
  which is approximately 1/n^3, convergent by p-series.

** Show that nth root of n! diverges

   2nth root of (2n)! .GEQ. 2nth root of (2n) (2n - 1) (n + 1)
                      .GEQ. 2nth root of n^n
                         =  square root of the nth root of n^n
                         =  square root of n

   Bounded below by a divergent limit.


** More continuity
   Review : f :: (E subset X) -> Y, for metric spaces X and Y

   (p in E) then f is continuous at p if

   (case 1) p is an isolated point of E
   (case 2) p in E' and lim (x -> p) f(x) = f(p)

   Therefore, for f :: X -> Y
   f is continuous on X if f is continuous at each p in X.

*** Topology definition / our theorem for continuity

   f continuous <-> (V open in Y -> f^-1(V) open in X)

   proof in the book.

*** Composition - Theorem 4.7

    for metric spaces X, Y, Z, E subset X

    f :: E -> Y, g :: Range f -> Z,

    then for some h = g `composed` f :: E -> Z, then if both f and g are
    continuous then their composition is continuous.

**** Proof

     Show that given epsilon > 0, exists delta s.t.

     d(x,p) < delta -> d(g . f $ x , g . f $ p) < epsilon.

     Step 1 : g continuous at f(p) implies that exists delta_g > 0.

     as f is continuous at p, exists some delta_f s.t.

     d(x,p) < delta_f -> d(f(x), f(p)) < delta_g

     Therefore d(x,p) < delta_f . Therefore delta(g(f(x)), g(f(p))) <
     epsilon. Done!

**** Some other formulations

     f :: X -> Y, g :: Y -> Z both continuous implies h = g . f continuous.

     (the topology way)
     V open in Z -> g^-1(V) open in Y.
     therefore f^-1(g^-1(V)) open in X.

*** Arithmetic of continuous functions (4.9 and 4.10)

    if f and g are continuous, then
    result 1 : f + g continuous
    result 2 : f * g continuous
    result 3 : if g(x) /= 0 then f / g continuous
    result 4 : f . g continuous

*** Remarks

**** Remark 1 : if F :: X -> RR^k is a k-tuple of functions then F is continuous
     iff each component function is continuous.

     Proof - Use the norms, and bound them by maximums.

**** Remark 2 : If P :: RR^k -> R and each P[i] continuous then P is continuous.

     If f is continuous then P . f is continuous.

     Then, for free, we get polynomials.

     for XX = (x_1, ... , x_k) -> SUM a_(mess) x_1^k x_2^j ... = finite sum.

     if the product a_i_1 * ... * a_i_k is zero for all but finitely many terms
     then the summation is nice (the projections are continuous, the product of
     continuous is continuous, summation continuous, finite sums continuous,
     etc. Keep applying!)

**** Remark 3 : x -> norm(x) must be continuous (backward triangle inequality)

     as norm(x) - norm(y) .LEQ. norm(x - y)
     as norm(y) - norm(x) .LEQ. norm(y - x)

     so the norm that projects from the metric space to real numbers is
     continuous! Useful.

*** Tricks with a metric space

    how can we make (X,X) a metric space?

    many tricks - can pick d((x,y), (x',y')) = max(individual distances), sum
    of distances, square root of sum of squares of distances, etc.


** Next - insert new ingredient - compactness - and stir. (see what we get)

   Lovely theorem - f :: X -> Y, f continuous, X compact -> (map f X) compact.

   This is so much fun we get two proofs.

*** Proof 1 - (covering arguement)
    General topological proof. Let {V_alpha : alpha in A} be an open cover of
    (map f X). (Now we must find a finite subcover) Look at the inverse image
    {f^-1(V_alpha)}, which must be an open cover of X (as X is
    compact). Therefore there must exist finitely many alphas s.t. X `subset`
    f^-1(V_alpha1) ... which implies that (map f X) `subset` the union of the
    V_alphas. Therefore done.

*** Proof 2
    For some metric space Z, (this is another definition), Z is compact iff it
    has the sequential Bolzano-Weirstrass property.

    Let {z_n} be any sequence in Z. Therefore there is a convergent subsequence
    {z_n_k}.

    To show that (map f X) is compact, we must show that any sequence {f(x_n)}
    contained in (map f X) has a convergent subsequence. Then {x_n} `in` X, so
    X is compact.

    First direction - assume that there is some convergent subsequence
    x_n_k -> x0. as f is continuous (at x0, in particular) then we have a
    convergent subsequence f(x_n_k)


* Jointly Continuous
  d :: (X,X) -> RR is jointly continuous means :

  given (x0, y0) in (X,X) and epsilon > 0, exists delta >  s.t.

  d((x,y),(x0,y0)) < delta -> abs(d(x,y) - d(x0,y0)) < epsilon

** Theorems

   If f :: X -> is continuous, then X compact -> (map f X) compact

   corollary - f : X -> RR^k continuous, X compact -> (map f X) closed/bounded

   corollary - f : X -> RR continuous, X compact -> f achieves a maximum and
   minimum

   (i.e. for X_max in X, X_min in X, f(X_min) .LEQ. f(X) .LEQ. f(X_max))

*** Example
    for f(x) = 1/x on [1, inf]

    continuous, but has no minimum.

    f(x) = 1/x on (0,1]

    bounded (below) but not closed.

    for E in RR, if E is not closed we say that x0 in (closure E) \ E.

    f(x) = 1 / (x - x0) is continuous, not bounded on E.

    g(x) = 1 / (1 + (x - x0)^2) is continuous but not a ???

** Theorem 4.17

   For f : (compact X) -> Y continuous and 1-1 then g = f^-1 Y -> X is well
   defined and continuous.

*** Proof

    (by soft topology)

    To show g^-1(open) = open

    where g = f^-1 -> g^-1 = f.

    to show : f(open) = open (i.e. f is an open mapping)

    f 1-1 -> f(U^c) = f(U)^c as f(closed) = closed.

    But F is closed and is a subset of some copact space, so F is compact.

    then f(F) is compact, subset of Y, so f(F) is closed.

** Continuous functions are not open maps (in general)

   A counterexample - f(x) = x^2 on (-1,1) but (map f (-1,1)) = [0,1) no
   longer open.

** Review of Exercises

   #5 - f :: E -> RR continuous, E subset RR closed -> f has a continuous
   extension g : RR -> RR

   extension means that g restricted to E = f (some function with the same
   results in E that applies to the whole set RR)

   Use the fact E closed -> E^c = U open.

   U subset RR open <-> U is a disjoint union of countably many finite
   intervals.

   (think about the Cantor set C, U = C^c, all the middle thirds, should be
   all over the place and open.)

   (this will be homework, eventually)


* Uniformly Continuous

  f :: X -> Y continuous means that forall p in X and epsilon > 0, we have
  that

  exists delta = delta(p,epsilon) > 0 s.t. d(p,q) < delta(p,q) < d(p,epsilon)
  -> d_Y(f(p),f(q)) < epsilon.

  f :: X -> Y is continuous, X is compact implies that f is uniformly continuous.

** Proof 1

   (sequential Bolzano-Weirstrass) Assume that X is compact, show that not
   uniformly continuous implies f discontinuous at some x0.

   f not uniformly continuous -> exists epsilon > 0, sequences {x_n}, {y_n} in
   X s.t. d(x_n,y_n) -> 0 while d(f(x_n), f(y_n)) .GEQ. epsilon.

   X is compact, so the sequential Bolzano-Weirstrass implies existence of a
   subsequence {x_n_k} s.t. x_n_k -> x_k in X.

   note - d(y_n,x0) .LEQ. d(y_n, x_n) + d(x_n_k, x0) (it is a metric)

   Therefore, d(x_n_k, y_n_k) -> 0 as y_n_k -> x0.

   Set some z_k = x_n_k, k even, or y_n_k, k odd. Then also z_k -> x0.

   But d(f(z_k+1), f(z_k)) .GEQ. epsilon -> f(z_k) not cauchy.

   Then the limit of f(z_k) -> x0, so f is not continuous at x0.

*** Proof 2

    (covering argument)

    Let epsilon > 0 : must find some delta = delta(epsilon) s.t. x,y in X with
    d_X(x,y) < delta(epsilon) -> d(f(x), f(y)) < epsilon.

    By the hypothesis that f is continuous, f is continuous at some x, so

    exists delta_x > 0 (for fixed epsilon) so that for each y in X, with

    d_X(,x) < delta_x -> d_Y(f(y), f(x)) < epsilon/2

    'lets do it with epsilon/2 to be safe' -- Dr. Ball

    Look at { N_(delta_x / 2) (x), for x in X} (some neighborhood of radius
    delta_x/2)

    this is some open cover of X (cover all points)

    however, as X is compact, there exist finitely many xs {x1, ... xn} so that
    the whole space is contained in

    X `subset` { N_(delta_x_1 / 2) (x_1), for x in X} `union` ... for n
    .LEQ. N, a finite union.

    Try delta < delta(epsilon) = 1/2 * min(delta xs) > 0.

    Now we have our candidate, so check if it works.

    Choose any x,y in X with d(x,y) < delta (same as above)
    then x in some neighborhood delta_x_j / 2 `subset` neighborhood delta_x_j

    therefore the distance

    d_Y(f(x), f(x_j)) < epsilon / 2.

    But also - d_X(y, x_j) .LEQ. d_X(y,x) + d_X(x, x_j) < delta + 1/2 delta_x_j

    but we picked delta less than 1/2 delta_x_j, so


    d_X(y, x_j) .LEQ. d_X(y,x) + d_X(x, x_j) < delta_x_j.

    Therefore d_Y(f(y), f(x_j)) < epsilon/2

    and d_Y(f(y), f(x)) .LEQ. d_Y(f(y), f(x_j)) + d_Y(f(x_j), f(x)) <
    d_X(y, x_j) .LEQ. d_X(y,x) + d_X(x, x_j) < epsilon/2 + epsilon/2 = epsilon.

*** Discussion of Number 11 on Homework

    f uniformly continuous from X into Y, {x_n} cauchy in X -> {f(x_n)} cauchy
    in Y.

    Ten, if E is dense in X, f uniformly continuous on E -> f has a continuous
    extension to bar(E).

    We could pick some x_n in E where x0 in closure(E), where f(x0) = lim
    f(x_n).


** How about not uniformly continuous?

   Not uniformly continuous means that there exists some epsilon > 0
   s.t. forall delta > 0, exist p_delta, q_delta in X where

   d_X(p_delta, q_delta) < delta s.t. d_Y(f(p_delta), f(q_delta))
   .GEQ. epsilon.

**** Sequential Version

     Choose some delta = 1/n. Exists epsilon0 and p_n, q_n in X with

     d_X(p_n, q_n) < 1/n and d_Y(f(p_n), f(q_n)) .GEQ. epsilon0.

     OR

     exists epsilon0 and {p_n}, {q_n} in X with

     d_X(p_n, q_n) > 0 while d_Y(f(p_n), f(q_n)) .GEQ. epsilon0.

**** Example

     f(x) = 1/x on (0,1]

     try 1 : x_n = 1/n, y_n = 1/(2n) so d(x_n,y_n) = 1/2n. This goes to zero
     but

     abs(f(x_n) - f(y_n)) = n - 2n = -n blows up.


** Exercise 8 - not assigned.

   'Looked too easy' - thanks, Dr. Ball.

   function f continuously bounded on some E in RR

   -> f bounded on some E1 -> second reason why some f(x) = 1/x not uniformly
   continuous on (0,1)

**** Proof

     Let delta = delta(1). Therefore (x,y) in E, d(x,y) < delta(1)

     so abs(f(x) - f(y)) < 1. Cover E with finitely many intervals of length
     delta (we can do that because E is bounded).

     Say that N is the number of finite subcovers needed. Then there is a chain
     x' = x0 < x1 < ... < x_n = y with n .LEQ. N steps and x_i - x_i-1 <
     delta(1)

     then abs(f(y) - f(x') ) = SUM (from i=1 to N) abs(f(x_i) - f(x_i-1))

     'crash through with absolute values'

     .LEQ. N

     so abs(f(y)) .LEQ. abs(f(x')) + N.



* Connectedness

  E is connected <-> E is not disconnected.

  so E = A `union` B with A /= nullset, B /= nullset,
  A intersect (closure B) = nullset,
  (closure A) intersect B = nullset.

  'a door is never open and closed at the same time, but a set can be' --
  Dr. Ball.

*** Theorem 4.22

    if f is continuous on some X -> Y, E subset X connected, then f(E) is
    connected.

    by previous remarks, this is equivalent to:

*** Theorem 4.22'

    f :: X -> Y continuous (and onto), X and Y metric spaces, and X connected
    implies that Y is connected.

**** Proof

     (show that Y = f(X) disconnected implies that X is disconnected)

     Say that Y = f(X) = A `union` B, for A, B both open, A, B both nonempty.

     Set U = f^-1(A), V = f^-1(B), and as f is onto and A, B nonempty, that U
     and V are nonempty.

     we also know that X = U `union` V as f is a function.

     U `intersect` V is empty (f(x) in A `intersect B impossible, assumed
     disjoint)

***** Recall some old homework problem

      X = some connected metric space containing at least two points p and q ->
      X uncountable.

      X connected -> there exists some p_delta with d(p,p_delta) = delta, for
      each delta in [0, d(p,q)] so delta -> p_delta is 1-1 implies X
      uncountable.


**** More fun

     By 10.1, f continuous, X connected by f(X) connected in RR shows that f(X)
     contains distinct number 0, d(p,q) > 0. so the connected subsets are
     uncountable.


*** Theorem 4.23

    f continuous, real-valued function on some interval [a,b] - suppose that
    f(a) < y < f(b) implies exists some x where f(x) = y.

    (since f([a,b]) is connected)

    converse not always true, since f(x) = sin (1/x) x/=0, = 0 at x=0, have the
    intermediate value property but is not continuous at x = 0
